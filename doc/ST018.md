18. **3.1 当前子任务上下文准备Agent (Current Subtask Context Preparer):**

I will retrieve all relevant information from the PKBM (as simulated by the provided documents) and package it for the subsequent detailed design agents.

```json
{
  "context_package_id": "CP_ST018_v1.0",
  "sub_task_id": "ST018",
  "sub_task_description": "Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.",
  "timestamp": "2023-10-29T09:00:00Z",
  "prepared_by": "3.1 Current Subtask Context Preparer",
  "contexts": {
    "user_stories_and_acceptance_criteria": {
      "source_document_id": "RequirementsSpecificationDocument_v1.0",
      "description": "User stories that necessitate interaction with an external LLM, thus requiring the LLM_Interaction_Gateway_Service. While these user stories don't directly define the gateway's internal HTTP logic, they establish the need for its functionality. The specific requirements for the gateway's HTTP logic are primarily derived from the architectural design.",
      "relevant_user_stories_ids": [
        "US021", 
        "US024", 
        "US026", 
        "US027", 
        "US029", 
        "US031", 
        "US035", 
        "US039" 
      ],
      "notes": "Detailed ACs for these user stories define what the LLM should achieve. ST018 provides the mechanism for the system to ask the LLM to achieve it."
    },
    "architectural_design": {
      "source_document_id": "HighLevelArchitectureDesignDocument_v1.0",
      "description": "Defines the LLM_Interaction_Gateway_Service, its responsibilities, and its interactions within the system.",
      "relevant_sections": [
        {
          "section_number": "2",
          "title": "Technology Selection and Rationale",
          "content_summary": "Specifies Python as the primary language and 'Standard REST API calls via a Web Service' for LLM Interaction."
        },
        {
          "section_number": "3",
          "title": "Component List and Responsibilities",
          "content_summary": "Defines 'LLM_Interaction_Gateway_Service' (Component 5) with core responsibilities including 'managing API calls (REST-based)', 'handling request construction and response parsing', 'implementing retry mechanisms'."
        },
        {
          "section_number": "4",
          "title": "Component Interaction and Major Interfaces",
          "content_summary": "Describes how agents interact with LLM_Interaction_Gateway_Service (e.g., `llm_gateway.execute_llm_call()`) and its role in consistent handling of API calls."
        },
        {
          "section_number": "5.2",
          "title": "Mermaid Diagram Description",
          "content_summary": "Visually places LLM_Interaction_Gateway_Service and its connection to an External LLM."
        }
      ],
      "related_interface_draft_id": "ComponentInterfaceDraft_v1.0",
      "interface_details_for_llm_gateway": [
        {
            "interface_id": "CID011",
            "description": "Internal call from Agent_Orchestration_Service to LLM_Interaction_Gateway_Service to execute an LLM task.",
            "type": "Internal Python Function Call",
            "caller": "Agent_Orchestration_Service (any agent needing LLM)",
            "callee": "LLM_Interaction_Gateway_Service",
            "preliminary_signature": "execute_llm_call(prompt: str, llm_config: dict) -> str",
            "purpose": "To send a prompt to a configured external LLM and receive its response.",
            "key_data_exchanged": ["Prompt text", "LLM specific configurations (e.g., model name, temperature)", "LLM response text"]
        }
      ]
    },
    "project_metadata": {
      "source_document_id": "项目元数据 v1.0",
      "description": "Provides overall project settings, including technology stack choices relevant to LLM interaction.",
      "relevant_data": {
        "project_name": "多Agent软件开发自动化系统 (简化版)",
        "technology_stack_preferences": {
          "primary_programming_language": {
            "language": "Python"
          },
          "llm_interaction": {
            "method": "Standard REST API calls via Web Service",
            "details": "LLM interaction integrated into the Flask web service, potentially for chat-like interfaces or CI workflows."
          }
        }
      }
    },
    "coding_specifications": {
      "source_document_id": "SystemInternal_CodingStandard_v1.0",
      "description": "The comprehensive coding standards to be strictly followed for all Python code development, including module design, configuration management, error handling, logging, code style, dependency management, and code reuse. No comments or docstrings are allowed.",
      "full_document_reference": "我目前暂时的编码规范如下 (as provided in the prompt history)."
    },
    "existing_code_or_file_structure": {
      "description": "This sub-task (ST018) involves implementing new core logic for making HTTP calls. No existing code snippets or specific file structures are provided for modification related to this exact task. The file structure will be planned by Agent 3.5 based on the project's overall structure and this component's nature.",
      "status": "Not applicable for modification; new implementation required."
    }
  }
}
```

19. **3.2 API详细设计Agent (API Detailed Design Agent):**

The output will be a Markdown document.

```markdown
# API Detailed Design: External LLM Interaction Profile

**Document ID:** API_Detailed_Design_ST018_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-29T09:30:00Z
**Generated By:** 3.2 API详细设计Agent
**Sub-Task ID:** ST018
**Sub-Task Description:** Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
**Based On:** Context Package `CP_ST018_v1.0`

## 1. Introduction

This document specifies the detailed characteristics and expected patterns of external REST APIs that the `LLM_Interaction_Gateway_Service` will interact with. The purpose of this service is to abstract and manage communications with various third-party Large Language Models (LLMs). This design outlines the common structures for requests, responses, authentication, and error handling that the gateway's HTTP client logic (developed under sub-task ST018) must accommodate.

This document does *not* define an API exposed *by* the `LLM_Interaction_Gateway_Service` for internal system components (that is covered by the internal Python function interface `execute_llm_call` as per `CID011` in `HighLevelArchitectureDesignDocument_v1.0`). Instead, it profiles the *target external APIs*.

## 2. General Assumptions for External LLM APIs

The `LLM_Interaction_Gateway_Service` will be built with the following general assumptions about the external LLM APIs it consumes:

*   **Communication Protocol:** HTTPS will be used for all communications to ensure data in transit is encrypted.
*   **Data Format:** Request and response bodies will primarily be in JSON format (`application/json`).
*   **Synchronous Interaction:** The initial design targets synchronous, request-response interactions. Streaming responses are considered out of scope for the initial implementation to maintain simplicity, but the design should be extensible for future support if needed.

## 3. Authentication and Authorization

A common pattern for authenticating with external LLM APIs is via API keys.

*   **Method:** API Key transmitted in the HTTP Authorization header.
*   **Header Format:** `Authorization: Bearer <LLM_API_KEY>`
*   **Key Management and Provision:**
    *   The actual `<LLM_API_KEY>` value, along with the specific target URL for the external LLM, is expected to be provided to the `LLM_Interaction_Gateway_Service` at runtime. This will likely be part of the `llm_config: dict` argument passed to its internal `execute_llm_call` function.
    *   In adherence with the project's security premise (prioritizing simplicity and functionality in a secure, isolated environment), no complex key management systems, vaults, or rotation mechanisms will be implemented within the gateway. The API key is treated as a direct configuration input.

## 4. Generic External LLM API Interaction Profile

The following details a generic profile for typical LLM API endpoints (e.g., for text completion or chat-based generation). The `LLM_Interaction_Gateway_Service` should be flexible enough to accommodate minor variations through its configuration.

### 4.1. HTTP Endpoint Details

*   **HTTP Method:** `POST`
    *   This is the most common method for sending data (like prompts and parameters) to an LLM API and receiving a generated response.
*   **URL Path:**
    *   The specific URL path is highly dependent on the LLM provider and the model being accessed.
    *   The `LLM_Interaction_Gateway_Service` will require the full target URL (e.g., `https://api.provider.com/v1/engines/davinci/completions` or `https://api.anotherprovider.ai/v2/chat`) to be provided as part of the `llm_config`.
    *   **Example Placeholders:**
        *   `https://{llm_provider_base_url}/vX/completions`
        *   `https://{llm_provider_base_url}/vX/chat/completions`
        *   `https://{llm_provider_base_url}/vX/models/{model_id}:generateText`

### 4.2. Request Headers

The following standard request headers are anticipated:

*   **`Content-Type: application/json`** (Mandatory for JSON request bodies)
*   **`Accept: application/json`** (Recommended to indicate desired response format)
*   **`Authorization: Bearer <LLM_API_KEY>`** (As detailed in Section 3)
*   **Provider-Specific Headers:** Some LLM providers might require additional custom headers (e.g., `OpenAI-Organization`, `X-API-Key` if not using Bearer token). The gateway should allow for passing arbitrary additional headers if specified in `llm_config`.

### 4.3. Request Body (JSON)

The request body structure will vary between LLM providers and models. The gateway should construct the request body based on parameters passed in `llm_config` and the core `prompt` or `messages`.

**Example JSON Schema for a Chat Completion Request:**
```json
{
  "type": "object",
  "properties": {
    "model": {
      "type": "string",
      "description": "The ID of the language model to use (e.g., 'gpt-4', 'claude-3-opus-20240229'). This will be part of llm_config."
    },
    "messages": {
      "type": "array",
      "description": "A list of message objects representing the conversation history.",
      "items": {
        "type": "object",
        "properties": {
          "role": {
            "type": "string",
            "enum": ["system", "user", "assistant"],
            "description": "The role of the message sender."
          },
          "content": {
            "type": "string",
            "description": "The content of the message."
          }
        },
        "required": ["role", "content"]
      }
    },
    "temperature": {
      "type": "number",
      "minimum": 0.0,
      "maximum": 2.0,
      "description": "Controls randomness. Lower values make output more deterministic. (Optional, from llm_config)"
    },
    "max_tokens": {
      "type": "integer",
      "minimum": 1,
      "description": "The maximum number of tokens to generate in the response. (Optional, from llm_config)"
    },
    "stream": {
        "type": "boolean",
        "default": false,
        "description": "Whether to stream back partial progress. For initial simplicity, the gateway will set this to false and handle non-streamed responses. (Internal default, not typically user-configured)"
    }
    // Other provider-specific parameters like top_p, presence_penalty, frequency_penalty, stop_sequences etc.
    // can be included if passed via llm_config.
  },
  "required": ["model", "messages"]
}
```

**Example JSON Schema for a Text Completion Request (Legacy/Simpler Models):**
```json
{
  "type": "object",
  "properties": {
    "model": {
      "type": "string",
      "description": "The ID of the language model to use."
    },
    "prompt": {
      "type": "string",
      "description": "The prompt text to generate completions for."
    },
    "temperature": {
      "type": "number"
    },
    "max_tokens": {
      "type": "integer"
    }
    // ... other parameters
  },
  "required": ["model", "prompt"]
}
```

### 4.4. Response Body (JSON)

The `LLM_Interaction_Gateway_Service` must parse the JSON response from the external LLM.

#### 4.4.1. Successful Response (HTTP 200 OK)

The structure varies, but a common pattern includes a list of choices, with each choice containing the generated text or message.

**Example JSON Schema for a Chat Completion Success Response:**
```json
{
  "type": "object",
  "properties": {
    "id": { "type": "string", "description": "A unique identifier for the completion." },
    "object": { "type": "string", "description": "The type of object, e.g., 'chat.completion'." },
    "created": { "type": "integer", "description": "Unix timestamp of when the completion was created." },
    "model": { "type": "string", "description": "The model used for the completion." },
    "choices": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "index": { "type": "integer" },
          "message": {
            "type": "object",
            "properties": {
              "role": { "type": "string", "enum": ["assistant"] },
              "content": { "type": "string", "description": "The generated message content." }
            },
            "required": ["role", "content"]
          },
          "finish_reason": { "type": "string", "description": "Reason the model stopped, e.g., 'stop', 'length', 'tool_calls'." }
        },
        "required": ["index", "message", "finish_reason"]
      }
    },
    "usage": { // Optional
      "type": "object",
      "properties": {
        "prompt_tokens": { "type": "integer" },
        "completion_tokens": { "type": "integer" },
        "total_tokens": { "type": "integer" }
      }
    }
  },
  "required": ["id", "object", "created", "model", "choices"]
}
```
The `LLM_Interaction_Gateway_Service` will primarily be interested in extracting the `content` from the first choice's `message`.

#### 4.4.2. Error Responses (HTTP 4xx, 5xx)

External LLM APIs typically return structured JSON errors. The gateway should attempt to parse these for logging and potentially for guiding retry logic.

**Example Generic JSON Schema for an Error Response:**
```json
{
  "type": "object",
  "properties": {
    "error": {
      "type": "object",
      "properties": {
        "message": { "type": "string", "description": "A human-readable description of the error." },
        "type": { "type": "string", "description": "A short string identifying the type of error (e.g., 'invalid_request_error', 'authentication_error')." },
        "param": { "type": ["string", "null"], "description": "The specific parameter that caused the error, if applicable." },
        "code": { "type": ["string", "null"], "description": "A provider-specific error code." }
      },
      "required": ["message", "type"]
    }
  }
}
```

**Common HTTP Status Codes and Their Implications for the Gateway:**

*   **`400 Bad Request`**: Indicates an issue with the request sent by the gateway (e.g., malformed JSON, missing required parameters). Should be logged, and likely indicates a bug in the gateway's request construction or invalid `llm_config`. Retrying the exact same request is unlikely to succeed.
*   **`401 Unauthorized`**: API key is invalid, missing, or expired. Log error. No retry should be attempted with the same key.
*   **`403 Forbidden`**: API key is valid but lacks permissions for the requested operation or resource. Log error. No retry.
*   **`429 Too Many Requests`**: Rate limit exceeded. The gateway might implement a simple retry mechanism after a delay for this, if configured. Response might include a `Retry-After` header.
*   **`500 Internal Server Error`**: An error occurred on the LLM provider's side. A limited number of retries might be appropriate.
*   **`503 Service Unavailable`**: The LLM provider's service is temporarily unavailable (e.g., overloaded or down for maintenance). Retries after a delay are appropriate. Response might include a `Retry-After` header.

## 5. Simplifications and Scope for Initial Implementation (ST018)

*   **No Streaming:** The gateway will initially only support non-streaming (synchronous) request-response cycles. It will wait for the full response before returning.
*   **Focus on Text/Chat:** The primary focus is on text-based generation (completion or chat). Other modalities (image, audio) are out of scope.
*   **Basic Retry Logic:** Retry logic within the gateway for transient errors (like 429, 500, 503) will be simple (e.g., a fixed number of attempts with a fixed delay), configurable via `llm_config` or gateway initialization. Complex exponential backoff with jitter may be deferred.
*   **Error Propagation:** The gateway will aim to capture the HTTP status code and the error message from the LLM's response and propagate this information back to its caller (the agent that invoked `execute_llm_call`).

This detailed design for external API interaction will guide the implementation of the HTTP call logic within the `LLM_Interaction_Gateway_Service` for sub-task ST018.
```
```

20. **3.3 数据模型详细设计Agent (Data Model Detailed Design Agent):**

The output will be a Markdown document.

```markdown
# Data Model Detailed Design: LLM Interaction Gateway Service (ST018)

**Document ID:** DataModel_Detailed_Design_ST018_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-29T10:00:00Z
**Generated By:** 3.3 数据模型详细设计Agent
**Sub-Task ID:** ST018
**Sub-Task Description:** Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
**Based On:** Context Package `CP_ST018_v1.0`, API Detailed Design `API_Detailed_Design_ST018_v1.0`

## 1. Introduction

This document details the design of internal data models required for the `LLM_Interaction_Gateway_Service`, specifically for the core logic of making HTTP calls to external Large Language Models (LLMs) as part of sub-task ST018. These models are intended to be implemented as Python dataclasses, adhering to the project's coding specifications.

The primary purpose of these data models is to provide a structured, type-safe way to handle:
*   Configuration for individual LLM API calls.
*   Representation of HTTP responses received from external LLMs.
*   Details of errors encountered during LLM interactions.

This design does not cover database table structures, as ST018 does not involve direct database persistence beyond the project-wide file-based PKBM.

## 2. Core Data Models (Python Dataclasses)

The following Python dataclasses are proposed to structure the data used by the `LLM_Interaction_Gateway_Service`'s HTTP client logic.

### 2.1. `RetryConfig`

This dataclass defines the configuration for retry attempts when an LLM API call fails with a transient error.

```python
from dataclasses import dataclass, field
from typing import List, Optional

@dataclass
class RetryConfig:
    """
    Configuration for retrying failed LLM API calls.
    """
    max_retries: int = 3
    delay_seconds: float = 5.0  # Fixed delay for simplicity
    retryable_status_codes: List[int] = field(default_factory=lambda: [429, 500, 503])

    # Attribute Descriptions:
    #
    # max_retries: int
    #   Description: The maximum number of retry attempts for a failed API call.
    #   Default: 3
    #
    # delay_seconds: float
    #   Description: The fixed number of seconds to wait before a retry attempt.
    #   Default: 5.0
    #
    # retryable_status_codes: List[int]
    #   Description: A list of HTTP status codes that should trigger a retry attempt.
    #   Default: [429 (Too Many Requests), 500 (Internal Server Error), 503 (Service Unavailable)]
```

### 2.2. `LLMCallConfig`

This dataclass represents the structured and validated configuration for a single call to an external LLM API. An instance of this class would typically be created from the `llm_config: dict` argument passed to the `LLM_Interaction_Gateway_Service`'s internal `execute_llm_call` function.

```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional

# Assuming RetryConfig is defined as above

@dataclass
class LLMCallConfig:
    """
    Structured configuration for an individual LLM API call.
    """
    target_url: str
    api_key: str # The Bearer token or relevant API key
    http_method: str = "POST"
    # request_payload_parameters: Dict[str, Any] -> This will be constructed by the service
    # based on the core prompt/messages and other parameters from the llm_config dict.
    # The actual prompt/messages are passed directly to execute_llm_call, not part of this static config object.
    additional_headers: Dict[str, str] = field(default_factory=dict)
    retry_config: Optional[RetryConfig] = field(default_factory=RetryConfig)
    timeout_seconds: float = 60.0  # Timeout for the HTTP request

    # Attribute Descriptions:
    #
    # target_url: str
    #   Description: The full target URL of the external LLM API endpoint.
    #   Example: "https://api.example-llm.com/v1/chat/completions"
    #
    # api_key: str
    #   Description: The API key or Bearer token required for authentication with the external LLM.
    #
    # http_method: str
    #   Description: The HTTP method to be used for the API call.
    #   Default: "POST"
    #
    # additional_headers: Dict[str, str]
    #   Description: A dictionary of any additional or override HTTP headers to be sent with the request,
    #                beyond standard ones like Content-Type or Authorization (which is built from api_key).
    #   Default: {}
    #
    # retry_config: Optional[RetryConfig]
    #   Description: Configuration for retry logic. If None, retries may be disabled or use system defaults.
    #   Default: An instance of RetryConfig with default values.
    #
    # timeout_seconds: float
    #   Description: The timeout in seconds for the HTTP request to the external LLM.
    #   Default: 60.0
```
**Note on `request_payload_parameters`:** The `LLMCallConfig` focuses on the static configuration of *how* to make the call (URL, key, method, retry, timeout). The actual dynamic payload (e.g., `{"model": "...", "messages": [...]}` or `{"model": "...", "prompt": "..."}`) will be constructed within the `execute_llm_call` method by combining the `prompt_data` (which could be a string or a list of message dicts) and other specific parameters (like `model`, `temperature`, `max_tokens`) that are also extracted from the generic `llm_config: dict` input. This keeps `LLMCallConfig` focused on connection and policy aspects.

### 2.3. `LLMErrorDetails`

This dataclass structures error information parsed from an unsuccessful LLM API response body. It aligns with the generic error response schema outlined in `API_Detailed_Design_ST018_v1.0`.

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class LLMErrorDetails:
    """
    Structured details of an error returned by an external LLM API.
    """
    message: str
    error_type: Optional[str] = None # 'type' is a reserved keyword, so 'error_type'
    param: Optional[str] = None
    code: Optional[str] = None

    # Attribute Descriptions:
    #
    # message: str
    #   Description: A human-readable description of the error.
    #
    # error_type: Optional[str]
    #   Description: A short string from the LLM API identifying the type of error (e.g., 'invalid_request_error').
    #   Default: None
    #
    # param: Optional[str]
    #   Description: The specific parameter that caused the error, if applicable and provided by the LLM API.
    #   Default: None
    #
    # code: Optional[str]
    #   Description: A provider-specific error code, if provided by the LLM API.
    #   Default: None
```

### 2.4. `LLMResponse`

This dataclass represents the outcome of an HTTP call made to an external LLM, including status, body, and any parsed error information. The `LLM_Interaction_Gateway_Service`'s `execute_llm_call` function is expected to return the LLM's textual response (`str`), but internally, this `LLMResponse` object will be used to manage the full context of the HTTP interaction.

```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional

# Assuming LLMErrorDetails is defined as above

@dataclass
class LLMResponse:
    """
    Represents the comprehensive result of an HTTP call to an external LLM.
    """
    status_code: int
    body_text: str # Raw response body as text
    headers: Dict[str, str] = field(default_factory=dict)
    parsed_body: Optional[Dict[str, Any]] = None # If JSON, the parsed dictionary
    error_details: Optional[LLMErrorDetails] = None

    @property
    def is_successful(self) -> bool:
        """Determines if the HTTP call was successful (2xx status code)."""
        return 200 <= self.status_code < 300

    # Attribute Descriptions:
    #
    # status_code: int
    #   Description: The HTTP status code received from the external LLM API.
    #
    # body_text: str
    #   Description: The raw response body received from the API, as a string.
    #
    # headers: Dict[str, str]
    #   Description: A dictionary of response headers received from the API.
    #   Default: {}
    #
    # parsed_body: Optional[Dict[str, Any]]
    #   Description: If the response body was successfully parsed as JSON, this holds the resulting dictionary.
    #                Otherwise, it is None.
    #   Default: None
    #
    # error_details: Optional[LLMErrorDetails]
    #   Description: If the API call resulted in an error and the error response was parsable into
    #                structured details, this holds an instance of LLMErrorDetails. Otherwise, it is None.
    #   Default: None
    #
    # is_successful (property): bool
    #   Description: A read-only property that returns True if the status_code is in the 2xx range,
    #                False otherwise.
```

## 3. Relationships and Usage

*   An `LLMCallConfig` object will be instantiated and used by the `LLM_Interaction_Gateway_Service` to configure each outgoing HTTP request. It may contain an instance of `RetryConfig`.
*   After an HTTP request is made, an `LLMResponse` object will be created to hold all details of the response from the external LLM.
*   If the `LLMResponse` indicates an error (e.g., non-2xx status code) and the error body can be parsed, an `LLMErrorDetails` object may be instantiated and associated with the `LLMResponse`.

These data models provide the necessary structure for the `LLM_Interaction_Gateway_Service` to manage its HTTP interactions in a robust and maintainable way, adhering to the project's coding standards by using Python dataclasses for data representation.
```
```

21. **3.4 类与函数逻辑设计Agent (Class & Function Logic Design Agent):**

Here is the Class & Function Logic Design Document:

```markdown
# Class & Function Logic Design: LLM_Interaction_Gateway_Service (ST018)

**Document ID:** ClassFunctionLogicDesign_ST018_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-29T10:30:00Z
**Generated By:** 3.4 类与函数逻辑设计Agent
**Sub-Task ID:** ST018
**Sub-Task Description:** Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
**Based On:** Context Package `CP_ST018_v1.0`, API Detailed Design `API_Detailed_Design_ST018_v1.0`, Data Model Design `DataModel_Detailed_Design_ST018_v1.0`

## 1. Introduction

This document outlines the design for classes and functions within the `LLM_Interaction_Gateway_Service` responsible for executing HTTP calls to external Large Language Models (LLMs). The design focuses on the internal logic required to fulfill sub-task ST018. All Python code structures will conform to the project's defined coding standards.

A central class, `LLMInteractionGatewayService`, will encapsulate this logic. It will utilize the previously defined data models (`LLMCallConfig`, `RetryConfig`, `LLMResponse`, `LLMErrorDetails`) to manage configuration, requests, and responses.

## 2. Custom Exceptions

To provide more specific error information from the gateway service, a custom exception class will be defined.

### 2.1. `LLMGatewayException`

*   **Purpose:** A base custom exception for errors originating within the `LLMInteractionGatewayService`. This allows callers to catch gateway-specific issues.
*   **Inheritance:** `Exception`
*   **Attributes:**
    *   `message: str`: Human-readable error message.
    *   `original_exception: Optional[Exception] = None`: The underlying exception, if any.
    *   `error_details: Optional[LLMErrorDetails] = None`: Structured error details from the LLM API, if available.
    *   `status_code: Optional[int] = None`: HTTP status code associated with the error, if applicable.

```python
from typing import Optional
# LLMErrorDetails would be imported from where it's defined (e.g., data_models.py)

class LLMGatewayException(Exception):
    def __init__(self, 
                 message: str, 
                 original_exception: Optional[Exception] = None,
                 error_details: Optional[Any] = None, # Type Any for LLMErrorDetails placeholder
                 status_code: Optional[int] = None) -> None:
        super().__init__(message)
        self.message: str = message
        self.original_exception: Optional[Exception] = original_exception
        self.error_details: Optional[Any] = error_details # Type Any for LLMErrorDetails placeholder
        self.status_code: Optional[int] = status_code
```

## 3. Main Service Class: `LLMInteractionGatewayService`

This class will contain the core logic for making HTTP calls to external LLMs.

*   **Purpose:** To abstract and manage interactions with external LLM APIs, including request construction, HTTP communication, response parsing, error handling, and retries.
*   **Key Python Libraries to be Used:** `requests` (for HTTP calls), `logging`, `time` (for retry delays).

### 3.1. `__init__` (Constructor)

*   **Method Signature:**
    ```python
    def __init__(self, default_retry_config: Optional[RetryConfig] = None, logger: Optional[logging.Logger] = None) -> None:
    ```
*   **Purpose:** Initializes the service, optionally with a default retry configuration and a logger instance.
*   **Parameters:**
    *   `default_retry_config: Optional[RetryConfig]`: An optional `RetryConfig` instance to be used if a specific call does not provide its own. If `None`, a default internal `RetryConfig` will be created.
    *   `logger: Optional[logging.Logger]`: An optional logger instance. If `None`, a default logger for this class/module will be obtained.
*   **Core Logic:**
    1.  Store `default_retry_config`. If `None`, instantiate a `RetryConfig` with its default values and store it.
    2.  Setup the logger: If `logger` is provided, use it. Otherwise, get a logger instance (e.g., `logging.getLogger(__name__)`).
*   **Error Handling:** None specific to `__init__` beyond standard Python object initialization.

### 3.2. `execute_llm_call` (Public Method)

*   **Method Signature (as per CID011, adapted for more flexible prompt_data):**
    ```python
    def execute_llm_call(self, prompt_data: Any, llm_config_dict: Dict[str, Any]) -> str:
    ```
*   **Purpose:** The primary public method to send a request to an external LLM and get its textual response. It orchestrates configuration parsing, request preparation, HTTP execution with retries, and response processing.
*   **Parameters:**
    *   `prompt_data: Any`: The core data for the LLM prompt. This could be a simple string for text completion or a list of message dictionaries for chat completions.
    *   `llm_config_dict: Dict[str, Any]`: A dictionary containing all necessary configuration for the LLM call, including `target_url`, `api_key`, `model`, `temperature`, `max_tokens`, and optionally retry settings, additional headers, etc.
*   **Returns:**
    *   `str`: The textual content from the LLM's successful response.
*   **Core Logic (Detailed Steps):**
    1.  Log the entry into the method, indicating the nature of `prompt_data` (e.g., type, length if string, number of messages if list) and the keys in `llm_config_dict` for traceability (avoid logging sensitive values like API keys directly in full).
    2.  **Parse Configuration:**
        *   Call a private helper method `_parse_and_validate_llm_call_config(llm_config_dict)` to convert `llm_config_dict` into a validated `LLMCallConfig` dataclass instance. This method will also extract other LLM-specific parameters (like `model`, `temperature`, `max_tokens`) not part of the static `LLMCallConfig` itself but needed for the payload.
        *   Store the returned `LLMCallConfig` instance and the dictionary of specific LLM parameters.
    3.  **Prepare Request Payload:**
        *   Call a private helper method `_prepare_request_payload(prompt_data, specific_llm_params_dict)` to construct the JSON request body.
    4.  **Initialize Retry Variables:**
        *   `retries_attempted: int = 0`
        *   `last_llm_gateway_exception: Optional[LLMGatewayException] = None`
        *   Determine the `current_retry_config`: Use `llm_call_config.retry_config` if provided and valid, otherwise use `self.default_retry_config`.
    5.  **Retry Loop:**
        *   `while retries_attempted <= current_retry_config.max_retries:`
            a.  Log the attempt number (e.g., `f"Attempt {retries_attempted + 1} of {current_retry_config.max_retries + 1}"`).
            b.  **Make HTTP Request:**
                *   Call a private helper method `_make_actual_http_request(llm_call_config, request_payload_dict)` which returns an `LLMResponse` object.
            c.  **Process HTTP Response:**
                *   If `llm_response.is_successful` (i.e., HTTP status 200-299):
                    i.  Log success (status code).
                    ii. Call `_extract_text_from_successful_llm_response(llm_response)` to get the string content.
                    iii. Log successful text extraction.
                    iv. Return the extracted text string.
                *   Else (HTTP error):
                    i.  Log failure (status code, response body text, error details if parsed).
                    ii. Create an `LLMGatewayException` with details from `llm_response.status_code` and `llm_response.error_details`. Store this as `last_llm_gateway_exception`.
                    iii. Check if `llm_response.status_code` is in `current_retry_config.retryable_status_codes`.
                    iv. If not retryable, or if `retries_attempted == current_retry_config.max_retries`, then `break` the loop (to re-raise `last_llm_gateway_exception` outside).
            d.  **Handle Request Exceptions (e.g., Timeout, Network Error from `_make_actual_http_request`):**
                *   If `_make_actual_http_request` raises an `LLMGatewayException` (e.g., due to `requests.Timeout` or `requests.ConnectionError`):
                    i.  Log the exception.
                    ii. Store this exception as `last_llm_gateway_exception`.
                    iii. If `retries_attempted == current_retry_config.max_retries`, then `break` the loop.
            e.  **Retry Delay:**
                *   If the loop hasn't broken or returned (meaning a retry is pending):
                    i.  Log the intent to retry after a delay.
                    ii. `time.sleep(current_retry_config.delay_seconds)`.
            f.  `retries_attempted += 1`.
    6.  **After Loop:**
        *   If the loop finishes due to max retries reached, log the final failure.
        *   Raise `last_llm_gateway_exception` (or a generic `LLMGatewayException` if `last_llm_gateway_exception` is somehow `None`, indicating "Failed after all retries").
*   **Error Handling:**
    *   Catches exceptions from helper methods (e.g., config parsing, request prep, HTTP call).
    *   Wraps them in `LLMGatewayException` where appropriate.
    *   Implements retry logic based on `RetryConfig`.
    *   Raises `LLMGatewayException` if the call ultimately fails after all retries or due to a non-retryable error.

### 3.3. `_parse_and_validate_llm_call_config` (Private Helper Method)

*   **Method Signature:**
    ```python
    def _parse_and_validate_llm_call_config(self, config_dict: Dict[str, Any]) -> Tuple[LLMCallConfig, Dict[str, Any]]:
    ```
*   **Purpose:** Parses the raw `llm_config_dict` into a structured `LLMCallConfig` object and extracts other LLM-specific parameters. Performs basic validation.
*   **Parameters:**
    *   `config_dict: Dict[str, Any]`: The raw configuration dictionary.
*   **Returns:**
    *   `Tuple[LLMCallConfig, Dict[str, Any]]`: A tuple containing the `LLMCallConfig` instance and a dictionary of specific LLM parameters (e.g., `model`, `temperature`, `max_tokens`, `stream`).
*   **Core Logic:**
    1.  Log entry.
    2.  Validate presence of mandatory keys in `config_dict` (e.g., `target_url`, `api_key`, `model`). If missing, log error and raise `LLMGatewayException("Missing required LLM configuration: [key_name]")`.
    3.  Extract values for `LLMCallConfig` fields: `target_url`, `api_key`.
    4.  Extract optional values for `LLMCallConfig`: `http_method` (default to "POST"), `additional_headers`, `timeout_seconds`.
    5.  Parse `retry_config` from `config_dict` if present, creating an `RetryConfig` instance.
    6.  Instantiate `LLMCallConfig` with these values.
    7.  Extract other LLM-specific parameters (e.g., `model`, `temperature`, `max_tokens`, `stream` (default to `False` as per API design)) from `config_dict` into a separate dictionary.
    8.  Log successful parsing.
    9.  Return the `LLMCallConfig` instance and the specific LLM parameters dictionary.
*   **Error Handling:**
    *   Raises `LLMGatewayException` for missing required fields or invalid data types during parsing.

### 3.4. `_prepare_request_payload` (Private Helper Method)

*   **Method Signature:**
    ```python
    def _prepare_request_payload(self, prompt_data: Any, specific_llm_params: Dict[str, Any]) -> Dict[str, Any]:
    ```
*   **Purpose:** Constructs the JSON payload dictionary to be sent to the external LLM API.
*   **Parameters:**
    *   `prompt_data: Any`: The core prompt information (string or list of message dicts).
    *   `specific_llm_params: Dict[str, Any]`: Dictionary containing LLM-specific parameters like `model`, `temperature`, `max_tokens`, `stream`.
*   **Returns:**
    *   `Dict[str, Any]`: The request payload as a Python dictionary.
*   **Core Logic:**
    1.  Log entry.
    2.  Initialize `payload: Dict[str, Any] = {}`.
    3.  Add `model` from `specific_llm_params` to `payload`. If missing, log error and raise `LLMGatewayException`.
    4.  **Determine Payload Structure (Chat vs. Completion):**
        *   Inspect `prompt_data`. If it's a list of dictionaries (assume chat messages format):
            *   Set `payload["messages"] = prompt_data`.
            *   Validate basic structure of messages if deemed necessary (e.g., each message has 'role' and 'content').
        *   Else if it's a string (assume text completion prompt):
            *   Set `payload["prompt"] = prompt_data`.
        *   Else (unsupported `prompt_data` type):
            *   Log error and raise `LLMGatewayException("Unsupported prompt_data type")`.
    5.  Add optional parameters from `specific_llm_params` to `payload` (e.g., `temperature`, `max_tokens`).
    6.  Ensure `payload["stream"]` is set to `False` (as per API design for ST018, which specifies non-streaming).
    7.  Log the constructed payload (potentially masking or summarizing sensitive/large parts).
    8.  Return `payload`.
*   **Error Handling:**
    *   Raises `LLMGatewayException` for unsupported `prompt_data` types or missing essential parameters like `model`.

### 3.5. `_make_actual_http_request` (Private Helper Method)

*   **Method Signature:**
    ```python
    def _make_actual_http_request(self, call_config: LLMCallConfig, payload_dict: Dict[str, Any]) -> LLMResponse:
    ```
*   **Purpose:** Executes the actual HTTP request using the `requests` library and wraps the result in an `LLMResponse` object.
*   **Parameters:**
    *   `call_config: LLMCallConfig`: The validated configuration for the call.
    *   `payload_dict: Dict[str, Any]`: The JSON request body as a dictionary.
*   **Returns:**
    *   `LLMResponse`: An object containing the HTTP status, body, headers, and any parsed error details.
*   **Core Logic:**
    1.  Log entry, indicating target URL and HTTP method.
    2.  **Prepare Headers:**
        *   `headers: Dict[str, str] = {}`
        *   `headers["Content-Type"] = "application/json"`
        *   `headers["Accept"] = "application/json"`
        *   `headers["Authorization"] = f"Bearer {call_config.api_key}"`
        *   Merge `call_config.additional_headers` into `headers`, allowing overrides.
    3.  **Make Request using `requests` library:**
        *   `try:`
            *   `http_lib_response = requests.request(`
                *   `method=call_config.http_method,`
                *   `url=call_config.target_url,`
                *   `headers=headers,`
                *   `json=payload_dict,`
                *   `timeout=call_config.timeout_seconds`
                *   `)`
            *   Log the raw response status code and potentially headers.
            *   `parsed_body_json: Optional[Dict[str, Any]] = None`
            *   `llm_error_details: Optional[LLMErrorDetails] = None`
            *   `try:`
                *   `parsed_body_json = http_lib_response.json()`
                *   If `not (200 <= http_lib_response.status_code < 300)` and `parsed_body_json` contains an 'error' key (as per API design):
                    *   `error_data = parsed_body_json.get("error", {})`
                    *   `llm_error_details = LLMErrorDetails(`
                        *   `message=error_data.get("message", http_lib_response.reason),`
                        *   `error_type=error_data.get("type"),`
                        *   `param=error_data.get("param"),`
                        *   `code=error_data.get("code")`
                        *   `)`
            *   `except requests.exceptions.JSONDecodeError:`
                *   Log warning: "Failed to decode JSON response body". `parsed_body_json` remains `None`.
                *   If not a 2xx response, `llm_error_details` might be created with just the `http_lib_response.text` or reason as message.
            *   Return `LLMResponse(`
                *   `status_code=http_lib_response.status_code,`
                *   `body_text=http_lib_response.text,`
                *   `headers=dict(http_lib_response.headers),`
                *   `parsed_body=parsed_body_json,`
                *   `error_details=llm_error_details`
                *   `)`
        *   `except requests.exceptions.Timeout as e:`
            *   Log error: "HTTP request timed out".
            *   Raise `LLMGatewayException(f"Request to {call_config.target_url} timed out", original_exception=e)`.
        *   `except requests.exceptions.ConnectionError as e:`
            *   Log error: "HTTP connection error".
            *   Raise `LLMGatewayException(f"Connection error for {call_config.target_url}", original_exception=e)`.
        *   `except requests.exceptions.RequestException as e:  # Catch other requests-related errors`
            *   Log error: "Generic HTTP request error".
            *   Raise `LLMGatewayException(f"HTTP request failed for {call_config.target_url}: {str(e)}", original_exception=e)`.
*   **Error Handling:**
    *   Catches `requests` library exceptions (Timeout, ConnectionError, other RequestException) and re-raises them as `LLMGatewayException` with context.
    *   Handles JSON decoding errors for the response body.
    *   Populates `LLMErrorDetails` if an error response from the LLM is successfully parsed.

### 3.6. `_extract_text_from_successful_llm_response` (Private Helper Method)

*   **Method Signature:**
    ```python
    def _extract_text_from_successful_llm_response(self, llm_response: LLMResponse) -> str:
    ```
*   **Purpose:** Extracts the primary textual content from a parsed successful LLM API response.
*   **Parameters:**
    *   `llm_response: LLMResponse`: The `LLMResponse` object, assumed to be for a successful call and have a `parsed_body`.
*   **Returns:**
    *   `str`: The extracted text content.
*   **Core Logic:**
    1.  Log entry.
    2.  Check if `llm_response.parsed_body` is `None`. If so, log error and raise `LLMGatewayException("Cannot extract text, response body not parsed or empty")`.
    3.  `parsed_json = llm_response.parsed_body`
    4.  **Try to extract based on common patterns (refer to `API_Detailed_Design_ST018_v1.0`):**
        *   If `parsed_json.get("choices")` is a list and not empty:
            *   `first_choice = parsed_json["choices"][0]`
            *   If `first_choice.get("message")` is a dict and `first_choice["message"].get("content")` is a string (chat completion style):
                *   Return `first_choice["message"]["content"]`.
            *   Else if `first_choice.get("text")` is a string (older completion style):
                *   Return `first_choice["text"]`.
        *   If other known successful structures exist, add checks for them.
    5.  If text cannot be extracted using known patterns:
        *   Log error: "Could not find expected text content in LLM response structure".
        *   Raise `LLMGatewayException("Failed to extract content from LLM response structure")`.
*   **Error Handling:**
    *   Raises `LLMGatewayException` if the `parsed_body` is missing or if the expected keys/structure for content extraction are not found.

This design provides a comprehensive plan for the classes and functions needed to implement the core HTTP calling logic for the `LLM_Interaction_Gateway_Service` under sub-task ST018, adhering to all specified constraints and requirements.
```
```

22. **3.5 文件与目录结构规划Agent (File & Directory Structure Planner Agent):**

Here is the **File and Directory Structure Plan**:

```markdown
# File and Directory Structure Plan for Sub-Task ST018

**Document ID:** FileStructurePlan_ST018_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-29T11:00:00Z
**Generated By:** 3.5 File & Directory Structure Planner Agent
**Sub-Task ID:** ST018
**Sub-Task Description:** Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
**Based On:** Context Package `CP_ST018_v1.0`, `API_Detailed_Design_ST018_v1.0`, `DataModel_Detailed_Design_ST018_v1.0`, `ClassFunctionLogicDesign_ST018_v1.0`, Project Coding Specifications.

## 1. Introduction

This document outlines the planned file and directory structure for the implementation of sub-task ST018, which concerns the `LLM_Interaction_Gateway_Service`. The structure aims to be modular, align with Python best practices, and provide a clear organization for the service's components, including its core logic, data models, and custom exceptions.

## 2. Guiding Principles

*   **Modularity:** Each component of the `LLM_Interaction_Gateway_Service` (service logic, data models, exceptions) will be organized to promote high cohesion and low coupling.
*   **Python Conventions:** Standard Python package and module naming conventions will be followed (e.g., lowercase with underscores for directories and files, `__init__.py` for packages).
*   **Component-Based Structure:** The overall project is envisioned to have a root source directory (e.g., `multi_agent_system`), under which each major architectural component will reside in its own sub-package.
*   **Clarity and Maintainability:** The structure should be intuitive and easy for developers to navigate and maintain.

## 3. Proposed Project Root Directory Structure (Conceptual)

For context, the `LLM_Interaction_Gateway_Service` will fit into a larger project structure. A conceptual top-level structure might look like this (ST018 additions are marked with `*`):

```
multi_agent_system/
├── __init__.py
├── agent_orchestrator/
│   └── ...
├── llm_gateway/                  * (New Directory for ST018)
│   ├── __init__.py               * (New File for ST018)
│   ├── data_models.py            * (New File for ST018)
│   ├── exceptions.py             * (New File for ST018)
│   └── service.py                * (New File for ST018)
├── pkbm/
│   └── ...
├── web_ui/
│   └── ...
├── workflow_manager/
│   └── ...
├── core/                         (Potential future directory for shared logic/configs as dataclasses)
│   ├── __init__.py
│   └── config_models.py          (Example: for project-wide Python dataclass configurations)
└── main.py                       (Or similar entry point for the application)
```
**Note:** This plan focuses on the creation of the `llm_gateway` sub-package and its contents for ST018. Other directories are illustrative of the broader project context.

## 4. Detailed File Plan for Sub-Task ST018

The following new files and directories will be created to implement the `LLM_Interaction_Gateway_Service` as per ST018:

1.  **Directory:** `multi_agent_system/llm_gateway/`
    *   **Full Path:** `multi_agent_system/llm_gateway/`
    *   **Type:** New Directory
    *   **Purpose:** To serve as the Python package root for all code related to the `LLM_Interaction_Gateway_Service`. This isolates the gateway's logic from other system components.

2.  **File:** `multi_agent_system/llm_gateway/__init__.py`
    *   **Full Path:** `multi_agent_system/llm_gateway/__init__.py`
    *   **Type:** New File
    *   **Purpose:**
        *   To mark the `llm_gateway` directory as a Python package.
        *   To potentially expose key classes or functions from the package for easier import by other parts of the system (e.g., `from multi_agent_system.llm_gateway import LLMInteractionGatewayService`).
    *   **Initial Content (Conceptual):**
        ```python
        # multi_agent_system/llm_gateway/__init__.py
        from .service import LLMInteractionGatewayService
        from .data_models import LLMCallConfig, RetryConfig, LLMResponse, LLMErrorDetails
        from .exceptions import LLMGatewayException

        __all__ = [
            "LLMInteractionGatewayService",
            "LLMCallConfig",
            "RetryConfig",
            "LLMResponse",
            "LLMErrorDetails",
            "LLMGatewayException",
        ]
        ```

3.  **File:** `multi_agent_system/llm_gateway/data_models.py`
    *   **Full Path:** `multi_agent_system/llm_gateway/data_models.py`
    *   **Type:** New File
    *   **Purpose:** To define all Python dataclasses used by the `LLM_Interaction_Gateway_Service` for structuring configuration, requests, and responses. This includes:
        *   `RetryConfig`
        *   `LLMCallConfig`
        *   `LLMErrorDetails`
        *   `LLMResponse`
    *   **Content Source:** Based on `DataModel_Detailed_Design_ST018_v1.0`.

4.  **File:** `multi_agent_system/llm_gateway/exceptions.py`
    *   **Full Path:** `multi_agent_system/llm_gateway/exceptions.py`
    *   **Type:** New File
    *   **Purpose:** To define custom exceptions specific to the `LLM_Interaction_Gateway_Service`. This includes:
        *   `LLMGatewayException`
    *   **Content Source:** Based on `ClassFunctionLogicDesign_ST018_v1.0`.

5.  **File:** `multi_agent_system/llm_gateway/service.py`
    *   **Full Path:** `multi_agent_system/llm_gateway/service.py`
    *   **Type:** New File
    *   **Purpose:** To implement the core logic of the `LLM_Interaction_Gateway_Service`, primarily the `LLMInteractionGatewayService` class. This class will handle:
        *   Parsing and validating LLM call configurations.
        *   Preparing HTTP request payloads.
        *   Making HTTP requests to external LLM APIs using the `requests` library.
        *   Handling HTTP responses, including parsing JSON and error conditions.
        *   Implementing retry logic.
        *   Extracting textual content from successful LLM responses.
    *   **Content Source:** Based on `ClassFunctionLogicDesign_ST018_v1.0`. This file will import from `data_models.py` and `exceptions.py` within the same package.

## 5. Rationale for Structure

*   **Encapsulation:** Placing all `LLM_Interaction_Gateway_Service` related code within its own package (`llm_gateway`) enhances encapsulation and makes the component self-contained.
*   **Separation of Concerns:**
    *   `data_models.py` separates data structure definitions from service logic.
    *   `exceptions.py` centralizes custom exception definitions for the gateway.
    *   `service.py` focuses on the operational logic of the gateway.
*   **Adherence to Coding Specifications:** The proposed structure supports modular design and high cohesion as per the project's coding standards. The use of `__init__.py` allows for controlled exposure of the package's public API.
*   **Scalability:** This structure allows for easy addition of new functionalities or internal modules within the `llm_gateway` package in the future without cluttering other parts of the project.

## 6. Assumptions and Notes

*   The root directory for the project's source code is assumed to be `multi_agent_system/`. If a different root is established (e.g., `src/multi_agent_system/`), all paths should be adjusted accordingly.
*   No existing files are being modified for ST018; all listed files and directories are new creations.
*   The `main.py` and other component directories (`agent_orchestrator`, `pkbm`, etc.) shown in the conceptual structure are for context and are not created as part of ST018's file plan.
*   The `core/config_models.py` is a placeholder suggestion for how project-wide Python dataclass configurations (as mandated by coding spec 2.1-2.5) might be organized, but not directly part of ST018. The `LLMInteractionGatewayService` itself will receive its specific configurations (like `LLMCallConfig`) dynamically.

This plan provides a clear path for organizing the files related to sub-task ST018.
```

23. **3.6 安全考量与检查点定义Agent (Security Considerations & Checkpoint Definer Agent):**

Here is the **Security Considerations Report & Checkpoint List for Sub-Task ST018**:

```markdown
# Security Considerations Report & Checkpoint List

**Document ID:** SecurityConsiderations_ST018_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-29T11:30:00Z
**Generated By:** 3.6 安全考量与检查点定义Agent
**Sub-Task ID:** ST018
**Sub-Task Description:** Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
**Based On:**
*   Context Package `CP_ST018_v1.0`
*   API Detailed Design `API_Detailed_Design_ST018_v1.0`
*   Data Model Detailed Design `DataModel_Detailed_Design_ST018_v1.0`
*   Class & Function Logic Design `ClassFunctionLogicDesign_ST018_v1.0`
*   Project Coding Specifications & "安全考量特殊指令"

## 1. Introduction

This document outlines security considerations and checkpoints for the implementation of the `LLM_Interaction_Gateway_Service` (sub-task ST018). The assessment is conducted under the **critical project premise** that the system will be deployed in a **completely isolated and absolutely secure environment**.

Consequently, all security considerations and checkpoints are defined with the **absolute priority given to 100% functional success, extreme simplicity of implementation, and ease of configuration and use.** Measures that might typically be recommended for environments with external exposure but would add complexity, user validation steps, sophisticated key management, or permission controls are **explicitly avoided** to align with the project's unique operational context and directives (Q3.6.1, Q3.6.2, and "安全考量特殊指令"). The focus is on code stability and ensuring the gateway reliably performs its function of interacting with external LLMs.

## 2. Security Risk Assessment & Corresponding Checkpoints

The following potential risks were considered. The checkpoints primarily reinforce existing good design aspects or suggest minimal, simple measures that do not impede functionality or add operational burden.

### 2.1. Input Validation and Robustness (Related to SSRF/Injection if not for isolated env.)

*   **Identified Risk (Standard Context):** If `target_url` or other parameters in `llm_config_dict` (originating from upstream agents/user input) are not validated, it could lead to the gateway attempting to call malformed URLs, or in other environments, potentially contribute to SSRF or injection if those parameters were used insecurely.
*   **Project-Specific Consideration:** The "isolated and absolutely secure environment" premise significantly mitigates the impact of risks like SSRF originating from a controlled `target_url`. The primary concern here is the functional stability of the gateway.
*   **Checkpoints & Recommendations (Simplicity First):**
    1.  **URL Format Validation (Minimal):**
        *   **Checkpoint:** The `_parse_and_validate_llm_call_config` method (as designed in `ClassFunctionLogicDesign_ST018_v1.0`) should validate that the `target_url` string starts with `http://` or `https://`.
        *   **Rationale:** This is a basic sanity check for functional correctness, ensuring the `requests` library receives a scheme it understands. No complex regex or domain allow-listing is required, as this would add unnecessary complexity.
    2.  **Parameter Type Validation:**
        *   **Checkpoint:** The `_parse_and_validate_llm_call_config` method should verify the basic types of essential configuration parameters (e.g., `api_key` is a string, `timeout_seconds` is a number).
        *   **Rationale:** Prevents runtime errors within the gateway due to incorrect configuration types, ensuring stability.
    3.  **Log Sanitization (Implicit):**
        *   **Checkpoint:** Rely on the standard Python `logging` library's default behavior for handling special characters. No custom sanitization logic for log messages is needed.
        *   **Rationale:** Standard library behavior is generally safe for log files and avoids adding complex sanitization code.

### 2.2. Sensitive Data Handling (API Keys)

*   **Identified Risk (Standard Context):** Accidental logging or improper handling of LLM API keys.
*   **Project-Specific Consideration:** API keys are necessary for functionality. The goal is to use them simply and avoid accidental exposure within the system's own artifacts (like logs).
*   **Checkpoints & Recommendations (Simplicity First):**
    1.  **No API Key Logging:**
        *   **Checkpoint:** Strictly adhere to the design note in `ClassFunctionLogicDesign_ST018_v1.0` to **avoid logging full API keys**. When logging `llm_config_dict` or related data for debugging, API keys must be masked (e.g., `api_key: "*******"`) or only their presence noted.
        *   **Rationale:** Prevents accidental exposure in logs, which is a common vulnerability, while keeping logging simple.
    2.  **In-Memory Handling:**
        *   **Checkpoint:** API keys are passed to `execute_llm_call` via the `llm_config_dict` and are used in memory. The gateway service itself should not store these keys persistently (e.g., on disk).
        *   **Rationale:** Aligns with simplicity; keys are managed by the calling agent/configuration mechanism.
    3.  **HTTPS for Transit:**
        *   **Checkpoint:** Confirm that the `requests` library is used to make calls to `https://` URLs (as specified in `API_Detailed_Design_ST018_v1.0` and validated by the minimal URL check).
        *   **Rationale:** Ensures data (including prompts, responses, and headers with API keys) is encrypted in transit to the external LLM. This is a standard feature of `requests` when using HTTPS URLs.

### 2.3. Error Handling and Information Disclosure

*   **Identified Risk (Standard Context):** Verbose error messages from the gateway itself (not the external LLM) could reveal too much internal system structure or data.
*   **Project-Specific Consideration:** Errors should be clear for debugging the multi-agent system but not overly verbose to the point of being a security risk in a less isolated environment (though here, isolation is assumed).
*   **Checkpoints & Recommendations (Simplicity First):**
    1.  **Structured Exceptions:**
        *   **Checkpoint:** Utilize the custom `LLMGatewayException` (as designed in `ClassFunctionLogicDesign_ST018_v1.0`) to wrap errors originating within the gateway. This exception should carry a clear message, an optional `original_exception` for detailed debugging by developers, and potentially `LLMErrorDetails` if the error came from the LLM.
        *   **Rationale:** Provides clear, actionable error information for system developers and calling agents without exposing raw, extensive stack traces directly in all error paths.
    2.  **Information to Calling Agent:**
        *   **Checkpoint:** Ensure that information propagated back to the calling agent (e.g., via a raised `LLMGatewayException`) is sufficient for the agent to understand the outcome (e.g., "LLM API returned 401", "Configuration error: target_url missing") without needing full internal gateway state.
        *   **Rationale:** Balances debuggability with encapsulation.

### 2.4. Denial of Service (DoS) / Resource Exhaustion

*   **Identified Risk (Standard Context):** Uncontrolled retries or lack of timeouts could lead to resource exhaustion in the gateway or contribute to DoS against the external LLM.
*   **Project-Specific Consideration:** The gateway should be robust but not overly complex in its retry/timeout mechanisms.
*   **Checkpoints & Recommendations (Simplicity First):**
    1.  **Configurable Timeouts:**
        *   **Checkpoint:** Implement the `timeout_seconds` parameter in `LLMCallConfig` and use it in `requests.request()` calls, as designed.
        *   **Rationale:** Prevents indefinite blocking on unresponsive LLM APIs, ensuring functional stability.
    2.  **Bounded Retries:**
        *   **Checkpoint:** Implement the retry logic using `RetryConfig` (max_retries, delay_seconds) as designed in `ClassFunctionLogicDesign_ST018_v1.0`.
        *   **Rationale:** Prevents uncontrolled retry loops, ensuring functional stability and avoiding excessive load on external services. Simple fixed delays are sufficient.
    3.  **No Internal Input Size Limiting:**
        *   **Checkpoint:** The gateway itself should not implement complex logic to limit the size of prompts or responses it handles. It should rely on the `requests` library's capabilities and the external LLM's own limits.
        *   **Rationale:** Adding size limiting introduces complexity and potential for incorrectly rejecting valid, large inputs/outputs, thus hindering functionality.

### 2.5. Secure Communication with External LLMs

*   **Identified Risk (Standard Context):** Use of unencrypted channels (HTTP) to communicate with external LLMs.
*   **Project-Specific Consideration:** Essential for protecting API keys and data in transit.
*   **Checkpoints & Recommendations (Simplicity First):**
    1.  **Enforce HTTPS:**
        *   **Checkpoint:** The gateway logic should primarily target HTTPS URLs. The minimal URL validation (`starts with https://`) helps guide this. Calls to plain `http://` URLs should be clearly identifiable as non-standard if ever allowed by configuration (though `https://` is the strong default expectation from `API_Detailed_Design_ST018_v1.0`).
        *   **Rationale:** Standard practice for secure API communication; `requests` handles TLS/SSL transparently.

## 3. Summary of Non-Applicable Security Measures

Due to the project's specific directives ("安全考量特殊指令," Q3.6.1, Q3.6.2) and the assumption of a secure, isolated deployment environment, the following common security measures are **intentionally excluded** from the recommendations for ST018 to maintain simplicity and direct functionality:

*   Complex input sanitization libraries or routines (beyond basic type/format checks for stability).
*   Web Application Firewall (WAF) style protections.
*   Advanced SSRF prevention mechanisms (e.g., network-level controls, strict domain allow-lists within the gateway's code).
*   User authentication/authorization mechanisms *within or for* the gateway.
*   Dedicated secrets management systems or vaults for API keys (keys are passed via configuration).
*   Intrusion Detection/Prevention Systems (IDS/IPS) integration.
*   Rate limiting *imposed by* the gateway on its callers (it *handles* rate limiting from external LLMs).
*   Security header hardening beyond standard `requests` behavior (e.g., HSTS, CSP - not applicable for a backend service client).

The security of this component relies on the robustness of the `requests` library for HTTP interactions, the simplicity of its own logic, careful handling of API keys (no logging), and the overarching security of its deployment environment.
```

24. **3.7 详细设计汇总与指令生成Agent (Detailed Design Aggregation & Instruction Generator Agent):**

First, I will generate the **《代码实现指导书 (Code Implementation Guide)》**.

## Output 1: 代码实现指导书 (Code Implementation Guide)

```markdown
# 代码实现指导书: LLM_Interaction_Gateway_Service (ST018)

**Document ID:** 实现指导书_ST018_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-29T12:00:00Z
**Generated By:** 3.7 详细设计汇总与指令生成Agent
**Sub-Task ID:** ST018
**Sub-Task Description:** Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
**Based On:**
*   API_Detailed_Design_ST018_v1.0
*   DataModel_Detailed_Design_ST018_v1.0
*   ClassFunctionLogicDesign_ST018_v1.0
*   FileStructurePlan_ST018_v1.0
*   SecurityConsiderations_ST018_v1.0
*   Project Coding Specifications

## 1. 概述 (Overview)

本指导书详细说明了实现 `LLM_Interaction_Gateway_Service` 核心HTTP调用逻辑 (子任务ST018) 的要求。该服务的目标是封装与外部大型语言模型 (LLM) API的交互，提供一个统一、可配置的接口供系统内其他Agent使用。

实现将遵循项目定义的编码规范，并特别注意本指导书中列出的安全检查点和设计细节。

## 2. 待创建的文件与目录结构 (Files & Directory Structure to be Created)

根据 `FileStructurePlan_ST018_v1.0`，将创建以下新的文件和目录：

*   **目录:** `multi_agent_system/llm_gateway/`
*   **文件:**
    1.  `multi_agent_system/llm_gateway/__init__.py`
    2.  `multi_agent_system/llm_gateway/data_models.py`
    3.  `multi_agent_system/llm_gateway/exceptions.py`
    4.  `multi_agent_system/llm_gateway/service.py`

## 3. 关键数据模型 (Key Data Models - Python Dataclasses)

在 `multi_agent_system/llm_gateway/data_models.py` 文件中定义以下Python dataclasses (源自 `DataModel_Detailed_Design_ST018_v1.0`):

### 3.1. `RetryConfig`
```python
from dataclasses import dataclass, field
from typing import List, Optional

@dataclass
class RetryConfig:
    max_retries: int = 3
    delay_seconds: float = 5.0
    retryable_status_codes: List[int] = field(default_factory=lambda: [429, 500, 503])
```

### 3.2. `LLMCallConfig`
```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional

# Assuming RetryConfig is defined above

@dataclass
class LLMCallConfig:
    target_url: str
    api_key: str
    http_method: str = "POST"
    additional_headers: Dict[str, str] = field(default_factory=dict)
    retry_config: Optional[RetryConfig] = field(default_factory=RetryConfig)
    timeout_seconds: float = 60.0
```

### 3.3. `LLMErrorDetails`
```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class LLMErrorDetails:
    message: str
    error_type: Optional[str] = None
    param: Optional[str] = None
    code: Optional[str] = None
```

### 3.4. `LLMResponse`
```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional

# Assuming LLMErrorDetails is defined above

@dataclass
class LLMResponse:
    status_code: int
    body_text: str
    headers: Dict[str, str] = field(default_factory=dict)
    parsed_body: Optional[Dict[str, Any]] = None
    error_details: Optional[LLMErrorDetails] = None

    @property
    def is_successful(self) -> bool:
        return 200 <= self.status_code < 300
```

## 4. 自定义异常 (Custom Exceptions)

在 `multi_agent_system/llm_gateway/exceptions.py` 文件中定义以下自定义异常 (源自 `ClassFunctionLogicDesign_ST018_v1.0`):

### 4.1. `LLMGatewayException`
```python
from typing import Optional, Any # Any for LLMErrorDetails placeholder

# LLMErrorDetails would be conceptually imported if this were actual code
# from .data_models import LLMErrorDetails 

class LLMGatewayException(Exception):
    def __init__(self, 
                 message: str, 
                 original_exception: Optional[Exception] = None,
                 error_details: Optional[Any] = None, # Placeholder for LLMErrorDetails
                 status_code: Optional[int] = None) -> None:
        super().__init__(message)
        self.message: str = message
        self.original_exception: Optional[Exception] = original_exception
        self.error_details: Optional[Any] = error_details # Placeholder
        self.status_code: Optional[int] = status_code
```

## 5. 服务类实现: `LLMInteractionGatewayService`

在 `multi_agent_system/llm_gateway/service.py` 文件中实现 `LLMInteractionGatewayService` 类。此类将包含核心的HTTP调用逻辑。

### 5.1. 必要的导入 (Required Imports)
```python
import time
import logging
import requests # External dependency
from typing import Dict, Any, Optional, Tuple, List # Standard typing

# Local imports from within the llm_gateway package
from .data_models import LLMCallConfig, RetryConfig, LLMResponse, LLMErrorDetails
from .exceptions import LLMGatewayException
```

### 5.2. `__init__` (Constructor)
*   **签名:** `def __init__(self, default_retry_config: Optional[RetryConfig] = None, logger: Optional[logging.Logger] = None) -> None:`
*   **逻辑:**
    1.  存储 `default_retry_config`。若为 `None`，则实例化一个具有默认值的 `RetryConfig`并存储。
    2.  设置日志记录器：若提供了 `logger`，则使用它；否则，获取一个默认的日志记录器 (例如, `logging.getLogger(__name__)`)。

### 5.3. `execute_llm_call` (Public Method)
*   **签名:** `def execute_llm_call(self, prompt_data: Any, llm_config_dict: Dict[str, Any]) -> str:`
*   **逻辑 (详细步骤参考 `ClassFunctionLogicDesign_ST018_v1.0` - Section 3.2):**
    1.  记录方法入口信息 (注意避免记录敏感数据如完整API Key)。
    2.  调用 `_parse_and_validate_llm_call_config` 解析并验证 `llm_config_dict`，获取 `LLMCallConfig` 实例和特定LLM参数。
    3.  调用 `_prepare_request_payload` 准备请求体。
    4.  初始化重试相关变量。
    5.  执行重试循环：
        a.  记录尝试次数。
        b.  调用 `_make_actual_http_request` 发起HTTP请求，获取 `LLMResponse`。
        c.  处理响应：若成功，调用 `_extract_text_from_successful_llm_response` 提取文本并返回；若失败，记录错误，检查是否可重试。
        d.  处理请求级异常 (如超时、连接错误)。
        e.  若需重试，则等待指定延迟。
        f.  增加尝试次数。
    6.  若循环结束仍失败，则抛出 `last_llm_gateway_exception`。

### 5.4. `_parse_and_validate_llm_call_config` (Private Helper Method)
*   **签名:** `def _parse_and_validate_llm_call_config(self, config_dict: Dict[str, Any]) -> Tuple[LLMCallConfig, Dict[str, Any]]:`
*   **逻辑 (详细步骤参考 `ClassFunctionLogicDesign_ST018_v1.0` - Section 3.3):**
    1.  验证 `config_dict` 中必需的键 (如 `target_url`, `api_key`, `model`)。
    2.  提取并构造 `LLMCallConfig` 实例。
    3.  提取其他特定于LLM的参数 (如 `model`, `temperature`, `max_tokens`, `stream`) 到一个单独的字典中。
    4.  返回 `LLMCallConfig` 实例和特定LLM参数字典。

### 5.5. `_prepare_request_payload` (Private Helper Method)
*   **签名:** `def _prepare_request_payload(self, prompt_data: Any, specific_llm_params: Dict[str, Any]) -> Dict[str, Any]:`
*   **逻辑 (详细步骤参考 `ClassFunctionLogicDesign_ST018_v1.0` - Section 3.4):**
    1.  根据 `prompt_data` 的类型 (字符串或消息列表) 构造请求体。
    2.  添加 `model` 和其他来自 `specific_llm_params` 的参数。
    3.  确保 `stream` 参数设置为 `False`。

### 5.6. `_make_actual_http_request` (Private Helper Method)
*   **签名:** `def _make_actual_http_request(self, call_config: LLMCallConfig, payload_dict: Dict[str, Any]) -> LLMResponse:`
*   **逻辑 (详细步骤参考 `ClassFunctionLogicDesign_ST018_v1.0` - Section 3.5):**
    1.  准备HTTP头部 (包括 `Content-Type`, `Accept`, `Authorization`)。
    2.  使用 `requests`库的 `requests.request()` 方法发起HTTP请求。
    3.  处理 `requests` 库可能抛出的异常 (超时、连接错误等)，将其包装在 `LLMGatewayException` 中。
    4.  尝试将响应体解析为JSON。
    5.  若为错误响应且可解析，则填充 `LLMErrorDetails`。
    6.  返回 `LLMResponse` 对象。

### 5.7. `_extract_text_from_successful_llm_response` (Private Helper Method)
*   **签名:** `def _extract_text_from_successful_llm_response(self, llm_response: LLMResponse) -> str:`
*   **逻辑 (详细步骤参考 `ClassFunctionLogicDesign_ST018_v1.0` - Section 3.6):**
    1.  检查 `llm_response.parsed_body` 是否存在。
    2.  根据 `API_Detailed_Design_ST018_v1.0` 中描述的成功响应结构 (例如，chat completion的 `choices[0].message.content` 或 legacy completion的 `choices[0].text`) 提取文本内容。
    3.  若无法按已知模式提取，则抛出 `LLMGatewayException`。

## 6. 外部API交互规范 (External API Interaction Profile Summary)

(基于 `API_Detailed_Design_ST018_v1.0`)
*   **通信协议:** HTTPS
*   **数据格式:** JSON (`application/json`)
*   **交互模式:** 同步请求-响应 (非流式)
*   **认证:** HTTP `Authorization: Bearer <LLM_API_KEY>` 头部。API Key通过 `llm_config_dict` 提供。
*   **请求方法:** 通常为 `POST`。
*   **请求体:** 结构取决于LLM提供商 (例如，包含 `model`, `messages` 或 `prompt`, `temperature`, `max_tokens`, `stream: false`)。
*   **成功响应 (HTTP 200):** 解析JSON，提取主要文本内容。
*   **错误响应 (HTTP 4xx, 5xx):** 解析JSON错误结构 (若有)，填充 `LLMErrorDetails`。特别关注 `400, 401, 403, 429, 500, 503` 状态码。

## 7. 安全检查点 (Security Checkpoints)

(基于 `SecurityConsiderations_ST018_v1.0` 及项目安全前提)
实施以下检查点，**绝对优先保证功能成功和实现简易性**：

1.  **URL格式验证 (最小化):** 在 `_parse_and_validate_llm_call_config` 中，验证 `target_url` 以 `http://` 或 `https://` 开头。
2.  **参数类型验证:** 在 `_parse_and_validate_llm_call_config` 中，验证基本配置参数类型 (如 `api_key` 为字符串, `timeout_seconds` 为数字)。
3.  **禁止API Key日志记录:** 严格避免在日志中记录完整的API Key。若记录包含API Key的配置，必须进行掩码处理。
4.  **API Key内存处理:** API Key仅在内存中使用，服务本身不持久化存储。
5.  **HTTPS传输:** 确保 `requests` 库用于对 `https://` URL的调用。
6.  **结构化异常:** 使用自定义的 `LLMGatewayException` 包装内部错误。
7.  **向调用方传递充分信息:** 确保异常信息对调用Agent足够清晰，以便理解错误原因。
8.  **可配置超时:** 实现并使用 `LLMCallConfig` 中的 `timeout_seconds`。
9.  **有界重试:** 根据 `RetryConfig` 实现重试逻辑 (最大次数、固定延迟)。
10. **不设内部输入大小限制:** 网关本身不实现复杂的输入/输出大小限制逻辑。
11. **强制HTTPS (优先):** 主要针对 HTTPS URL。

## 8. 编码规范 (Coding Specifications)

**必须严格遵守项目提供的完整Python编码规范。** 关键点包括：
*   模块化设计 (职责单一, 高内聚低耦合)
*   配置管理 (使用Python dataclasses, 禁止硬编码和外部配置文件)
*   错误处理 (全面捕获, try-except, 清晰提示, 避免崩溃)
*   日志记录 (独立文件, 多级别, 详细信息含时间戳/模块/级别/函数/行号/trace)
*   代码风格 (4空格缩进, 行长<79字符, 驼峰类名, 下划线函数/变量/方法名, 大写常量, 描述性命名, 类型提示)
*   依赖管理 (优先Python标准库和高质量第三方库, 减少外部工具依赖)
*   代码复用 (抽象重复逻辑为函数/方法)
*   **禁止代码注释和DocString。**

## 9. `__init__.py` 文件内容

在 `multi_agent_system/llm_gateway/__init__.py` 中，包含以下内容以正确设置包并导出关键类：
```python
# multi_agent_system/llm_gateway/__init__.py
from .service import LLMInteractionGatewayService
from .data_models import LLMCallConfig, RetryConfig, LLMResponse, LLMErrorDetails
from .exceptions import LLMGatewayException

__all__ = [
    "LLMInteractionGatewayService",
    "LLMCallConfig",
    "RetryConfig",
    "LLMResponse",
    "LLMErrorDetails",
    "LLMGatewayException",
]
```

---
```

Next, I will generate the **精确指令文本 (Prompt)** for the "Roo code LLM".

## Output 2: 给代码生成LLM的精确指令文本 (Prompt for Code Generation LLM)

```text
You are an expert Python programmer tasked with implementing a software component according to very specific design documents. Your goal is to write clean, robust, and fully compliant Python code.

**Overall Task:**
Implement the `LLM_Interaction_Gateway_Service` for a multi-agent software development system. This service will act as a gateway to interact with external Large Language Model (LLM) APIs via HTTP calls.

**Project Root Directory (Conceptual):** `multi_agent_system/`

**Files to Create:**
You must create the following four Python files with the exact paths and content as specified below.

**File 1: `multi_agent_system/llm_gateway/exceptions.py`**

*   **Purpose:** Defines custom exceptions for the LLM gateway.
*   **Content:**
    ```python
    from typing import Optional, Any # Any is a placeholder for LLMErrorDetails type from data_models

    # In a real scenario, LLMErrorDetails would be imported like:
    # from .data_models import LLMErrorDetails

    class LLMGatewayException(Exception):
        """
        Base custom exception for errors originating within the LLMInteractionGatewayService.
        Allows callers to catch gateway-specific issues.
        """
        def __init__(self, 
                     message: str, 
                     original_exception: Optional[Exception] = None,
                     error_details: Optional[Any] = None, # Use 'Any' as LLMErrorDetails is in another file
                     status_code: Optional[int] = None) -> None:
            super().__init__(message)
            self.message: str = message
            self.original_exception: Optional[Exception] = original_exception
            self.error_details: Optional[Any] = error_details # Use 'Any'
            self.status_code: Optional[int] = status_code
    ```

**File 2: `multi_agent_system/llm_gateway/data_models.py`**

*   **Purpose:** Defines Python dataclasses for structuring configuration, requests, and responses.
*   **Content:**
    ```python
    from dataclasses import dataclass, field
    from typing import List, Optional, Dict, Any

    @dataclass
    class RetryConfig:
        """
        Configuration for retrying failed LLM API calls.
        """
        max_retries: int = 3
        delay_seconds: float = 5.0
        retryable_status_codes: List[int] = field(default_factory=lambda: [429, 500, 503])

    @dataclass
    class LLMCallConfig:
        """
        Structured configuration for an individual LLM API call.
        """
        target_url: str
        api_key: str 
        http_method: str = "POST"
        additional_headers: Dict[str, str] = field(default_factory=dict)
        retry_config: Optional[RetryConfig] = field(default_factory=RetryConfig)
        timeout_seconds: float = 60.0

    @dataclass
    class LLMErrorDetails:
        """
        Structured details of an error returned by an external LLM API.
        """
        message: str
        error_type: Optional[str] = None 
        param: Optional[str] = None
        code: Optional[str] = None

    @dataclass
    class LLMResponse:
        """
        Represents the comprehensive result of an HTTP call to an external LLM.
        """
        status_code: int
        body_text: str 
        headers: Dict[str, str] = field(default_factory=dict)
        parsed_body: Optional[Dict[str, Any]] = None 
        error_details: Optional[LLMErrorDetails] = None

        @property
        def is_successful(self) -> bool:
            """Determines if the HTTP call was successful (2xx status code)."""
            return 200 <= self.status_code < 300
    ```

**File 3: `multi_agent_system/llm_gateway/service.py`**

*   **Purpose:** Implements the `LLMInteractionGatewayService` class containing the core HTTP calling logic.
*   **Content:**
    ```python
    import time
    import logging
    import requests # External dependency, ensure it's available
    from typing import Dict, Any, Optional, Tuple, List # Standard typing

    # Local imports from within the llm_gateway package
    from .data_models import LLMCallConfig, RetryConfig, LLMResponse, LLMErrorDetails
    from .exceptions import LLMGatewayException

    class LLMInteractionGatewayService:
        """
        Service to abstract and manage interactions with external LLM APIs,
        including request construction, HTTP communication, response parsing,
        error handling, and retries.
        """

        def __init__(self, 
                     default_retry_config: Optional[RetryConfig] = None, 
                     logger: Optional[logging.Logger] = None) -> None:
            if default_retry_config is None:
                self.default_retry_config: RetryConfig = RetryConfig()
            else:
                self.default_retry_config: RetryConfig = default_retry_config
            
            if logger is None:
                self.logger: logging.Logger = logging.getLogger(__name__)
                # Basic config if no logger is passed and no root config exists
                if not self.logger.hasHandlers():
                    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            else:
                self.logger: logging.Logger = logger

        def _parse_and_validate_llm_call_config(self, 
                                                config_dict: Dict[str, Any]
                                                ) -> Tuple[LLMCallConfig, Dict[str, Any]]:
            self.logger.debug(f"Parsing LLM call config from dict: {list(config_dict.keys())}")
            
            required_keys: List[str] = ["target_url", "api_key", "model"]
            for key in required_keys:
                if key not in config_dict:
                    msg: str = f"Missing required LLM configuration key: '{key}'"
                    self.logger.error(msg)
                    raise LLMGatewayException(msg)

            target_url: str = config_dict["target_url"]
            api_key: str = config_dict["api_key"]

            if not isinstance(target_url, str) or not (target_url.startswith("http://") or target_url.startswith("https://")):
                msg = f"Invalid target_url: '{target_url}'. Must be a string starting with 'http://' or 'https://'."
                self.logger.error(msg)
                raise LLMGatewayException(msg)
            
            if not isinstance(api_key, str):
                msg = "Invalid api_key: Must be a string."
                self.logger.error(msg)
                raise LLMGatewayException(msg)

            http_method: str = config_dict.get("http_method", "POST")
            if not isinstance(http_method, str):
                 raise LLMGatewayException("http_method must be a string.")

            additional_headers: Dict[str, str] = config_dict.get("additional_headers", {})
            if not isinstance(additional_headers, dict):
                raise LLMGatewayException("additional_headers must be a dictionary.")

            timeout_seconds: float = float(config_dict.get("timeout_seconds", 60.0))
            if not isinstance(timeout_seconds, (int, float)) or timeout_seconds <= 0:
                raise LLMGatewayException("timeout_seconds must be a positive number.")

            retry_conf_dict: Optional[Dict[str, Any]] = config_dict.get("retry_config")
            parsed_retry_config: Optional[RetryConfig] = self.default_retry_config
            if retry_conf_dict is not None:
                if not isinstance(retry_conf_dict, dict):
                    raise LLMGatewayException("retry_config must be a dictionary if provided.")
                parsed_retry_config = RetryConfig(
                    max_retries=int(retry_conf_dict.get("max_retries", self.default_retry_config.max_retries)),
                    delay_seconds=float(retry_conf_dict.get("delay_seconds", self.default_retry_config.delay_seconds)),
                    retryable_status_codes=list(retry_conf_dict.get("retryable_status_codes", self.default_retry_config.retryable_status_codes))
                )
            
            llm_call_config_obj: LLMCallConfig = LLMCallConfig(
                target_url=target_url,
                api_key=api_key,
                http_method=http_method.upper(),
                additional_headers=additional_headers,
                retry_config=parsed_retry_config,
                timeout_seconds=timeout_seconds
            )

            specific_llm_params: Dict[str, Any] = {
                "model": config_dict["model"] # Already validated presence
            }
            if "temperature" in config_dict:
                specific_llm_params["temperature"] = float(config_dict["temperature"])
            if "max_tokens" in config_dict:
                specific_llm_params["max_tokens"] = int(config_dict["max_tokens"])
            
            # stream is always false for this service version
            specific_llm_params["stream"] = False 

            self.logger.debug("LLM call config parsed successfully.")
            return llm_call_config_obj, specific_llm_params

        def _prepare_request_payload(self, 
                                     prompt_data: Any, 
                                     specific_llm_params: Dict[str, Any]
                                     ) -> Dict[str, Any]:
            self.logger.debug("Preparing request payload.")
            payload: Dict[str, Any] = {}
            
            model: Optional[str] = specific_llm_params.get("model")
            if not model or not isinstance(model, str):
                msg: str = "LLM 'model' parameter is missing or invalid in specific_llm_params."
                self.logger.error(msg)
                raise LLMGatewayException(msg)
            payload["model"] = model

            if isinstance(prompt_data, list): # Assumed chat messages
                # Basic validation for chat messages structure
                for msg_item in prompt_data:
                    if not (isinstance(msg_item, dict) and "role" in msg_item and "content" in msg_item):
                        err_msg = "Invalid chat message structure in prompt_data list."
                        self.logger.error(err_msg)
                        raise LLMGatewayException(err_msg)
                payload["messages"] = prompt_data
            elif isinstance(prompt_data, str): # Assumed text completion prompt
                payload["prompt"] = prompt_data
            else:
                msg = f"Unsupported prompt_data type: {type(prompt_data)}. Must be str or list of message dicts."
                self.logger.error(msg)
                raise LLMGatewayException(msg)

            # Add other specific LLM parameters
            for key, value in specific_llm_params.items():
                if key not in payload: # model is already added
                    payload[key] = value
            
            payload["stream"] = False # Explicitly set stream to False

            # Avoid logging potentially large payloads directly, log keys instead
            self.logger.debug(f"Request payload prepared with keys: {list(payload.keys())}")
            return payload

        def _make_actual_http_request(self, 
                                      call_config: LLMCallConfig, 
                                      payload_dict: Dict[str, Any]
                                      ) -> LLMResponse:
            self.logger.info(f"Making HTTP {call_config.http_method} request to {call_config.target_url}")
            
            headers: Dict[str, str] = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "Authorization": f"Bearer {call_config.api_key}" # API Key Security: Handled in memory
            }
            if call_config.additional_headers:
                headers.update(call_config.additional_headers)

            http_lib_response: Optional[requests.Response] = None
            try:
                http_lib_response = requests.request(
                    method=call_config.http_method,
                    url=call_config.target_url,
                    headers=headers,
                    json=payload_dict,
                    timeout=call_config.timeout_seconds # Timeout Security Checkpoint
                )
                self.logger.debug(f"Received response with status: {http_lib_response.status_code}")

                parsed_body_json: Optional[Dict[str, Any]] = None
                llm_error_details_obj: Optional[LLMErrorDetails] = None
                
                try:
                    parsed_body_json = http_lib_response.json()
                    if not (200 <= http_lib_response.status_code < 300) and isinstance(parsed_body_json, dict):
                        error_data: Dict[str, Any] = parsed_body_json.get("error", {})
                        if isinstance(error_data, dict): # Ensure error_data is a dict
                             llm_error_details_obj = LLMErrorDetails(
                                message=str(error_data.get("message", http_lib_response.reason)),
                                error_type=str(error_data.get("type")) if error_data.get("type") is not None else None,
                                param=str(error_data.get("param")) if error_data.get("param") is not None else None,
                                code=str(error_data.get("code")) if error_data.get("code") is not None else None
                            )
                        else: # error field is not a dict
                            llm_error_details_obj = LLMErrorDetails(message=http_lib_response.text)
                except requests.exceptions.JSONDecodeError:
                    self.logger.warning("Failed to decode JSON response body from LLM.")
                    if not (200 <= http_lib_response.status_code < 300):
                         llm_error_details_obj = LLMErrorDetails(message=http_lib_response.text or http_lib_response.reason)


                return LLMResponse(
                    status_code=http_lib_response.status_code,
                    body_text=http_lib_response.text,
                    headers=dict(http_lib_response.headers),
                    parsed_body=parsed_body_json,
                    error_details=llm_error_details_obj
                )

            except requests.exceptions.Timeout as e:
                msg: str = f"HTTP request to {call_config.target_url} timed out after {call_config.timeout_seconds}s."
                self.logger.error(msg)
                raise LLMGatewayException(msg, original_exception=e)
            except requests.exceptions.ConnectionError as e:
                msg = f"HTTP connection error for {call_config.target_url}."
                self.logger.error(msg)
                raise LLMGatewayException(msg, original_exception=e)
            except requests.exceptions.RequestException as e:
                # Catch other requests-related errors
                msg = f"Generic HTTP request error for {call_config.target_url}: {str(e)}"
                self.logger.error(msg)
                status_code_from_exception: Optional[int] = None
                if hasattr(e, 'response') and e.response is not None:
                     status_code_from_exception = e.response.status_code
                raise LLMGatewayException(msg, original_exception=e, status_code=status_code_from_exception)

        def _extract_text_from_successful_llm_response(self, 
                                                       llm_response_obj: LLMResponse
                                                       ) -> str:
            self.logger.debug("Extracting text from successful LLM response.")
            if not llm_response_obj.parsed_body or not isinstance(llm_response_obj.parsed_body, dict):
                msg: str = "Cannot extract text, LLM response body not parsed or not a dictionary."
                self.logger.error(msg)
                raise LLMGatewayException(msg, status_code=llm_response_obj.status_code)

            parsed_json: Dict[str, Any] = llm_response_obj.parsed_body
            
            choices: Optional[List[Any]] = parsed_json.get("choices")
            if isinstance(choices, list) and len(choices) > 0:
                first_choice: Any = choices[0]
                if isinstance(first_choice, dict):
                    message: Optional[Dict[str, Any]] = first_choice.get("message")
                    if isinstance(message, dict) and "content" in message and isinstance(message["content"], str):
                        self.logger.debug("Extracted content from chat completion structure.")
                        return message["content"]
                    
                    text_content: Optional[str] = first_choice.get("text")
                    if isinstance(text_content, str): # Legacy completion style
                        self.logger.debug("Extracted content from legacy text completion structure.")
                        return text_content
            
            # Add other extraction patterns here if new LLM response structures are supported.

            msg = "Could not find expected text content in LLM response structure."
            self.logger.error(f"{msg} Response dump: {str(parsed_json)[:500]}") # Log part of response
            raise LLMGatewayException(msg, status_code=llm_response_obj.status_code)

        def execute_llm_call(self, 
                             prompt_data: Any, 
                             llm_config_dict: Dict[str, Any]
                             ) -> str:
            # Log entry, avoid logging full API key from llm_config_dict
            self.logger.info(f"Executing LLM call. Prompt data type: {type(prompt_data)}. Config keys: {list(llm_config_dict.keys())}")
            # API Key Security: Avoid direct logging of llm_config_dict if it contains sensitive data. Mask if necessary.
            # For this controlled environment, we assume higher-level logging might filter, or this is acceptable in isolation.

            current_llm_call_config: LLMCallConfig
            specific_llm_params: Dict[str, Any]
            try:
                current_llm_call_config, specific_llm_params = self._parse_and_validate_llm_call_config(llm_config_dict)
            except LLMGatewayException as e: # Catch parsing/validation errors
                self.logger.error(f"Configuration error for LLM call: {e.message}")
                raise # Re-raise immediately, no retry for config errors

            request_payload: Dict[str, Any] = self._prepare_request_payload(prompt_data, specific_llm_params)
            
            retries_attempted: int = 0
            last_llm_gateway_exception: Optional[LLMGatewayException] = None
            
            effective_retry_config: RetryConfig = current_llm_call_config.retry_config or self.default_retry_config
            # Ensure effective_retry_config is an instance, not None
            if not isinstance(effective_retry_config, RetryConfig): # Should not happen if defaults are set
                 effective_retry_config = self.default_retry_config


            while retries_attempted <= effective_retry_config.max_retries: # Bounded Retries Security Checkpoint
                self.logger.info(f"LLM call attempt {retries_attempted + 1} of {effective_retry_config.max_retries + 1}")
                
                try:
                    llm_response_obj: LLMResponse = self._make_actual_http_request(
                        current_llm_call_config, 
                        request_payload
                    )

                    if llm_response_obj.is_successful:
                        self.logger.info(f"LLM call successful with status: {llm_response_obj.status_code}")
                        extracted_text: str = self._extract_text_from_successful_llm_response(llm_response_obj)
                        self.logger.info("Successfully extracted text from LLM response.")
                        return extracted_text
                    else: # HTTP error from LLM
                        error_message: str = f"LLM API returned error status: {llm_response_obj.status_code}."
                        if llm_response_obj.error_details:
                            error_message += f" Details: {llm_response_obj.error_details.message}"
                        elif llm_response_obj.body_text:
                             error_message += f" Body: {llm_response_obj.body_text[:200]}" # Log snippet of body

                        self.logger.warning(error_message)
                        last_llm_gateway_exception = LLMGatewayException(
                            error_message, 
                            error_details=llm_response_obj.error_details,
                            status_code=llm_response_obj.status_code
                        )
                        if llm_response_obj.status_code not in effective_retry_config.retryable_status_codes:
                            self.logger.warning(f"Status code {llm_response_obj.status_code} is not retryable. Failing call.")
                            raise last_llm_gateway_exception 
                        # Else, if retryable, continue to retry logic below
                
                except LLMGatewayException as e: # Catches exceptions from _make_actual_http_request (e.g. timeout, connection)
                    self.logger.warning(f"LLM call attempt failed: {e.message}")
                    last_llm_gateway_exception = e
                    # For network/timeout errors, status_code might be None in exception.
                    # These are generally considered retryable unless max_retries is hit.
                    # If specific status codes from RequestException need to be non-retryable, add logic here.

                # If this point is reached, it means an error occurred (HTTP error or request exception)
                if retries_attempted >= effective_retry_config.max_retries:
                    self.logger.error("Max retries reached. LLM call failed.")
                    if last_llm_gateway_exception:
                        raise last_llm_gateway_exception
                    else: # Should not happen if logic is correct
                        raise LLMGatewayException("LLM call failed after max retries, but no specific exception was captured.")
                
                self.logger.info(f"Retrying LLM call in {effective_retry_config.delay_seconds} seconds...")
                time.sleep(effective_retry_config.delay_seconds)
                retries_attempted += 1
            
            # Fallback, should ideally be unreachable if loop logic is correct
            if last_llm_gateway_exception:
                raise last_llm_gateway_exception
            raise LLMGatewayException("LLM call failed after exhausting retries (fallback).")

    ```

**File 4: `multi_agent_system/llm_gateway/__init__.py`**

*   **Purpose:** Marks `llm_gateway` as a Python package and conveniently exports key classes.
*   **Content:**
    ```python
    # multi_agent_system/llm_gateway/__init__.py
    from .service import LLMInteractionGatewayService
    from .data_models import LLMCallConfig, RetryConfig, LLMResponse, LLMErrorDetails
    from .exceptions import LLMGatewayException

    __all__ = [
        "LLMInteractionGatewayService",
        "LLMCallConfig",
        "RetryConfig",
        "LLMResponse",
        "LLMErrorDetails",
        "LLMGatewayException",
    ]
    ```

**General Implementation Instructions:**

1.  **Strictly Adhere to Coding Specifications:**
    *   **Modular Design:** Ensure classes and methods have single responsibilities.
    *   **Configuration Management:** All data structures are Python dataclasses. No hardcoding of values that should be configurable (though for ST018, most config comes via `llm_config_dict`). No external config files.
    *   **Error Handling:** Use `try-except` blocks comprehensively. Raise `LLMGatewayException` for errors originating in this service, providing clear messages and context.
    *   **Logging:** Use the `logging` module extensively. Log entry/exit of methods, key decisions, errors, and successful operations. Ensure logs include timestamps, module name, level, function name, and line numbers (achieved by logger configuration).
    *   **Code Style:** 4-space indent, line length < 79 chars, CamelCase for classes, snake_case for functions/methods/variables, UPPER_SNAKE_CASE for constants. Use descriptive names. All functions and methods MUST have type hints for all arguments and return values.
    *   **Dependencies:** Only use Python standard library, `requests`, and the local modules defined here.
    *   **Code Reuse:** Abstract common logic into helper methods where appropriate.
    *   **NO COMMENTS OR DOCSTRINGS:** Per specification, do not include any `#` comments or `"""Docstrings"""` in the generated Python code files. The descriptions provided in this prompt are for your understanding only.

2.  **Implement Security Checkpoints:**
    *   URL Validation (minimal: starts with `http://` or `https://` in `_parse_and_validate_llm_call_config`).
    *   Parameter Type Validation (in `_parse_and_validate_llm_call_config`).
    *   No API Key Logging (ensure API keys from `llm_config_dict` are not logged directly).
    *   API Keys Handled In-Memory (no persistent storage by this service).
    *   HTTPS for Transit (ensure `requests` calls HTTPS URLs primarily).
    *   Structured Exceptions (`LLMGatewayException`).
    *   Sufficient Info to Caller (via exceptions).
    *   Configurable Timeouts (use `timeout_seconds` from `LLMCallConfig`).
    *   Bounded Retries (use `RetryConfig` for `max_retries` and `delay_seconds`).
    *   No Internal Input Size Limiting by this service.

3.  **Output:**
    *   Your output should be the raw content for these four complete Python files. Do not add any explanatory text outside of what's requested for the file contents.
    *   Ensure all imports are correct, including relative imports for local modules (e.g., `from .data_models import ...`).

This prompt provides all necessary details for you to generate the required Python code.
```
