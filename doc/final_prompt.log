# 原始用户需求描述


1.  **0.1 用户初始需求接收Agent (User Initial Request Receiver):**
    *   **任务:**
        1.  接收用户输入的原始自然语言需求文本。
        2.  将原始文本一字不差地记录下来。
        3.  将记录的原始需求文本传递给“项目知识库构建与管理Agent (PKBM)”进行存储，标记为“原始用户需求 v1.0”。
        4.  将记录的原始需求文本传递给“初步需求意图识别Agent”。
    *   **输入:** 用户提供的自然语言需求。
    *   **输出:** 原始需求文本的精确副本。

2.  **0.2 项目知识库构建与管理Agent (Project Knowledge Base Manager - PKBM):**
    *   **职责 (持续进行):**
        *   接收来自各个Agent的结构化信息和文档。
        *   为每条信息打上标签（如：`需求规格`、`架构决策`、`代码模块X`、`测试用例Y`、`人工审核意见Z`），并记录版本和时间戳。
        *   建立信息之间的关联（如：`代码模块X`实现了`需求点A`，`测试用例Y`用于验证`代码模块X`）。
        *   提供基于标签、关键词、关联性的精确信息检索服务给其他Agent。
    *   **输入/输出:** 持续接收和提供结构化数据。

3.  **0.3 初步需求意图识别Agent (Initial Intent Recognition Agent):**
    *   **任务:**
        1.  接收“原始用户需求文本”。
        2.  识别需求的核心动词和目标（例如：“创建”、“修改”、“修复”、“添加功能”）。
        3.  识别需求中提及的关键名词或实体（例如：“用户注册”、“购物车”、“API”、“数据库表”）。
        4.  输出一份包含“核心意图”和“关键实体列表”的初步分析报告。
        5.  将此报告存入PKBM，标记为“初步意图分析 v1.0”，并关联“原始用户需求 v1.0”。
        6.  将此报告传递给“项目类型与技术栈引导Agent”。
    *   **输入:** 原始用户需求文本。
    *   **输出:** 结构化的初步意图分析报告 (例如：`{ "intent": "create_new_feature", "entities": ["user_login", "oauth2"] }`)。

4.  **0.4 项目类型与技术栈引导Agent (Project Type & Tech Stack Elicitation Agent):**
    *   **任务:**
        1.  接收“初步意图分析报告”。
        2.  从PKBM调取项目历史信息（如果是已有项目）。
        3.  根据意图和实体，生成一系列针对性的问题，以获取或确认项目类型（Web应用、库、CLI工具等）、主要编程语言、核心框架、数据库类型等。问题示例：“您希望使用哪种编程语言来实现此功能？”“此项目是否基于特定的Web框架（如Flask, Django, Spring Boot）？”
        4.  通过“用户交互代理”（人工）向用户呈现这些问题。
        5.  接收用户的回答。
        6.  将问答记录和最终确认的项目类型及技术栈信息整理成结构化文档。
        7.  将此文档存入PKBM，标记为“项目元数据 v1.0”，并关联“初步意图分析 v1.0”。
        8.  将此文档传递给“详细需求澄清Agent”。
    *   **输入:** 初步意图分析报告，用户回答。
    *   **输出:** 结构化的项目元数据文档。

**阶段一：深度需求理解与规格化**

5.  **1.1 需求细节与边界条件挖掘Agent (Requirement Detail & Boundary Miner Agent):**
    *   **任务:**
        1.  接收“原始用户需求文本”和“项目元数据”。
        2.  从PKBM调取“初步意图分析报告”。
        3.  逐条分析原始需求中的每个句子/要点。
        4.  针对每个要点，生成旨在挖掘具体细节、非功能性需求（性能、易用性等）、边界条件、用户场景、预期行为和异常处理的问题。示例：“对于用户登录，密码错误次数达到多少次后应锁定账户？”“注册成功后，用户应被重定向到哪个页面？”“数据量预计有多大？”
        5.  将生成的问题列表通过“用户交互代理”呈现给用户。
        6.  接收用户的回答。
        7.  将问答记录整理并存入PKBM，标记为“需求澄清细节QA v1.0”。
        8.  将所有澄清后的细节传递给“用户故事生成Agent”和“验收标准定义Agent”。
    *   **输入:** 原始需求，项目元数据，用户回答。
    *   **输出:** 包含详细澄清问答的记录。

6.  **1.2 用户故事生成Agent (User Story Generation Agent):**
    *   **任务:**
        1.  接收来自“需求细节与边界条件挖掘Agent”的澄清后细节。
        2.  从PKBM调取“原始用户需求文本”和“项目元数据”。
        3.  根据“As a [type of user], I want [an action] so that [a benefit/value]”的模板，将澄清后的需求点转化为一系列用户故事。
        4.  确保每个用户故事都是独立的、可协商的、有价值的、可估计的、小的、可测试的（INVEST原则）。
        5.  输出用户故事列表（例如，Markdown或JSON格式）。
        6.  将用户故事列表存入PKBM，标记为“用户故事集 v1.0”。
        7.  将用户故事列表传递给“人工审核点1协调Agent”。
    *   **输入:** 澄清后的需求细节。
    *   **输出:** 结构化的用户故事列表。

7.  **1.3 验收标准定义Agent (Acceptance Criteria Definition Agent):**
    *   **任务:**
        1.  接收来自“需求细节与边界条件挖掘Agent”的澄清后细节。
        2.  从PKBM调取“用户故事集 v1.0”（由用户故事生成Agent刚生成）。
        3.  为每个用户故事，根据Gherkin语言格式 (Given-When-Then) 或其他清晰的、可验证的条件列表，定义详细的验收标准。
        4.  确保每个验收标准都是具体的、可衡量的、可实现的、相关的、有时间限制的（SMART原则，虽然时间限制可能由后续规划决定）。
        5.  输出与用户故事对应的验收标准集合。
        6.  将验收标准集合存入PKBM，标记为“验收标准集 v1.0”，并关联“用户故事集 v1.0”。
        7.  将验收标准集合传递给“人工审核点1协调Agent”。
    *   **输入:** 澄清后的需求细节，用户故事列表。
    *   **输出:** 结构化的验收标准集合。

8.  **1.4 需求规格文档编纂Agent (Requirements Specification Compiler Agent):**
    *   **任务:**
        1.  从PKBM调取“用户故事集 v1.0”和“验收标准集 v1.0”。
        2.  将用户故事和对应的验收标准整合成一份统一的、结构清晰的需求规格说明书文档 (例如，Markdown或PDF格式)。
        3.  文档应包含引言（项目目标简述）、用户故事列表、每个用户故事的详细验收标准。
        4.  将此文档存入PKBM，标记为“需求规格说明书 v1.0”。
        5.  将此文档传递给“人工审核点1协调Agent”。
    *   **输入:** 用户故事列表，验收标准集合。
    *   **输出:** 完整的需求规格说明书文档。

9.  **1.5 人工审核点1协调Agent (Human Review Point 1 Coordinator):**
    *   **任务:**
        1.  从PKBM接收“需求规格说明书 v1.0”（包含用户故事和验收标准）。
        2.  准备审核材料包，呈现给人工审核者（用户/产品经理）。
        3.  **审核要点清单（供人工参考）：**
            *   需求是否完整准确地反映了原始意图？
            *   用户故事是否清晰、独立、可测试？
            *   验收标准是否具体、可衡量，并能充分验证用户故事？
            *   是否存在遗漏的关键场景或边界条件？
            *   是否存在逻辑矛盾或不清晰之处？
        4.  接收人工审核结果（选项：批准通过 / 驳回并附带修改意见）。
        5.  如果“批准通过”：
            *   在PKBM中将“需求规格说明书 v1.0”标记为“已审核通过”。
            *   通知“系统架构方案提议Agent”开始工作。
        6.  如果“驳回”：
            *   将修改意见结构化记录，存入PKBM，标记为“需求审核反馈 v1.x”。
            *   根据反馈内容，决定是将任务重新路由回“需求细节与边界条件挖掘Agent”、“用户故事生成Agent”或“验收标准定义Agent”进行修改，并附上反馈意见。
    *   **输入:** 需求规格说明书，人工审核者的反馈。
    *   **输出:** 审核状态，（若驳回）结构化的修改意见。

**阶段二：架构设计与任务规划**
*(假设需求已通过审核)*

10. **2.1 技术选型建议Agent (Technology Selection Advisor Agent):**
    *   **任务:**
        1.  从PKBM调取“已审核通过的需求规格说明书 v1.0”和“项目元数据 v1.0”。
        2.  如果项目元数据中已明确技术栈，则跳过此步骤，直接将现有技术栈信息传递给后续Agent。
        3.  如果技术栈未定或需调整，根据需求（特别是NFRs，如性能、并发、安全性）、项目类型，生成一个包含2-3个候选技术栈（语言、框架、关键库、数据库）的建议列表。
        4.  为每个候选技术栈提供简要的优缺点分析、适用场景以及与项目需求的匹配度说明。
        5.  输出技术选型建议报告。
        6.  将报告存入PKBM，标记为“技术选型建议 v1.0”。
        7.  通过“用户交互代理”将建议报告提交给用户（或技术负责人）进行选择。
        8.  接收用户的最终技术选型决定。
        9.  将最终选定的技术栈更新到PKBM的“项目元数据”中（或创建新版本）。
        10. 将确认的技术栈信息传递给“高层组件划分Agent”。
    *   **输入:** 需求规格，项目元数据，用户选择。
    *   **输出:** 技术选型建议报告，更新后的项目元数据。

11. **2.2 高层组件划分Agent (High-Level Component Definition Agent):**
    *   **任务:**
        1.  从PKBM调取“已审核通过的需求规格说明书 v1.0”和已确认的“项目元数据”（含技术栈）。
        2.  根据需求和技术栈，识别出系统需要的主要高层模块/服务/组件（例如：用户认证服务、订单处理模块、产品目录API、数据持久化层、前端UI）。
        3.  为每个组件初步定义其核心职责。
        4.  输出高层组件列表及其职责描述。
        5.  将此列表存入PKBM，标记为“高层组件定义 v1.0”。
        6.  将此列表传递给“组件间接口草拟Agent”。
    *   **输入:** 需求规格，项目元数据。
    *   **输出:** 高层组件列表及职责描述。

12. **2.3 组件间接口草拟Agent (Component Interface Draft Agent):**
    *   **任务:**
        1.  从PKBM调取“高层组件定义 v1.0”。
        2.  分析组件间的依赖关系和数据流。
        3.  为需要交互的组件之间草拟初步的接口定义（例如：REST API端点、函数签名、消息队列主题和消息格式）。仅关注接口名称、主要参数和预期返回类型，无需详细数据结构。
        4.  输出组件间接口草案。
        5.  将草案存入PKBM，标记为“组件接口草案 v1.0”。
        6.  将草案传递给“架构图描述生成Agent”。
    *   **输入:** 高层组件定义。
    *   **输出:** 组件间接口草案。

13. **2.4 架构图描述生成Agent (Architecture Diagram Description Agent):**
    *   **任务:**
        1.  从PKBM调取“高层组件定义 v1.0”和“组件接口草案 v1.0”。
        2.  生成一段自然语言描述，该描述能够清晰地表达系统的高层架构，包括主要组件、它们之间的关系以及关键接口。（目标是让一个LLM能够基于此描述生成一个架构图，或让人能理解架构）。
        3.  （可选，如果LLM能力支持）尝试生成PlantUML或Mermaid等文本格式的图表描述。
        4.  输出架构图的文本描述。
        5.  将描述存入PKBM，标记为“架构图文本描述 v1.0”。
        6.  将描述传递给“架构设计文档编纂Agent”。
    *   **输入:** 高层组件定义，组件接口草案。
    *   **输出:** 架构图的文本描述。

14. **2.5 架构设计文档编纂Agent (Architecture Design Compiler Agent):**
    *   **任务:**
        1.  从PKBM调取“技术选型最终决定”（来自项目元数据）、“高层组件定义 v1.0”、“组件接口草案 v1.0”、“架构图文本描述 v1.0”。
        2.  将以上信息整合成一份高层架构设计文档。
        3.  文档应包含：技术选型及其理由、组件列表及其职责、组件间主要接口、架构图（或其文本描述）。
        4.  将文档存入PKBM，标记为“高层架构设计文档 v1.0”。
        5.  将文档传递给“人工审核点2协调Agent”。
    *   **输入:** 各架构设计元素。
    *   **输出:** 高层架构设计文档。

15. **2.6 人工审核点2协调Agent (Human Review Point 2 Coordinator):**
    *   **任务:**
        1.  从PKBM接收“高层架构设计文档 v1.0”。
        2.  准备审核材料包，呈现给人工审核者（技术负责人/架构师）。
        3.  **审核要点清单（供人工参考）：**
            *   技术选型是否合理、可行、符合项目需求和长期目标？
            *   组件划分是否清晰、职责是否单一、内聚是否高？
            *   组件间接口设计是否满足功能需求，是否考虑了可扩展性和可维护性？
            *   架构是否能支持需求的非功能性要求（性能、安全、伸缩性等）？
            *   是否存在明显的设计缺陷或风险点？
        4.  接收人工审核结果（批准通过 / 驳回并附带修改意见）。
        5.  如果“批准通过”：
            *   在PKBM中将“高层架构设计文档 v1.0”标记为“已审核通过”。
            *   通知“编码任务分解Agent”开始工作。
        6.  如果“驳回”：
            *   将修改意见结构化记录，存入PKBM，标记为“架构审核反馈 v1.x”。
            *   根据反馈，可能需要重新路由到“技术选型建议Agent”、“高层组件划分Agent”等。
    *   **输入:** 高层架构设计文档，人工审核反馈。
    *   **输出:** 审核状态，（若驳回）结构化的修改意见。

16. **2.7 编码任务分解Agent (Coding Task Breakdown Agent):**
    *   **任务:**
        1.  从PKBM调取“已审核通过的需求规格说明书 v1.0”和“已审核通过的高层架构设计文档 v1.0”。
        2.  将每个用户故事/功能点映射到架构中的一个或多个组件。
        3.  将大的功能点或组件实现进一步分解为更小的、可独立开发和测试的编码子任务（例如：实现用户模型的CRUD操作、设计登录API的请求/响应结构、编写密码加密工具类）。
        4.  为每个子任务明确其目标、预期产出（如某个类的实现、某个API端点）。
        5.  输出编码子任务列表。
        6.  将列表存入PKBM，标记为“编码子任务列表 v1.0”。
        7.  将列表传递给“任务依赖与顺序规划Agent”。
    *   **输入:** 需求规格，架构设计。
    *   **输出:** 编码子任务列表。

17. **2.8 任务依赖与顺序规划Agent (Task Dependency & Sequencing Agent):**
    *   **任务:**
        1.  从PKBM调取“编码子任务列表 v1.0”。
        2.  分析子任务之间的逻辑依赖关系（例如，必须先实现用户模型，才能实现用户注册API）。
        3.  根据依赖关系，确定一个合理的开发顺序或并行分组。
        4.  输出带有依赖关系和建议执行顺序的编码任务计划（例如，有向无环图描述或带优先级的列表）。
        5.  将计划存入PKBM，标记为“编码任务计划 v1.0”。
        6.  后续流程将按此计划逐个处理子任务。
    *   **输入:** 编码子任务列表。
    *   **输出:** 带有依赖和顺序的编码任务计划。

**阶段三：详细设计 (针对每个编码子任务循环)**
*(协调器会根据“编码任务计划 v1.0”逐个启动以下流程)*

18. **3.1 当前子任务上下文准备Agent (Current Subtask Context Preparer):**
    *   **任务:**
        1.  从“编码任务计划 v1.0”中获取当前待执行的子任务。
        2.  从PKBM调取与此子任务相关的全部上下文：
            *   对应的用户故事和验收标准。
            *   相关的架构设计部分（组件职责、接口定义）。
            *   项目元数据（技术栈、编码规范）。
            *   （如果是修改任务）现有代码库中相关的代码片段或文件结构（可能需要人工辅助定位并提供给PKBM）。
        3.  将这些上下文打包，传递给后续的详细设计Agent。
    *   **输入:** 当前子任务ID，PKBM。
    *   **输出:** 当前子任务的完整上下文包。

19. **3.2 API详细设计Agent (API Detailed Design Agent):** *(如果子任务涉及API)*
    *   **任务:**
        1.  接收“当前子任务上下文包”。
        2.  根据子任务需求和架构中的接口草案，详细设计API：
            *   精确的URL路径和HTTP方法。
            *   请求头、路径参数、查询参数的详细定义。
            *   请求体的JSON/XML Schema (或等效的详细结构描述)。
            *   响应体的JSON/XML Schema (或等效的详细结构描述) for 各种状态码 (200, 201, 400, 401, 403, 500等)。
            *   认证和授权机制说明。
        3.  输出API详细设计文档（例如，OpenAPI/Swagger片段或Markdown格式的详细描述）。
        4.  将文档存入PKBM，标记为“API详细设计_[子任务ID]_v1.0”。
    *   **输入:** 子任务上下文包。
    *   **输出:** API详细设计文档。

20. **3.3 数据模型详细设计Agent (Data Model Detailed Design Agent):** *(如果子任务涉及数据持久化或复杂数据结构)*
    *   **任务:**
        1.  接收“当前子任务上下文包”。
        2.  设计或更新数据库表结构：
            *   表名、列名、数据类型、约束（主键、外键、非空、唯一、检查约束）。
            *   索引建议。
            *   表间关系。
        3.  或设计程序内部使用的数据对象/类的属性和类型。
        4.  输出数据模型详细设计（例如，SQL DDL语句、类图的文本描述、JSON Schema）。
        5.  将设计存入PKBM，标记为“数据模型详细设计_[子任务ID]_v1.0”。
    *   **输入:** 子任务上下文包。
    *   **输出:** 数据模型详细设计。

21. **3.4 类与函数逻辑设计Agent (Class & Function Logic Design Agent):** *(如果子任务涉及具体业务逻辑实现)*
    *   **任务:**
        1.  接收“当前子任务上下文包”。
        2.  确定需要创建或修改的类和函数。
        3.  为每个关键类/函数：
            *   定义其职责。
            *   设计其公共接口（方法签名：名称、参数类型、返回类型）。
            *   用伪代码或详细的步骤描述其核心实现逻辑，包括主要算法、条件分支、循环、异常处理逻辑。
        4.  输出类与函数逻辑设计文档。
        5.  将文档存入PKBM，标记为“类函数逻辑设计_[子任务ID]_v1.0”。
    *   **输入:** 子任务上下文包。
    *   **输出:** 类与函数逻辑设计文档。

22. **3.5 文件与目录结构规划Agent (File & Directory Structure Planner Agent):**
    *   **任务:**
        1.  接收“当前子任务上下文包”。
        2.  从PKBM调取项目现有的（如果适用）文件结构和编码规范中关于结构的部分。
        3.  根据子任务的性质以及相关的API设计、数据模型设计、类函数设计，规划：
            *   需要创建的新文件名及其在项目中的完整路径。
            *   需要修改的现有文件名及其路径。
            *   是否需要创建新的目录。
        4.  确保规划的结构符合项目技术栈的最佳实践和编码规范。
        5.  输出文件和目录结构变更计划。
        6.  将计划存入PKBM，标记为“文件结构计划_[子任务ID]_v1.0”。
    *   **输入:** 子任务上下文包，相关详细设计。
    *   **输出:** 文件和目录结构变更计划。

23. **3.6 安全考量与检查点定义Agent (Security Considerations & Checkpoint Definer Agent):**
    *   **任务:**
        1.  接收“当前子任务上下文包”以及该子任务相关的所有详细设计文档（API、数据、逻辑）。
        2.  从PKBM调取项目安全规范、已知的安全漏洞模式 (如OWASP Top 10)。
        3.  针对当前子任务的设计，识别潜在的安全风险点（例如：SQL注入、XSS、CSRF、不安全的API暴露、敏感数据处理不当、权限控制缺失等）。
        4.  为每个风险点，提出具体的安全加固建议或必须在代码实现中包含的安全检查点/措施。
        5.  输出安全考量报告及检查点列表。
        6.  将报告存入PKBM，标记为“安全考量_[子任务ID]_v1.0”。
    *   **输入:** 子任务上下文包，所有相关详细设计。
    *   **输出:** 安全考量报告及检查点列表。

24. **3.7 详细设计汇总与指令生成Agent (Detailed Design Aggregation & Instruction Generator Agent):**
    *   **任务:**
        1.  接收当前子任务的所有详细设计文档（API、数据、逻辑、文件结构、安全考量）。
        2.  将这些设计整合成一份针对该子任务的《代码实现指导书》。
        3.  基于《代码实现指导书》，生成给“代码生成/修改Agent (CGMA)”或外部“Roo code LLM”的**极其具体、包含完整上下文的指令文本 (Prompt)**。该指令必须：
            *   明确是创建新文件还是修改现有文件（并提供完整路径）。
            *   如果修改，必须提供**原始代码的精确片段**（人工或辅助Agent从代码库获取并提供给PKBM）。
            *   清晰描述要实现的**功能逻辑、算法步骤、API行为、数据结构**。
            *   包含所有必要的**类名、函数签名、变量名约定**。
            *   强调必须遵循的**编码规范**（从PKBM调取）。
            *   列出必须实现的**安全检查点**。
            *   提供必要的**上下文代码片段**（如import语句、父类定义等，帮助LLM理解环境）。
            *   指定预期的**输出代码格式**（例如，一个完整的Python文件，或一个Java类中的特定方法）。
        4.  将《代码实现指导书》存入PKBM，标记为“实现指导书_[子任务ID]_v1.0”。
        5.  将生成的精确指令文本传递给“人工审核点3协调Agent”。
    *   **输入:** 子任务的所有详细设计文档，PKBM（编码规范、原始代码片段等）。
    *   **输出:** 《代码实现指导书》，给代码生成LLM的精确指令文本。

25. **3.8 人工审核点3协调Agent (Human Review Point 3 Coordinator):**
    *   **任务:**
        1.  接收为代码生成LLM准备的“精确指令文本”和对应的《代码实现指导书_[子任务ID]_v1.0》。
        2.  准备审核材料包，呈现给人工审核者（通常是开发人员）。
        3.  **审核要点清单（供人工参考）：**
            *   指令是否清晰、无歧义、完整？
            *   指令是否准确反映了《代码实现指导书》的所有要求？
            *   指令中包含的上下文信息（原始代码、规范、安全点）是否充分且正确？
            *   预期的代码输出描述是否明确？
            *   是否存在任何可能导致LLM误解或生成错误代码的表述？
        4.  接收人工审核结果（批准执行 / 驳回并附带修改指令的建议）。
        5.  如果“批准执行”：
            *   在PKBM中将“精确指令文本_[子任务ID]_v1.0”标记为“已审核待执行”。
            *   将指令文本传递给“代码生成/修改执行协调Agent”。
        6.  如果“驳回”：
            *   将修改建议存入PKBM。
            *   将任务路由回“详细设计汇总与指令生成Agent”进行修改。
    *   **输入:** 精确指令文本，实现指导书，人工审核反馈。
    *   **输出:** 审核状态，（若批准）待执行的指令文本。

**阶段四：代码生成与初步校验 (通过内部或外部LLM，针对每个子任务)**

26. **4.1 代码生成/修改执行协调Agent (CGMA Execution Coordinator):**
    *   **任务:**
        1.  接收“已审核待执行的精确指令文本_[子任务ID]_v1.0”。
        2.  **[手动交互点]** 将此指令文本复制粘贴到您指定的“Roo code LLM”或其他代码生成工具的输入界面。
        3.  触发“Roo code LLM”执行。
        4.  **[手动交互点]** 从“Roo code LLM”获取其生成的代码文本（或错误信息/状态）。
        5.  将“Roo code LLM”的原始输出（代码或错误信息）记录到PKBM，标记为“CGMA原始输出_[子任务ID]_v1.0”。
        6.  如果输出是代码，则将其传递给“代码基本语法与格式校验Agent”。如果输出是错误，则记录错误并可能需要人工介入或回溯。
    *   **输入:** 审核通过的精确指令文本。
    *   **输出:** CGMA生成的原始代码或错误信息。

27. **4.2 代码基本语法与格式校验Agent (Code Syntax & Basic Formatting Validator Agent):**
    *   **任务:**
        1.  接收“CGMA原始输出_[子任务ID]_v1.0”（假设是代码）。
        2.  从PKBM调取项目编码规范中关于格式化的部分和语言版本。
        3.  使用Linter（如ESLint, Pylint, Checkstyle）或编译器对代码进行基本的语法检查。
        4.  （可选，若LLM支持）尝试自动修复简单的格式问题。
        5.  输出校验结果报告（语法错误列表、格式问题列表）和（可能）初步格式化后的代码。
        6.  将校验报告和格式化代码存入PKBM，标记为“代码初步校验报告_[子任务ID]_v1.0”和“初步格式化代码_[子任务ID]_v1.0”。
        7.  如果存在严重语法错误导致无法进行后续步骤，标记问题并可能需要回溯到指令生成或人工介入。否则，将初步格式化代码传递给“静态代码分析Agent”。
    *   **输入:** CGMA原始输出代码。
    *   **输出:** 初步校验报告，初步格式化代码。

28. **4.3 静态代码分析Agent (Static Code Analyzer Agent):**
    *   **任务:**
        1.  接收“初步格式化代码_[子任务ID]_v1.0”。
        2.  从PKBM调取项目配置的静态分析规则集（例如，SonarQube规则子集、FindBugs规则）。
        3.  运行更深入的静态分析工具，检查潜在bug、代码异味、复杂度过高、未使用的变量/导入、简单的安全漏洞模式（如硬编码密码）。
        4.  输出详细的静态分析报告，包含问题描述、位置、严重级别和建议修复。
        5.  将报告存入PKBM，标记为“静态分析报告_[子任务ID]_v1.0”。
        6.  将报告和代码传递给“AI代码审查员Agent”。
    *   **输入:** 初步格式化代码。
    *   **输出:** 详细静态分析报告。

**阶段五：深度审查、测试与迭代 (针对每个子任务的产出)**

29. **5.1 AI代码审查员Agent (AI Code Reviewer Agent):**
    *   **任务:**
        1.  接收“初步格式化代码_[子任务ID]_v1.0”和“静态分析报告_[子任务ID]_v1.0”。
        2.  从PKBM调取该子任务的《实现指导书_[子任务ID]_v1.0》（包含所有设计要求、安全检查点）和“编码规范”。
        3.  **核心审查任务：**
            *   **功能符合性：** 代码是否实现了《实现指导书》中描述的所有功能点和逻辑？
            *   **设计遵循性：** 代码结构、类/函数设计是否与详细设计一致？API实现是否符合API设计？
            *   **规范遵循性：** 代码是否严格遵守了编码规范（命名、注释、风格等）？
            *   **可读性与可维护性：** 代码是否清晰易懂？是否存在过于复杂或难以维护的结构？
            *   **错误处理：** 是否恰当地处理了预期的异常和边界条件？
            *   **性能初步评估：** 是否存在明显的性能瓶颈（如不必要的循环、低效算法）？
            *   **安全性：** 是否落实了《实现指导书》中要求的安全检查点？是否存在静态分析未发现的、基于上下文的潜在安全问题？
        4.  针对每个发现的问题，提供具体的描述、代码位置、以及修改建议。
        5.  输出AI代码审查报告。
        6.  将报告存入PKBM，标记为“AI代码审查报告_[子任务ID]_v1.0”。
        7.  将报告、代码、静态分析报告传递给“人工审核点4协调Agent”。
    *   **输入:** 代码，静态分析报告，实现指导书，编码规范。
    *   **输出:** AI代码审查报告。

30. **5.2 人工审核点4协调Agent (Human Review Point 4 Coordinator - Code & AI Review):**
    *   **任务:**
        1.  接收“初步格式化代码_[子任务ID]_v1.0”、“静态分析报告_[子任务ID]_v1.0”和“AI代码审查报告_[子任务ID]_v1.0”。
        2.  准备审核材料包，呈现给人工审核者（开发人员/技术组长）。
        3.  **审核要点清单（供人工参考）：**
            *   AI生成的代码质量如何？是否基本可用？
            *   静态分析报告中的关键问题是否需要处理？
            *   AI代码审查报告是否准确？其提出的问题和建议是否合理？
            *   是否存在AI未能发现的关键问题（逻辑错误、深层设计缺陷、安全漏洞）？
            *   代码是否真正符合原始需求和设计意图？
        4.  接收人工审核结果（选项：批准进入测试 / 驳回并附带修改意见 / 直接手动修改代码并提交修改后版本）。
        5.  如果“批准进入测试”：
            *   在PKBM中将“初步格式化代码_[子任务ID]_v1.0”标记为“代码审核通过”。
            *   通知“单元测试用例生成Agent”开始工作。
        6.  如果“驳回”：
            *   将修改意见（针对代码或针对AI审查报告）结构化记录，存入PKBM。
            *   根据反馈，决定是重新生成指令（回溯到3.7）、要求CGMA重新生成（回溯到4.1）、或由AI代码审查员Agent尝试根据反馈生成修复建议。
        7.  如果“人工手动修改”：
            *   人工将修改后的代码提供给协调Agent。
            *   协调Agent将新代码存入PKBM，标记为“人工修正代码_[子任务ID]_v1.0”，并将其视为“代码审核通过”，通知测试。
    *   **输入:** 代码，各类报告，人工审核反馈/修改后的代码。
    *   **输出:** 审核状态，（若批准）进入测试的代码版本。

31. **5.3 单元测试用例生成Agent (Unit Test Case Generator Agent):**
    *   **任务:**
        1.  接收“代码审核通过”状态的代码（无论是AI生成后通过，还是人工修正后通过）。
        2.  从PKBM调取该子任务的《实现指导书》（特别是验收标准部分）、“类函数逻辑设计”、“API详细设计”以及项目使用的测试框架信息和测试编码规范。
        3.  针对代码中的每个主要函数/方法/类/API端点：
            *   生成覆盖正常路径的测试用例。
            *   生成覆盖已知边界条件的测试用例。
            *   生成覆盖预期异常处理的测试用例。
            *   确保测试用例的断言与《实现指导书》中的预期行为和验收标准一致。
        4.  输出单元测试用例代码文件（符合项目测试框架的格式）。
        5.  将测试用例代码存入PKBM，标记为“单元测试代码_[子任务ID]_v1.0”。
        6.  将测试用例代码和被测代码一起传递给“测试执行协调Agent”。
    *   **输入:** 审核通过的代码，实现指导书，设计文档，测试框架信息。
    *   **输出:** 单元测试用例代码。

32. **5.4 测试执行协调Agent (Test Execution Coordinator):**
    *   **任务:**
        1.  接收被测代码和“单元测试代码_[子任务ID]_v1.0”。
        2.  **[手动交互点]** 协助用户将单元测试代码集成到本地项目的测试套件中。
        3.  **[手动交互点]** 指导用户在本地环境中运行这些单元测试。
        4.  **[手动交互点]** 从用户处获取测试执行结果（例如，测试框架输出的报告文本，包含通过/失败数量、失败用例的名称、错误信息和堆栈跟踪）。
        5.  将测试结果报告原文存入PKBM，标记为“单元测试结果原始报告_[子任务ID]_v1.0”。
        6.  将报告传递给“测试结果分析与调试指引Agent”。
    *   **输入:** 被测代码，单元测试代码，用户提供的测试结果。
    *   **输出:** 单元测试结果原始报告。

33. **5.5 测试结果分析与调试指引Agent (Test Result Analyzer & Debugging Guide Agent):**
    *   **任务:**
        1.  接收“单元测试结果原始报告_[子任务ID]_v1.0”。
        2.  从PKBM调取相关的被测代码、单元测试代码、《实现指导书》。
        3.  分析测试报告：
            *   如果所有测试通过：在PKBM中标记子任务状态为“测试通过”，通知“子任务完成与集成准备Agent”。
            *   如果存在失败的测试用例：
                *   针对每个失败的用例，提取错误信息和堆栈跟踪。
                *   尝试将错误定位到被测代码或测试用例代码的具体行号。
                *   分析失败的可能原因（例如：逻辑错误、边界条件处理不当、测试用例断言错误、环境问题）。
                *   生成一份调试分析报告，包含：失败的测试列表、每个失败的详细错误、初步的原因分析、以及建议的调试步骤或修改方向（是修改被测代码还是修改测试用例）。
        4.  将调试分析报告（如果测试失败）存入PKBM，标记为“调试分析报告_[子任务ID]_v1.0”。
        5.  将报告传递给“人工审核点5协调Agent”。
    *   **输入:** 测试结果报告，相关代码和设计。
    *   **输出:** 子任务状态（测试通过/失败），（若失败）调试分析报告。

34. **5.6 人工审核点5协调Agent (Human Review Point 5 Coordinator - Debugging & Iteration):**
    *   **任务:**
        1.  接收“调试分析报告_[子任务ID]_v1.0”（如果测试失败）。
        2.  准备审核材料包（报告、相关代码），呈现给人工审核者（开发人员）。
        3.  **审核要点清单（供人工参考）：**
            *   AI对测试失败原因的分析是否准确？
            *   AI提出的调试建议或修改方向是否合理？
            *   确定问题根源是在产品代码、测试代码还是设计本身？
        4.  接收人工审核决策和指令（例如：确认是产品代码bug，请AI尝试修复；确认是测试用例问题，请AI修改测试；确认是设计问题，需要回溯到设计阶段；或人工直接修复并提交新代码/测试）。
        5.  如果需要AI修复/修改：
            *   将人工确认的问题和修改方向传递给“代码/测试修复指令生成Agent”。
        6.  如果人工直接修复：
            *   接收人工修改后的代码/测试，存入PKBM，然后重新触发“测试执行协调Agent”（返回5.4）。
        7.  如果需要回溯设计：
            *   在PKBM中记录问题，并通知中央协调器进行流程回溯。
    *   **输入:** 调试分析报告，人工反馈和指令。
    *   **输出:** 下一步行动指令（AI修复、人工修复、回溯）。

35. **5.7 代码/测试修复指令生成Agent (Code/Test Remediation Instruction Agent):**
    *   **任务:**
        1.  接收来自人工确认的“调试分析报告”和修复方向/要求。
        2.  从PKBM调取相关的代码（产品代码或测试代码）。
        3.  生成一条**极其具体**的指令给“代码生成/修改Agent (CGMA)”或外部“Roo code LLM”，用于修复已识别的bug或修改测试用例。指令应包含：
            *   要修改的文件名和精确的代码片段。
            *   清晰描述要修正的逻辑或行为。
            *   预期的正确行为。
        4.  将修复指令存入PKBM，标记为“修复指令_[子任务ID]_v1.x”。
        5.  将指令传递给“代码生成/修改执行协调Agent”（返回4.1），开始新一轮的代码生成->校验->审查->测试循环。
    *   **输入:** 确认的调试分析和修复要求，相关代码。
    *   **输出:** 给CGMA的精确修复指令。

**阶段六：集成、文档、交付与学习 (当一个或多个相关子任务“测试通过”后)**

36. **6.1 子任务完成与集成准备Agent (Subtask Completion & Integration Prep Agent):**
    *   **任务:**
        1.  当一个子任务在PKBM中被标记为“测试通过”时被激活。
        2.  检查“编码任务计划 v1.0”中该子任务的依赖关系。
        3.  如果所有前置依赖任务也都“测试通过”，则将此子任务的最终代码（在PKBM中标记为“最终版本代码_[子任务ID]”）标记为“待集成”。
        4.  通知“代码集成协调Agent”有一批新的“待集成”代码。
    *   **输入:** 子任务状态更新，编码任务计划。
    *   **输出:** “待集成”代码列表。

37. **6.2 代码集成协调Agent (Code Integration Coordinator):**
    *   **任务:**
        1.  接收一批“待集成”的代码模块/文件。
        2.  **[手动交互点]** 协助用户将这些新代码或修改后的代码合并到项目的主开发分支（或指定的目标分支）。
        3.  **[手动交互点]** 协助用户解决可能出现的合并冲突。
        4.  记录集成操作（哪些文件被合并，是否有冲突及如何解决）到PKBM，标记为“集成日志_[批次ID]_v1.0”。
        5.  集成完成后，通知“构建与冒烟测试Agent”。
    *   **输入:** 待集成代码列表，用户操作。
    *   **输出:** 集成操作日志。

38. **6.3 构建与冒烟测试Agent (Build & Smoke Test Agent):**
    *   **任务:**
        1.  在代码集成后被激活。
        2.  从PKBM调取项目构建脚本（pom.xml, package.json等）和构建说明。
        3.  **[手动交互点]** 协助用户执行完整的项目构建过程。
        4.  如果构建成功，**[手动交互点]** 协助用户执行一小组核心功能的快速冒烟测试（测试场景可能需要预先定义或由LLM根据需求和架构生成一个“冒烟测试计划”）。
        5.  收集构建结果（成功/失败，错误日志）和冒烟测试结果。
        6.  将结果存入PKBM，标记为“构建日志_[批次ID]_v1.0”和“冒烟测试报告_[批次ID]_v1.0”。
        7.  如果构建或冒烟测试失败，标记问题，可能需要回溯到代码修复或集成步骤。
        8.  如果通过，通知“技术文档更新Agent”和“最终人工验收协调Agent”。
    *   **输入:** 项目代码，构建脚本，（可选）冒烟测试计划，用户操作结果。
    *   **输出:** 构建和冒烟测试结果。

39. **6.4 技术文档更新Agent - 代码注释 (Technical Documentation Updater - Code Comments):**
    *   **任务:**
        1.  接收“构建与冒烟测试通过”的信号和涉及的已集成代码模块列表。
        2.  从PKBM调取这些代码模块的《实现指导书》和“编码规范”（关于注释的部分）。
        3.  为代码中的主要类、函数/方法、复杂逻辑块生成符合规范的、解释性的注释。
        4.  输出带有新增/更新注释的代码版本。
        5.  **[手动交互点]** 协助用户将这些注释应用到实际代码中。
        6.  将更新后的代码（带注释）在PKBM中标记为最终版本。
    *   **输入:** 已集成代码，实现指导书，编码规范。
    *   **输出:** 带注释的代码。

40. **6.5 技术文档更新Agent - API文档与README (Technical Documentation Updater - API Docs & README):**
    *   **任务:**
        1.  接收“构建与冒烟测试通过”的信号。
        2.  从PKBM调取所有相关的“API详细设计文档”、“高层架构设计文档”、“需求规格说明书”以及项目现有的API文档（如Swagger/OpenAPI文件）和README文件。
        3.  根据最新的代码和设计，更新或生成API文档。
        4.  更新项目README文件，包含新功能说明、安装/使用指南变更等。
        5.  生成本次迭代/变更的Changelog条目。
        6.  将更新后的文档（API文档、README、Changelog）存入PKBM。
    *   **输入:** 所有相关设计和需求文档，现有文档。
    *   **输出:** 更新后的API文档、README、Changelog。

41. **6.6 人工审核点6协调Agent (Human Review Point 6 Coordinator - Final Acceptance):**
    *   **任务:**
        1.  在构建和冒烟测试通过，且文档初步更新后被激活。
        2.  准备最终验收材料包：
            *   原始需求规格说明书。
            *   最终集成并测试通过的代码（或其访问方式）。
            *   更新后的技术文档（API文档、README）。
            *   冒烟测试报告。
            *   Changelog。
        3.  呈现给最终验收人（用户/产品负责人/测试负责人）。
        4.  **验收要点清单（供人工参考）：**
            *   交付的功能是否完整满足原始需求和验收标准？
            *   系统在冒烟测试中表现是否稳定？
            *   文档是否清晰、准确、完整？
            *   是否有任何未解决的重大问题或遗漏？
        5.  接收人工验收结果（批准发布 / 附条件批准 / 拒绝并说明原因）。
        6.  将验收结果和任何反馈记录到PKBM，标记为“最终验收报告_v1.0”。
        7.  如果“批准发布”，则流程结束。否则，根据反馈可能触发新的小型迭代或问题修复流程。
    *   **输入:** 最终交付物和报告，人工验收反馈。
    *   **输出:** 最终验收状态和报告。

42. **6.7 项目总结与知识沉淀Agent (Project Summary & Knowledge Distillation Agent):**
    *   **任务 (流程结束后或定期运行):**
        1.  从PKBM调取整个项目/迭代的所有数据：需求、设计、代码（各版本）、指令、审核反馈、测试结果、错误报告、修复记录等。
        2.  生成一份项目总结报告，包含：
            *   完成的功能点回顾。
            *   每个阶段LLM Agent的表现（例如，哪些Agent生成的初稿质量高，哪些Agent经常需要人工修正其输出）。
            *   人工审核点发现的主要问题类型和频率。
            *   整个流程中的瓶颈环节和耗时分析（基于时间戳）。
            *   从本次项目中可以提炼出的可复用知识/模式（例如，某个特定问题的优雅解决方案，某个易错点的警示）。
        3.  将总结报告和提炼的知识点存入PKBM，用于未来改进LLM的Prompt、微调LLM模型或优化流程本身。
    *   **输入:** PKBM中的全部项目数据。
    *   **输出:** 项目总结报告，可复用知识点。
# 已完成阶段输出

4.  **0.4 项目类型与技术栈引导Agent (Project Type & Tech Stack Elicitation Agent):**

{
  "document_id": "项目元数据 v1.0",
  "version": "1.0",
  "timestamp": "2023-10-27T11:00:00Z", // 实际应为当前时间
  "associated_intent_analysis": "初步意图分析 v1.0",
  "project_name": "多Agent软件开发自动化系统 (简化版)",
  "elicitation_method": "Agent-guided Q&A via User Interaction Proxy (Iterative Refinement)",
  "project_type_and_interaction": {
    "summary": "主要为后台运行的多Agent自动化系统，配备极简Web用户界面。",
    "details": "该系统核心是一系列后台Python Agent (基于AutoGen) 执行自动化软件开发任务，通过一个极简的Web界面（基于Flask）供用户进行人工审核、任务配置、查看流程状态和知识库内容。流程设计为线性流水线，简化回退和并发复杂性。",
    "question_ref": ["Q1", "Q6.1"],
    "user_answer_code": ["Q1:b", "Q6.1:c"]
  },
  "technology_stack_preferences": {
    "primary_programming_language": {
      "language": "Python",
      "reasoning": "AI/ML库丰富，社区支持广泛，适合快速原型和Agent开发。",
      "question_ref": "Q2",
      "user_answer_code": "a"
    },
    "agent_framework": {
      "framework": "AutoGen",
      "question_ref": "Q3 (original)",
      "user_answer_text": "Agent框架使用 AutoGen。"
    },
    "web_service_framework": {
      "framework": "Flask",
      "reasoning": "用户倾向于其极简和灵活性，适合快速搭建基础Web界面和API。",
      "question_ref": "Q3.1.A",
      "user_answer_code": "i"
    },
    "project_knowledge_base_manager_pkbm": {
      "type": "File-based Knowledge Base",
      "organization": "Specific directory structure with individual files for each artifact.",
      "information_linking": "Through filename and directory structure conventions.",
      "write_management": "Strictly serialized writes to the knowledge base.",
      "reasoning": "User preference for simplicity, minimal deployment/maintenance, and insensitivity to performance/efficiency for this aspect. Database systems (relational, graph, vector) are explicitly excluded.",
      "question_ref": ["Q4.1.A", "Q4.1.B", "Q4.1.C"],
      "user_answer_code": ["Q4.1.A:b", "Q4.1.B:a", "Q4.1.C:a"]
    },
    "llm_interaction": {
      "method": "Standard REST API calls via Web Service",
      "details": "LLM interaction integrated into the Flask web service, potentially for chat-like interfaces or CI workflows.",
      "question_ref": "Q5",
      "user_answer_code": "a"
    },
    "frontend_technology": {
      "approach": "AI-assisted design and generation, user review.",
      "style_complexity": "Minimalistic, focusing on basic operations, allowing for future modular expansion.",
      "performance_sensitivity": "Low",
      "question_ref": ["Q6 (original)", "Q6.1"],
      "user_answer_text": "Web前端不熟悉，AI设计编写，用户检查界面。我对性能和效率不敏感. 极简，仅满足基本操作即可。"
    }
  },
  "key_simplifications_and_constraints": [
    "PKBM is file-based, no traditional databases.",
    "Overall process is a linear pipeline.",
    "Reduced emphasis on efficiency and complex rollback mechanisms.",
    "Vector database functionality removed.",
    "Web UI is minimalistic."
  ],
  "open_questions_for_later_stages": [
    "Specific directory structure conventions for the file-based PKBM.",
    "Detailed definition of how AI will assist in frontend design and generation.",
    "Specific interaction patterns between Flask, AutoGen, and LLM APIs for workflows."
  ]
}


8.  **1.4 需求规格文档编纂Agent (Requirements Specification Compiler Agent):**

**输出:** 完整的需求规格说明书文档 (Markdown格式):

```markdown
# Requirements Specification Document v1.0

**Document ID:** RequirementsSpecificationDocument_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-28T17:00:00Z 
**Generated By:** 1.4 Requirements Specification Compiler Agent

## 1. Introduction

This document outlines the requirements for an AI-driven software development automation system. The primary goal of this system is to assist developers by automating various stages of the software development lifecycle, from initial requirement gathering through to code generation, testing, and documentation. The system is envisioned as a collection of specialized agents working in concert, managed through a web-based interface, and relying on a central file-based Project Knowledge Base (PKBM) for storing and retrieving all project artifacts. Key interactions will involve user input via a web interface, LLM-driven processing by various agents, and manual review checkpoints to ensure quality and alignment with user intent.

## 2. User Stories and Acceptance Criteria

Below is a list of user stories identified for the system, along with their detailed acceptance criteria.

---

**User Story ID:** US001
**As a:** Developer
**I want to:** input my raw natural language software requirements via a web interface text area
**So that:** the system can capture and store them accurately for initial processing.
**Notes:** Corresponds to Agent 0.1. Input validation for empty/short text needed (Q0.1.2).

**Acceptance Criteria:**
  - **AC_ID:** US001_AC1
    **Description:** Verify successful capture of valid raw requirements.
    **Gherkin:**
    ```gherkin
    Given the user is on the requirement input page via the web interface (Q0.1.1)
    When the user enters a natural language requirement text (>=10 characters) into the text area
    And submits the form
    Then the '0.1 User Initial Request Receiver Agent' records the exact text
    And passes the text to PKBM for storage as '原始用户需求 v1.0'
    And passes the text to the '0.3 初步需求意图识别Agent'.
    ```
  - **AC_ID:** US001_AC2
    **Description:** Verify error handling for empty requirement input.
    **Gherkin:**
    ```gherkin
    Given the user is on the requirement input page via the web interface (Q0.1.1)
    When the user submits an empty requirement text
    Then the system displays an error message '提示错误信息并要求重新输入' (Q0.1.2)
    And the '0.1 User Initial Request Receiver Agent' does not proceed with storing or passing the text.
    ```
  - **AC_ID:** US001_AC3
    **Description:** Verify error handling for too short requirement input.
    **Gherkin:**
    ```gherkin
    Given the user is on the requirement input page via the web interface (Q0.1.1)
    When the user submits a requirement text shorter than 10 characters
    Then the system displays an error message '提示错误信息并要求重新输入' (Q0.1.2)
    And the '0.1 User Initial Request Receiver Agent' does not proceed with storing or passing the text.
    ```

---

**User Story ID:** US002
**As a:** System
**I want to:** store any received document or structured information in a file-based Project Knowledge Base (PKBM) using predefined directory structures and naming conventions that include version (timestamp) and descriptive tags in the filename
**So that:** all agents have a consistent, versioned, and retrievable source of project artifacts.
**Notes:** Core PKBM functionality (Agent 0.2). Covers QG5, QG6, QG7, Q0.2.1. Write access is serialized (QG8).

**Acceptance Criteria:**
  - **AC_ID:** US002_AC1
    **Description:** Verify correct storage of documents in PKBM with proper naming and versioning.
    **Gherkin:**
    ```gherkin
    Given an Agent has a document or structured information to store
    And the PKBM uses a predefined directory structure (QG9)
    When the '0.2 Project Knowledge Base Manager Agent' receives the information
    Then a file is created in the appropriate predefined directory
    And the filename includes a descriptive tag, a version (timestamp as per QG5), and the source agent ID (Q0.2.1)
    And the content of the file exactly matches the information provided for storage
    And this write operation is serialized (QG8).
    ```
  - **AC_ID:** US002_AC2
    **Description:** Verify PKBM provides information retrieval based on filenames and partial content scanning.
    **Gherkin:**
    ```gherkin
    Given information is stored in PKBM with descriptive filenames (Q0.2.1)
    When an Agent requests information from PKBM using tags, keywords, or associations
    Then the PKBM (simulated by Agent scanning filenames/content - QG10) can locate and provide the relevant file(s) or their paths.
    ```
  - **AC_ID:** US002_AC3
    **Description:** Verify PKBM uses TXT as default format if not specified.
    **Gherkin:**
    ```gherkin
    Given an Agent provides information to PKBM without specifying a format
    And the information is not explicitly Markdown or JSON
    Then the PKBM stores the information as a plain TXT file (QG7).
    ```

---

**User Story ID:** US003
**As a:** Developer
**I want to:** have the system automatically analyze my raw requirements to identify core intent (e.g., 'create_feature') and key entities (e.g., 'user_login')
**So that:** I get a preliminary structured understanding of my request, which is stored in PKBM.
**Notes:** Corresponds to Agent 0.3. Handles uncertainty by asking for clarification (Q0.3.1).

**Acceptance Criteria:**
  - **AC_ID:** US003_AC1
    **Description:** Verify successful identification of core intent and key entities from raw requirements.
    **Gherkin:**
    ```gherkin
    Given the '0.3 Initial Intent Recognition Agent' receives '原始用户需求文本 v1.0'
    When the agent processes the text
    Then the agent outputs a structured preliminary analysis report (JSON format)
    And the report contains a dynamically generated 'intent' (e.g., 'create_new_feature') (Q0.3.2)
    And the report contains a list of identified 'entities' (e.g., ['user_login', 'oauth2'])
    And this report is stored in PKBM as '初步意图分析 v1.0', associated with '原始用户需求 v1.0'.
    ```
  - **AC_ID:** US003_AC2
    **Description:** Verify handling of uncertain intent/entity identification.
    **Gherkin:**
    ```gherkin
    Given the '0.3 Initial Intent Recognition Agent' receives '原始用户需求文本 v1.0'
    And the agent cannot clearly identify core intent or key entities
    Then the agent outputs an analysis report marked as 'uncertain' or with empty fields
    And the system prompts the user for clarification via the web interface until the analysis is corrected and confirmed by the user (Q0.3.1).
    ```

---

**User Story ID:** US004
**As a:** Developer
**I want to:** be prompted by the system with targeted questions to define or confirm the project type, primary programming language, and core frameworks/libraries
**So that:** the system establishes essential project metadata for subsequent planning and design, storing it in PKBM.
**Notes:** Corresponds to Agent 0.4. Handles 'not sure' answers by deferring to later agents (Q0.4.3). Project history for existing projects determined by pre-configured folder (Q0.4.1).

**Acceptance Criteria:**
  - **AC_ID:** US004_AC1
    **Description:** Verify system prompts for project metadata and stores it.
    **Gherkin:**
    ```gherkin
    Given the '0.4 Project Type & Tech Stack Elicitation Agent' receives '初步意图分析报告 v1.0'
    And the project is new (determined by pre-configured folder - Q0.4.1)
    When the agent generates targeted questions about project type, language, frameworks (Q0.4.2)
    And these questions are presented to the user via the '用户交互代理' (web interface - QG3)
    And the user provides answers as text strings (QG4)
    Then the agent compiles these Q&A and confirmed metadata into a structured document
    And this document is stored in PKBM as '项目元数据 v1.0', associated with '初步意图分析 v1.0'.
    ```
  - **AC_ID:** US004_AC2
    **Description:** Verify handling of 'not sure' answers for tech stack.
    **Gherkin:**
    ```gherkin
    Given the user is asked about technology stack details
    When the user answers '不确定' or '由系统推荐'
    Then the '0.4 Project Type & Tech Stack Elicitation Agent' marks these items in '项目元数据 v1.0' as 'to be determined by Technology Selection Advisor Agent' (Q0.4.3).
    ```
  - **AC_ID:** US004_AC3
    **Description:** Verify retrieval of project history if it's an existing project.
    **Gherkin:**
    ```gherkin
    Given the project is identified as an existing project (based on pre-configured folder Q0.4.1)
    When the '0.4 Project Type & Tech Stack Elicitation Agent' starts
    Then the agent attempts to retrieve existing '项目元数据' from PKBM before asking questions.
    ```

---

**User Story ID:** US005
**As a:** Developer
**I want to:** be asked by the system, through the web interface, specific questions to clarify details, non-functional requirements (NFRs), boundary conditions, user scenarios, expected behaviors, and error handling for each part of my initial request
**So that:** all ambiguities are resolved, and the detailed Q&A is stored in PKBM for future reference.
**Notes:** Corresponds to Agent 1.1. Covers general error handling (QG1, QG2) and user interaction (QG3, QG4).

**Acceptance Criteria:**
  - **AC_ID:** US005_AC1
    **Description:** Verify system asks clarifying questions and stores Q&A.
    **Gherkin:**
    ```gherkin
    Given the '1.1 Requirement Detail & Boundary Miner Agent' receives '原始用户需求文本' and '项目元数据 v1.0'
    And retrieves '初步意图分析报告 v1.0' from PKBM
    When the agent analyzes the requirements and generates clarifying questions (details, NFRs, boundaries, scenarios, behaviors, error handling)
    And presents these questions to the user via the web interface (QG3)
    And the user provides answers as text strings (QG4)
    Then the agent compiles the Q&A into a structured record
    And stores this record in PKBM as '需求澄清细节QA v1.0'.
    ```
  - **AC_ID:** US005_AC2
    **Description:** Verify general system error handling during agent task execution.
    **Gherkin:**
    ```gherkin
    Given an Agent is executing a task
    When the Agent encounters an internal error (e.g., PKBM access failure, LLM API call failure)
    Then the Agent attempts to automatically retry the operation 3 times (QG1)
    And if still failing, records the error in PKBM and stops the current sub-task
    And notifies the user of the error via the web interface session/log area, awaiting manual intervention (QG1, QG2).
    ```

---

**User Story ID:** US006
**As a:** Developer
**I want to:** have the system automatically transform the clarified requirements into a list of user stories following the 'As a [user type], I want [action] so that [benefit]' template
**So that:** I receive a structured set of requirements that adhere to INVEST principles, stored in PKBM.
**Notes:** Corresponds to Agent 1.2 (this agent). User type in story from LLM (Q1.2.3). Handles large stories by reporting (Q1.2.2).

**Acceptance Criteria:**
  - **AC_ID:** US006_AC1
    **Description:** Verify successful transformation of clarified requirements into INVEST user stories.
    **Gherkin:**
    ```gherkin
    Given the '1.2 User Story Generation Agent' receives clarified requirement details from '需求澄清细节QA v1.0'
    And retrieves '原始用户需求文本' and '项目元数据 v1.0' from PKBM
    And the LLM can infer '[type of user]' from requirements (Q1.2.3)
    When the agent processes these details
    Then the agent outputs a list of user stories in the format 'As a [type of user], I want [an action] so that [a benefit/value]'
    And each story aims to be Independent, Negotiable, Valuable, Estimable (e.g. <=3 dev days - Q1.2.1), Small, and Testable
    And this list is stored in PKBM as '用户故事集 v1.0'.
    ```
  - **AC_ID:** US006_AC2
    **Description:** Verify handling of overly large/complex requirements for user story generation.
    **Gherkin:**
    ```gherkin
    Given clarified requirement details are very large or complex
    When the '1.2 User Story Generation Agent' attempts to generate user stories
    And finds it difficult to create a single story adhering to INVEST (especially 'Small')
    Then the agent reports this situation to the user via the web interface (Q1.2.2)
    And if the user accepts, proceeds with the larger story.
    ```

---

**User Story ID:** US007
**As a:** Developer
**I want to:** have the system define detailed acceptance criteria (preferably in Gherkin Given-When-Then format, but other clear, verifiable conditions are acceptable) for each generated user story
**So that:** each user story has clear, measurable, and testable completion conditions, stored in PKBM and linked to stories.
**Notes:** Corresponds to Agent 1.3. Gherkin is recommended (Q1.3.1). SMART's Time-bound ignored for now (Q1.3.2). Handles unquantifiable ACs by recommending and asking (Q1.3.3).

**Acceptance Criteria:**
  - **AC_ID:** US007_AC1
    **Description:** Verify definition of detailed acceptance criteria for each user story.
    **Gherkin:**
    ```gherkin
    Given the '1.3 Acceptance Criteria Definition Agent' receives clarified details from '需求澄清细节QA v1.0'
    And retrieves '用户故事集 v1.0' from PKBM
    When the agent processes each user story
    Then for each user story, the agent defines a set of detailed acceptance criteria
    And these criteria are preferably in Gherkin (Given-When-Then) format or as a clear, verifiable list (Q1.3.1)
    And each criterion is specific, measurable, achievable, and relevant (SMART, time-bound ignored for now Q1.3.2)
    And the collection of acceptance criteria is stored in PKBM as '验收标准集 v1.0', associated with '用户故事集 v1.0'.
    ```
  - **AC_ID:** US007_AC2
    **Description:** Verify handling of user stories where aspects are hard to quantify for ACs.
    **Gherkin:**
    ```gherkin
    Given a user story has aspects that are difficult to quantify for very specific ACs
    When the '1.3 Acceptance Criteria Definition Agent' defines ACs for it
    Then the agent provides recommended ACs for those aspects
    And presents these recommendations to the user via the web interface for confirmation (Q1.3.3).
    ```

---

**User Story ID:** US008
**As a:** Developer
**I want to:** have the system compile the user stories and their corresponding acceptance criteria into a unified Requirements Specification Document (Markdown format)
**So that:** I have a comprehensive and reviewable specification document, including an auto-generated introduction, stored in PKBM.
**Notes:** Corresponds to Agent 1.4. Intro is auto-generated (Q1.4.1). Markdown format (Q1.4.2).

**Acceptance Criteria:**
  - **AC_ID:** US008_AC1
    **Description:** Verify compilation of user stories and ACs into a Requirements Specification Document.
    **Gherkin:**
    ```gherkin
    Given the '1.4 Requirements Specification Compiler Agent' has access to '用户故事集 v1.0' and '验收标准集 v1.0' from PKBM
    When the agent processes this information
    Then the agent generates a unified Requirements Specification Document in Markdown format (Q1.4.2)
    And the document includes an introduction (project goal summary, auto-generated from existing info - Q1.4.1), the list of user stories, and detailed ACs for each story
    And this document is stored in PKBM as '需求规格说明书 v1.0'.
    ```

---

**User Story ID:** US009
**As a:** Developer
**I want to:** be presented with the Requirements Specification Document along with relevant source documents (paths to original request, intent analysis) via the web interface for my review and approval or rejection (with feedback)
**So that:** I can ensure the specification's accuracy and completeness before architectural design begins.
**Notes:** Corresponds to Agent 1.5. Feedback format is free text (Q1.5.2). Rerouting logic is in agent (Q1.5.3).

**Acceptance Criteria:**
  - **AC_ID:** US009_AC1
    **Description:** Verify presentation of Requirements Specification Document for human review.
    **Gherkin:**
    ```gherkin
    Given '需求规格说明书 v1.0' is available in PKBM
    When the '1.5 Human Review Point 1 Coordinator' prepares the review package
    Then the package presented to the user (e.g., Product Manager) via the web interface includes '需求规格说明书 v1.0', and file paths to '原始用户需求 v1.0' and '初步意图分析 v1.0' (Q1.5.1)
    And includes a checklist of review points for guidance.
    ```
  - **AC_ID:** US009_AC2
    **Description:** Verify handling of 'Approved' review outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the Requirements Specification Document
    When the user selects 'Approve'
    Then '需求规格说明书 v1.0' in PKBM is marked as '已审核通过'
    And the '2.1 技术选型建议Agent' or subsequent relevant agent is notified to begin work.
    ```
  - **AC_ID:** US009_AC3
    **Description:** Verify handling of 'Rejected' review outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the Requirements Specification Document
    When the user selects 'Reject' and provides modification feedback (free text - Q1.5.2)
    Then the feedback is structurally recorded and stored in PKBM as '需求审核反馈 v1.x'
    And the '1.5 Human Review Point 1 Coordinator' internally determines (Q1.5.3) and re-routes the task to '1.1 Requirement Detail & Boundary Miner Agent', '1.2 User Story Generation Agent', or '1.3 Acceptance Criteria Definition Agent' with the feedback.
    ```

---

**User Story ID:** US010
**As a:** Developer
**I want to:** receive 2-3 candidate technology stack suggestions (language, framework, DB, key libraries) with pros/cons and suitability analysis if my project's tech stack is initially undefined or if new requirements suggest a review
**So that:** I can make an informed decision on the technology stack, which is then updated in PKBM's project metadata.
**Notes:** Corresponds to Agent 2.1. Warns if existing stack is unsuitable (Q2.1.1). Suggestions from LLM (Q2.1.2). Detailed analysis (Q2.1.3).

**Acceptance Criteria:**
  - **AC_ID:** US010_AC1
    **Description:** Verify generation of tech stack suggestions if undefined.
    **Gherkin:**
    ```gherkin
    Given '已审核通过的需求规格说明书 v1.0' and '项目元数据 v1.0' are available
    And '项目元数据 v1.0' indicates the tech stack is undefined or requires review (e.g., due to user input in Q0.4.3)
    When the '2.1 Technology Selection Advisor Agent' processes this
    Then the agent generates a report with 2-3 candidate tech stacks (language, framework, DB, libraries) using LLM (Q2.1.2)
    And for each candidate, provides a detailed paragraph on pros/cons, applicable scenarios, and project requirement match (Q2.1.3)
    And this report is stored in PKBM as '技术选型建议 v1.0' and presented to the user for selection.
    ```
  - **AC_ID:** US010_AC2
    **Description:** Verify handling if existing tech stack is unsuitable.
    **Gherkin:**
    ```gherkin
    Given '项目元数据 v1.0' contains a defined tech stack
    And the '2.1 Technology Selection Advisor Agent' analyzes new requirements from '需求规格说明书 v1.0'
    And determines the existing stack may be unsuitable
    Then the agent issues a warning or suggestion with reasons to the user (Q2.1.1) and may proceed to suggest alternatives as per US010_AC1.
    ```
  - **AC_ID:** US010_AC3
    **Description:** Verify skipping tech stack suggestion if already defined and suitable.
    **Gherkin:**
    ```gherkin
    Given '项目元数据 v1.0' contains a clearly defined and suitable tech stack
    When the '2.1 Technology Selection Advisor Agent' starts
    Then the agent confirms the existing stack and passes this information directly to the '2.2 高层组件划分Agent' without generating new suggestions.
    ```
  - **AC_ID:** US010_AC4
    **Description:** Verify updating PKBM with user's final tech stack decision.
    **Gherkin:**
    ```gherkin
    Given the user has been presented with '技术选型建议 v1.0' (if generated)
    When the user makes a final tech stack selection
    Then the '2.1 Technology Selection Advisor Agent' updates '项目元数据' in PKBM with this decision (creating a new version if necessary).
    ```

---

**User Story ID:** US011
**As a:** Developer
**I want to:** have the system identify and define the major high-level modules/services/components required for the project, along with their core responsibilities (as a list of main features)
**So that:** a foundational architectural structure is proposed, based on requirements and chosen tech stack, and stored in PKBM.
**Notes:** Corresponds to Agent 2.2. Granularity and detail based on LLM and design guidelines (Q2.2.1).

**Acceptance Criteria:**
  - **AC_ID:** US011_AC1
    **Description:** Verify identification and definition of high-level components.
    **Gherkin:**
    ```gherkin
    Given '已审核通过的需求规格说明书 v1.0' and confirmed '项目元数据' (with tech stack) are available
    And design guidelines (including how to define component responsibilities) are available in PKBM (Q2.2.1)
    When the '2.2 高层组件划分Agent' processes this information (using LLM)
    Then the agent outputs a list of major high-level modules/services/components
    And for each component, defines its core responsibilities as a list of main features (Q2.2.1)
    And this list is stored in PKBM as '高层组件定义 v1.0'.
    ```

---

**User Story ID:** US012
**As a:** Developer
**I want to:** have the system draft preliminary interface definitions (e.g., REST API endpoints with HTTP methods, function signatures, message queue topics/formats) between interacting high-level components
**So that:** the initial communication pathways and contracts between components are outlined, stored in PKBM.
**Notes:** Corresponds to Agent 2.3. Includes HTTP methods for REST (Q2.3.1). Considers event-driven patterns (Q2.3.2).

**Acceptance Criteria:**
  - **AC_ID:** US012_AC1
    **Description:** Verify drafting of preliminary component interface definitions.
    **Gherkin:**
    ```gherkin
    Given '高层组件定义 v1.0' is available from PKBM
    When the '2.3 组件间接口草拟Agent' analyzes component dependencies and data flow
    Then the agent outputs a draft of preliminary interface definitions for interacting components
    And for REST APIs, this includes API endpoints and HTTP methods (Q2.3.1)
    And for other interactions (e.g., function calls, message queues), it includes function signatures or message topics/formats (Q2.3.2)
    And the draft focuses on interface name, main parameters, and expected return types, without full data structures
    And this draft is stored in PKBM as '组件接口草案 v1.0'.
    ```

---

**User Story ID:** US013
**As a:** Developer
**I want to:** have the system generate a natural language description and a Mermaid diagram (showing components, connections, interface names) representing the high-level system architecture
**So that:** I can visualize and understand the proposed system architecture, and this description is stored in PKBM.
**Notes:** Corresponds to Agent 2.4. Mermaid diagram for technical audience (Q2.4.1, Q2.4.2).

**Acceptance Criteria:**
  - **AC_ID:** US013_AC1
    **Description:** Verify generation of natural language architecture description and Mermaid diagram.
    **Gherkin:**
    ```gherkin
    Given '高层组件定义 v1.0' and '组件接口草案 v1.0' are available from PKBM
    When the '2.4 架构图描述生成Agent' processes this information
    Then the agent outputs a natural language description of the high-level architecture for a technical audience (Q2.4.1)
    And the agent outputs a Mermaid diagram description text that includes components, their connections, and interface names (Q2.4.1, Q2.4.2)
    And these outputs are stored in PKBM as '架构图文本描述 v1.0'.
    ```

---

**User Story ID:** US014
**As a:** Developer
**I want to:** have the system compile the confirmed technology stack, high-level component definitions, interface drafts, and architecture diagram description into a High-Level Architecture Design Document
**So that:** I receive a consolidated document detailing the proposed architecture, stored in PKBM.
**Notes:** Corresponds to Agent 2.5. Tech stack rationale is复述 (Q2.5.1).

**Acceptance Criteria:**
  - **AC_ID:** US014_AC1
    **Description:** Verify compilation of High-Level Architecture Design Document.
    **Gherkin:**
    ```gherkin
    Given the final technology selection (from '项目元数据'), '高层组件定义 v1.0', '组件接口草案 v1.0', and '架构图文本描述 v1.0' are available from PKBM
    When the '2.5 架构设计文档编纂Agent' processes these inputs
    Then the agent generates a consolidated High-Level Architecture Design Document
    And the document includes the technology selection and its rationale (复述 if previously defined - Q2.5.1), component list with responsibilities, main inter-component interfaces, and the architecture diagram (or its text description)
    And this document is stored in PKBM as '高层架构设计文档 v1.0'.
    ```

---

**User Story ID:** US015
**As a:** Developer
**I want to:** be presented with the High-Level Architecture Design Document for my review and approval or rejection (with feedback) via the web interface
**So that:** I can validate the architectural design's soundness and suitability before detailed design and task breakdown.
**Notes:** Corresponds to Agent 2.6. Similar review process to US009 (Q2.6).

**Acceptance Criteria:**
  - **AC_ID:** US015_AC1
    **Description:** Verify presentation of Architecture Design Document for human review.
    **Gherkin:**
    ```gherkin
    Given '高层架构设计文档 v1.0' is available in PKBM
    When the '2.6 人工审核点2协调Agent' prepares the review package
    Then the package presented to the user (e.g., Tech Lead/Architect) via the web interface includes '高层架构设计文档 v1.0' and a checklist of review points (Q2.6).
    ```
  - **AC_ID:** US015_AC2
    **Description:** Verify handling of 'Approved' architecture review outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the Architecture Design Document
    When the user selects 'Approve'
    Then '高层架构设计文档 v1.0' in PKBM is marked as '已审核通过'
    And the '2.7 编码任务分解Agent' is notified to begin work.
    ```
  - **AC_ID:** US015_AC3
    **Description:** Verify handling of 'Rejected' architecture review outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the Architecture Design Document
    When the user selects 'Reject' and provides modification feedback (free text)
    Then the feedback is structurally recorded and stored in PKBM as '架构审核反馈 v1.x'
    And the '2.6 人工审核点2协调Agent' internally determines and re-routes the task to the appropriate earlier architecture agent (e.g., '2.1 Technology Selection Advisor Agent', '2.2 高层组件划分Agent') with the feedback (Q2.6).
    ```

---

**User Story ID:** US016
**As a:** Developer
**I want to:** have the system break down each approved user story/feature into smaller, independently developable and testable coding sub-tasks (granularity: one sub-task per class to be implemented/modified)
**So that:** large features are decomposed into manageable units of work, with each sub-task's goal and expected output (JSON format) defined and stored in PKBM.
**Notes:** Corresponds to Agent 2.7. Granularity is one class per sub-task (Q2.7.1). Output is detailed JSON (Q2.7.2).

**Acceptance Criteria:**
  - **AC_ID:** US016_AC1
    **Description:** Verify breakdown of user stories into coding sub-tasks.
    **Gherkin:**
    ```gherkin
    Given '已审核通过的需求规格说明书 v1.0' and '已审核通过的高层架构设计文档 v1.0' are available
    When the '2.7 编码任务分解Agent' processes these
    Then each user story/feature is mapped to one or more architectural components
    And large features/components are decomposed into smaller coding sub-tasks, where one sub-task typically corresponds to one class to be implemented/modified (Q2.7.1)
    And for each sub-task, its goal and expected output are defined in a detailed JSON format (Q2.7.2)
    And the list of these sub-tasks is stored in PKBM as '编码子任务列表 v1.0'.
    ```

---

**User Story ID:** US017
**As a:** Developer
**I want to:** have the system analyze logical dependencies between coding sub-tasks and generate an ordered development plan (e.g., JSON list with IDs, descriptions, dependencies, priorities)
**So that:** a clear and sequenced plan for development is established, stored in PKBM, ensuring tasks are tackled in a logical order.
**Notes:** Corresponds to Agent 2.8. Handles circular dependencies by erroring and requesting manual intervention (Q2.8.1). Output is JSON (Q2.8.2). Tasks are sequential (Q6.1.1).

**Acceptance Criteria:**
  - **AC_ID:** US017_AC1
    **Description:** Verify generation of an ordered development plan with dependencies.
    **Gherkin:**
    ```gherkin
    Given '编码子任务列表 v1.0' is available from PKBM
    When the '2.8 任务依赖与顺序规划Agent' analyzes sub-task dependencies
    Then the agent outputs a development plan (e.g., JSON list with task ID, description, dependency IDs, priority - Q2.8.2)
    And this plan defines a logical execution order (tasks are sequential, no parallelism - Q6.1.1)
    And this plan is stored in PKBM as '编码任务计划 v1.0'.
    ```
  - **AC_ID:** US017_AC2
    **Description:** Verify handling of circular dependencies.
    **Gherkin:**
    ```gherkin
    Given '编码子任务列表 v1.0' is being processed
    When the '2.8 任务依赖与顺序规划Agent' detects a circular dependency among sub-tasks
    Then the agent reports an error via the web interface
    And requests manual intervention from the user to resolve the conflict (Q2.8.1).
    ```

---

**User Story ID:** US018
**As a:** System
**I want to:** prepare a complete context package for each coding sub-task by retrieving related user stories, acceptance criteria, relevant architecture (component/interface definitions), project metadata (tech stack, coding/design guidelines from PKBM), and any existing relevant code snippets (pasted by user for modification tasks)
**So that:** subsequent detailed design agents receive all necessary information to perform their specific design tasks for the current sub-task.
**Notes:** Corresponds to Agent 3.1. Existing code provided via web paste (Q3.1.1).

**Acceptance Criteria:**
  - **AC_ID:** US018_AC1
    **Description:** Verify preparation of complete context package for a sub-task.
    **Gherkin:**
    ```gherkin
    Given a sub-task ID is selected from '编码任务计划 v1.0'
    When the '3.1 当前子任务上下文准备Agent' executes for this sub-task
    Then the agent retrieves from PKBM: the corresponding user story and ACs, relevant architecture design (component/interface defs), project metadata (tech stack, coding/design guidelines)
    And if it's a modification task, it includes existing code snippets/file structures pasted by the user via the web interface (Q3.1.1)
    And all this context is packaged and passed to subsequent detailed design agents.
    ```

---

**User Story ID:** US019
**As a:** Developer
**I want to:** have the system perform detailed API design for sub-tasks involving APIs, specifying precise URLs, HTTP methods, headers, path/query parameters, and detailed request/response body JSON schemas (including field types, necessity, examples) for various status codes
**So that:** a comprehensive API specification is created for the sub-task, stored in PKBM, guiding implementation.
**Notes:** Corresponds to Agent 3.2. JSON Schema detail (Q3.2.1). Authentication/Authorization not considered for now (Q3.2.2).

**Acceptance Criteria:**
  - **AC_ID:** US019_AC1
    **Description:** Verify detailed API design for sub-tasks involving APIs.
    **Gherkin:**
    ```gherkin
    Given a '当前子任务上下文包' for an API-related sub-task is received by '3.2 API详细设计Agent'
    When the agent performs detailed API design
    Then the output includes: precise URL paths and HTTP methods, detailed request headers, path/query parameters, request body JSON schema (all fields, types, necessity, examples - Q3.2.1), response body JSON schema for various status codes (200, 201, 400, etc. - Q3.2.1)
    And authentication/authorization mechanisms are noted as 'not considered for now' (Q3.2.2)
    And the API design document (OpenAPI fragment or Markdown) is stored in PKBM as 'API详细设计_[子任务ID]_v1.0'.
    ```

---

**User Story ID:** US020
**As a:** Developer
**I want to:** have the system perform detailed data model design for sub-tasks involving data persistence or complex structures, defining data file formats, schemas, and file/organization within the PKBM's file-based system if applicable, or class/object properties and types for in-memory structures
**So that:** a clear data structure or persistence plan is established for the sub-task, stored in PKBM.
**Notes:** Corresponds to Agent 3.3. For PKBM, this means defining how information is structured in files (Q3.3.1).

**Acceptance Criteria:**
  - **AC_ID:** US020_AC1
    **Description:** Verify detailed data model design for sub-tasks.
    **Gherkin:**
    ```gherkin
    Given a '当前子任务上下文包' for a data-related sub-task is received by '3.3 数据模型详细设计Agent'
    When the agent performs detailed data model design
    Then the output defines data file formats, schemas, and file organization if for PKBM-like file storage (Q3.3.1), or class/object properties and types for in-memory structures
    And this design (e.g., SQL DDL, class diagram text, JSON Schema) is stored in PKBM as '数据模型详细设计_[子任务ID]_v1.0'.
    ```

---

**User Story ID:** US021
**As a:** Developer
**I want to:** have the system design class and function logic for sub-tasks, defining responsibilities, public interfaces (method signatures), and core implementation logic using unambiguous pseudocode or detailed steps covering algorithms, conditions, loops, and error handling
**So that:** the internal logic of classes and functions is clearly outlined before coding, stored in PKBM.
**Notes:** Corresponds to Agent 3.4. Prefers pseudocode, logically complete (Q3.4.1).

**Acceptance Criteria:**
  - **AC_ID:** US021_AC1
    **Description:** Verify design of class and function logic using unambiguous pseudocode.
    **Gherkin:**
    ```gherkin
    Given a '当前子任务上下文包' is received by '3.4 类与函数逻辑设计Agent'
    When the agent designs class/function logic
    Then the output identifies classes/functions to be created/modified
    And for each, defines its responsibility, public interface (method signatures: name, params, return type)
    And describes its core implementation logic using unambiguous pseudocode covering algorithms, conditions, loops, and error handling (Q3.4.1)
    And this design is stored in PKBM as '类函数逻辑设计_[子任务ID]_v1.0'.
    ```

---

**User Story ID:** US022
**As a:** Developer
**I want to:** have the system plan the file and directory structure for new or modified code related to a sub-task, ensuring compliance with project best practices and coarse-grained coding/architecture guidelines from PKBM
**So that:** code organization is determined upfront, promoting consistency, and the plan is stored in PKBM.
**Notes:** Corresponds to Agent 3.5. Guidelines are coarse (Q3.5.1).

**Acceptance Criteria:**
  - **AC_ID:** US022_AC1
    **Description:** Verify planning of file and directory structure.
    **Gherkin:**
    ```gherkin
    Given a '当前子任务上下文包' and related detailed designs are received by '3.5 文件与目录结构规划Agent'
    And the agent retrieves existing file structure and coarse-grained coding/architecture guidelines from PKBM (Q3.5.1)
    When the agent plans file/directory structure
    Then the output details new file names and paths, existing files to be modified, and new directories needed
    And the planned structure aligns with project best practices and guidelines
    And this plan is stored in PKBM as '文件结构计划_[子任务ID]_v1.0'.
    ```

---

**User Story ID:** US023
**As a:** Developer
**I want to:** have the system perform a basic review of the sub-task's design for any obvious security missteps, focusing on ensuring code stability and meeting user expectations rather than comprehensive security hardening
**So that:** a minimal check is done as a reminder, even if security is not a primary project focus, with findings stored in PKBM.
**Notes:** Corresponds to Agent 3.6. Security is low priority (Q3.6.1, Q3.6.2).

**Acceptance Criteria:**
  - **AC_ID:** US023_AC1
    **Description:** Verify basic security review focusing on stability and user expectations.
    **Gherkin:**
    ```gherkin
    Given a '当前子任务上下文包' and all related detailed design documents are received by '3.6 安全考量与检查点定义Agent'
    And the project priority is code stability and user expectation over comprehensive security (Q3.6.1, Q3.6.2)
    When the agent performs a security review
    Then the agent identifies any obvious security missteps relevant to stability/expectations
    And proposes minimal checks or considerations if any
    And this report is stored in PKBM as '安全考量_[子任务ID]_v1.0'.
    ```

---

**User Story ID:** US024
**As a:** Developer
**I want to:** have the system aggregate all detailed designs (API, data, logic, file structure, security considerations) for a sub-task into a comprehensive 'Code Implementation Guide' and generate an extremely specific, context-rich prompt text for an external code generation LLM (Roo code LLM)
**So that:** a complete guide for coding is created, and a high-quality prompt is ready for the LLM, maximizing the chance of correct code generation. Both are stored in PKBM.
**Notes:** Corresponds to Agent 3.7. Prompt includes original code for modifications (from web paste or file path Q3.7.1), naming conventions (from spec + agent suggestions Q3.7.2), full file context for relevant files (Q3.7.3).

**Acceptance Criteria:**
  - **AC_ID:** US024_AC1
    **Description:** Verify aggregation of designs into 'Code Implementation Guide' and generation of LLM prompt.
    **Gherkin:**
    ```gherkin
    Given all detailed design documents for a sub-task (API, data, logic, file structure, security) are available
    And relevant coding规范 and original code snippets (for modifications, from web paste or file path - Q3.7.1) are accessible from PKBM
    When the '3.7 详细设计汇总与指令生成Agent' executes
    Then a '代码实现指导书_[子任务ID]_v1.0' is created and stored in PKBM
    And an extremely specific, context-rich prompt text for 'Roo code LLM' is generated
    And this prompt includes: file path for creation/modification, exact original code for modifications, clear functional logic/algorithms, class/function signatures (from spec + agent suggestions Q3.7.2), variable naming conventions, coding规范 to follow, security checkpoints (if any from US023), and full file context for relevant files (Q3.7.3), and expected output format.
    ```

---

**User Story ID:** US025
**As a:** Developer
**I want to:** be presented with the 'Code Implementation Guide' and the generated LLM prompt text for my review and approval or rejection (with feedback) via the web interface
**So that:** I can ensure the LLM prompt is clear, complete, and accurately reflects all design requirements before code generation.
**Notes:** Corresponds to Agent 3.8. Similar review process to US009 (Q3.8).

**Acceptance Criteria:**
  - **AC_ID:** US025_AC1
    **Description:** Verify presentation of 'Code Implementation Guide' and LLM prompt for human review.
    **Gherkin:**
    ```gherkin
    Given '代码实现指导书_[子任务ID]_v1.0' and the LLM prompt text are generated
    When the '3.8 人工审核点3协调Agent' prepares the review package
    Then the package presented to the user (Developer) via the web interface includes both documents and a checklist of review points (Q3.8).
    ```
  - **AC_ID:** US025_AC2
    **Description:** Verify handling of 'Approved' prompt review outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the LLM prompt and guide
    When the user selects 'Approve'
    Then the '精确指令文本_[子任务ID]_v1.0' in PKBM is marked as '已审核待执行'
    And the prompt text is passed to '4.1 代码生成/修改执行协调Agent'.
    ```
  - **AC_ID:** US025_AC3
    **Description:** Verify handling of 'Rejected' prompt review outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the LLM prompt and guide
    When the user selects 'Reject' and provides modification feedback
    Then the feedback is stored in PKBM
    And the task is re-routed to '3.7 详细设计汇总与指令生成Agent' for prompt modification (Q3.8).
    ```

---

**User Story ID:** US026
**As a:** Developer
**I want to:** use the web interface to copy the approved LLM prompt, paste it into the external 'Roo code LLM', and then paste the LLM's generated code (or error message if LLM fails) back into the web interface
**So that:** I can leverage the external LLM for code generation/modification, and its raw output is captured by the system in PKBM.
**Notes:** Corresponds to Agent 4.1. UI support for copy/paste (Q4.1.1). Human handles LLM errors and retries before informing system (Q4.1.2).

**Acceptance Criteria:**
  - **AC_ID:** US026_AC1
    **Description:** Verify coordination of code generation via external LLM.
    **Gherkin:**
    ```gherkin
    Given an '已审核待执行的精确指令文本_[子任务ID]_v1.0' is received by '4.1 代码生成/修改执行协调Agent'
    And the web interface provides a text box for the prompt and a 'Copy' button (Q4.1.1a)
    When the user copies the prompt, pastes it into 'Roo code LLM', triggers execution, and obtains the output
    And pastes the LLM's raw output (code or error info) back into another text box in the web interface (Q4.1.1b)
    Then the '4.1 代码生成/修改执行协调Agent' records this raw output in PKBM as 'CGMA原始输出_[子任务ID]_v1.0'
    And if it's code, passes it to '4.2 代码基本语法与格式校验Agent'.
    ```
  - **AC_ID:** US026_AC2
    **Description:** Verify human handling of LLM errors before system notification.
    **Gherkin:**
    ```gherkin
    Given the user is interacting with 'Roo code LLM'
    When 'Roo code LLM' returns an error or fails to generate code
    Then the user first attempts retries with the LLM (Q4.1.2)
    And if still unsuccessful, the user provides this error information (as if it were LLM output) back to the '4.1 代码生成/修改执行协调Agent' (Q4.1.2).
    ```

---

**User Story ID:** US027
**As a:** Developer
**I want to:** have the system perform basic syntax and formatting checks on the LLM-generated code using a linter (selected based on project language in metadata) and allow me to confirm any auto-suggested simple formatting fixes (which would then be formulated as a new prompt for the LLM to apply)
**So that:** the generated code is validated for basic correctness and adherence to formatting standards, with results and potentially formatted code stored in PKBM.
**Notes:** Corresponds to Agent 4.2. Linter config from metadata (Q4.2.1). Auto-fixes confirmed by user and applied via LLM (Q4.2.2). Human decides on回溯 for severe errors (Q4.2.3).

**Acceptance Criteria:**
  - **AC_ID:** US027_AC1
    **Description:** Verify basic syntax and formatting checks on LLM-generated code.
    **Gherkin:**
    ```gherkin
    Given 'CGMA原始输出_[子任务ID]_v1.0' (code) is received by '4.2 代码基本语法与格式校验Agent'
    And project metadata specifies the language, allowing selection of a default linter (Q4.2.1)
    When the agent performs syntax and formatting checks
    Then a validation report (syntax errors, format issues) is generated
    And stored in PKBM as '代码初步校验报告_[子任务ID]_v1.0'.
    ```
  - **AC_ID:** US027_AC2
    **Description:** Verify user confirmation for auto-suggested formatting fixes applied via LLM.
    **Gherkin:**
    ```gherkin
    Given the linter suggests simple formatting fixes
    When '4.2 代码基本语法与格式校验Agent' identifies these
    Then the agent presents these suggested fixes to the user for confirmation (Q4.2.2)
    And if confirmed, a new prompt is generated for 'Roo code LLM' to apply these format changes
    And the (potentially) formatted code is stored as '初步格式化代码_[子任务ID]_v1.0'.
    ```
  - **AC_ID:** US027_AC3
    **Description:** Verify handling of severe syntax errors requiring human decision for回溯.
    **Gherkin:**
    ```gherkin
    Given '代码初步校验报告_[子任务ID]_v1.0' indicates severe syntax errors preventing further steps
    When '4.2 代码基本语法与格式校验Agent' identifies this
    Then the system flags the issue and awaits a human decision (via web interface) on whether to回溯 to prompt generation (Q4.2.3).
    ```
  - **AC_ID:** US027_AC4
    **Description:** Verify passing of valid, formatted code to static analysis.
    **Gherkin:**
    ```gherkin
    Given code has passed syntax checks and is (potentially) formatted ('初步格式化代码_[子任务ID]_v1.0')
    And no severe syntax errors exist
    Then this code is passed to '4.3 静态代码分析Agent'.
    ```

---

**User Story ID:** US028
**As a:** Developer
**I want to:** have the system run static analysis tools on the (syntax-checked and formatted) code using a default ruleset (based on project language in metadata) to identify potential bugs, code smells, and simple security vulnerabilities
**So that:** deeper code quality issues are identified, and a plain text report is generated (for LLM consumption) and stored in PKBM.
**Notes:** Corresponds to Agent 4.3. Ruleset from metadata (Q4.3.1). Report is plain text for LLM (Q4.3.2).

**Acceptance Criteria:**
  - **AC_ID:** US028_AC1
    **Description:** Verify static analysis on code.
    **Gherkin:**
    ```gherkin
    Given '初步格式化代码_[子任务ID]_v1.0' is received by '4.3 静态代码分析Agent'
    And project metadata allows selection of a default static analysis ruleset for the language (Q4.3.1)
    When the agent runs static analysis tools
    Then a detailed static analysis report (plain text for LLM - Q4.3.2) is generated, listing issues, locations, severity, and suggestions
    And this report is stored in PKBM as '静态分析报告_[子任务ID]_v1.0'
    And the report and code are passed to '5.1 AI代码审查员Agent'.
    ```

---

**User Story ID:** US029
**As a:** Developer
**I want to:** have an AI Code Reviewer agent analyze the code against the 'Implementation Guide', coding规范, and static analysis report, checking for functional conformity, design adherence, readability, error handling, and basic performance/security points
**So that:** I receive an automated code review report (list of issues with location, description, suggestion, severity) highlighting potential issues beyond static analysis, stored in PKBM.
**Notes:** Corresponds to Agent 5.1. Performance check is low-strictness (Q5.1.1). Report is a list of issues (Q5.1.2).

**Acceptance Criteria:**
  - **AC_ID:** US029_AC1
    **Description:** Verify AI code review against implementation guide and standards.
    **Gherkin:**
    ```gherkin
    Given '初步格式化代码_[子任务ID]_v1.0', '静态分析报告_[子任务ID]_v1.0', '实现指导书_[子任务ID]_v1.0', and '编码规范' are received by '5.1 AI代码审查员Agent'
    When the agent performs the code review
    Then an 'AI代码审查报告_[子任务ID]_v1.0' (list of issues with location, description, suggestion, severity - Q5.1.2) is generated
    And the review checks for: functional conformity to guide, design adherence,规范 compliance, readability, error handling, basic performance (low-strictness complexity analysis - Q5.1.1), and security checkpoints from guide
    And this report is stored in PKBM and passed with code and static analysis report to '5.2 人工审核点4协调Agent'.
    ```

---

**User Story ID:** US030
**As a:** Developer
**I want to:** be presented with the code, static analysis report, and AI code review report via the web interface for my review and decision (approve for testing, or reject with feedback on what to fix)
**So that:** I can make an informed decision on code quality, leveraging AI insights, before proceeding to testing.
**Notes:** Corresponds to Agent 5.2. No direct manual code modification by user in system; if changes needed, agent is asked to update PKBM (Q5.2.1). LLM suggests next steps on rejection (Q5.2.2).

**Acceptance Criteria:**
  - **AC_ID:** US030_AC1
    **Description:** Verify presentation of code and AI review reports for human decision.
    **Gherkin:**
    ```gherkin
    Given '初步格式化代码_[子任务ID]_v1.0', '静态分析报告_[子任务ID]_v1.0', and 'AI代码审查报告_[子任务ID]_v1.0' are available
    When '5.2 人工审核点4协调Agent' prepares the review package
    Then the package is presented to the user (Developer/Tech Lead) via web interface with a checklist.
    ```
  - **AC_ID:** US030_AC2
    **Description:** Verify handling of 'Approve for testing' outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the code and reports
    When the user selects 'Approve for testing'
    Then '初步格式化代码_[子任务ID]_v1.0' is marked '代码审核通过' in PKBM
    And '5.3 单元测试用例生成Agent' is notified.
    ```
  - **AC_ID:** US030_AC3
    **Description:** Verify handling of 'Reject with feedback' outcome, with LLM suggesting next steps.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the code and reports
    When the user selects 'Reject' and provides feedback
    Then the feedback is stored in PKBM
    And the agent's LLM suggests next steps (e.g., re-generate prompt, re-run CGMA, AI reviewer generates fix suggestions) and asks for user confirmation on how to proceed (Q5.2.2).
    ```
  - **AC_ID:** US030_AC4
    **Description:** Verify handling if user identifies issue requiring PKBM update (simulating manual modification).
    **Gherkin:**
    ```gherkin
    Given the user reviews and identifies a code change is needed
    Instead of direct manual modification in the system (Q5.2.1)
    When the user specifies the file and the change required
    Then the user instructs an agent (e.g., via '5.2 人工审核点4协调Agent') to update the relevant code file in PKBM, effectively creating a new version for the system to process.
    ```

---

**User Story ID:** US031
**As a:** Developer
**I want to:** have the system generate unit test case code (for the project's testing framework, based on metadata/source) covering normal paths, boundary conditions, and expected error handling for the approved code of a sub-task, based on its 'Implementation Guide' (especially ACs) and detailed designs
**So that:** test coverage is automatically generated, aiming for good practice but best-effort based on LLM, and test code is stored in PKBM.
**Notes:** Corresponds to Agent 5.3. Test framework info generated by agent (Q5.3.1). Coverage is best-effort (Q5.3.2). Mocks/stubs ignored (Q5.3.3).

**Acceptance Criteria:**
  - **AC_ID:** US031_AC1
    **Description:** Verify generation of unit test cases.
    **Gherkin:**
    ```gherkin
    Given code for a sub-task is '代码审核通过'
    And '实现指导书_[子任务ID]_v1.0' (with ACs), '类函数逻辑设计', 'API详细设计' are available
    And '5.3 单元测试用例生成Agent' can generate project testing framework info and test coding规范 based on metadata/source (Q5.3.1)
    When the agent generates unit tests
    Then test case code is generated for main functions/methods/classes/API endpoints
    And tests cover normal paths, boundary conditions, and expected error handling, aligning with '实现指导书' ACs
    And tests that would require mocks/stubs for external dependencies are ignored (Q5.3.3)
    And test coverage is best-effort based on LLM capability (Q5.3.2)
    And the unit test code is stored in PKBM as '单元测试代码_[子任务ID]_v1.0' and passed with source code to '5.4 测试执行协调Agent'.
    ```

---

**User Story ID:** US032
**As a:** Developer
**I want to:** be provided with prompts (via the web interface) to guide an LLM (Roo code plugin) to automatically integrate the generated unit test code into my local project's test suite and then run these tests, after which I will paste the raw test execution results back into the web interface
**So that:** unit tests are executed in the local environment, and their results are captured by the system in PKBM.
**Notes:** Corresponds to Agent 5.4. 'Assistance' is LLM prompt for test integration (Q5.4.1). Results pasted as text (Q5.4.1b).

**Acceptance Criteria:**
  - **AC_ID:** US032_AC1
    **Description:** Verify coordination of unit test execution in local environment.
    **Gherkin:**
    ```gherkin
    Given被测代码 and '单元测试代码_[子任务ID]_v1.0' are received by '5.4 测试执行协调Agent'
    When the agent provides prompts (via web interface) to an LLM (Roo code plugin) to integrate test code into the user's local project and run tests (Q5.4.1a)
    And the user pastes the raw test execution result text back into the web interface (Q5.4.1b)
    Then this raw report is stored in PKBM as '单元测试结果原始报告_[子任务ID]_v1.0'
    And passed to '5.5 测试结果分析与调试指引Agent'.
    ```

---

**User Story ID:** US033
**As a:** Developer
**I want to:** have the system analyze the unit test execution results, and if all tests pass, mark the sub-task as 'Test Passed'; if tests fail, generate a Markdown debugging analysis report detailing failures, potential causes (in code or tests), and suggested debugging steps or modifications
**So that:** I get automated feedback on test outcomes, with guidance for debugging if necessary, and the report is stored in PKBM.
**Notes:** Corresponds to Agent 5.5. Error localization is best-effort (Q5.5.1). Report is Markdown (Q5.5.2).

**Acceptance Criteria:**
  - **AC_ID:** US033_AC1
    **Description:** Verify analysis of successful unit test results.
    **Gherkin:**
    ```gherkin
    Given '单元测试结果原始报告_[子任务ID]_v1.0' indicates all tests passed
    When '5.5 测试结果分析与调试指引Agent' analyzes it
    Then the sub-task status in PKBM is marked as '测试通过'
    And '6.1 子任务完成与集成准备Agent' is notified.
    ```
  - **AC_ID:** US033_AC2
    **Description:** Verify analysis of failed unit test results and generation of debugging report.
    **Gherkin:**
    ```gherkin
    Given '单元测试结果原始报告_[子任务ID]_v1.0' indicates test failures
    And relevant code and '实现指导书' are available
    When '5.5 测试结果分析与调试指引Agent' analyzes it
    Then a Markdown debugging analysis report is generated (Q5.5.2)
    And the report includes: list of failed tests, error details, best-effort error localization (Q5.5.1), potential cause analysis (code/test error), and suggested debug/modification steps
    And this report is stored in PKBM as '调试分析报告_[子任务ID]_v1.0' and passed to '5.6 人工审核点5协调Agent'.
    ```

---

**User Story ID:** US034
**As a:** Developer
**I want to:** be presented with the debugging analysis report (if tests failed) via the web interface to confirm the issue and instruct the system on next steps (e.g., ask AI to generate fix for code/test, or flag design issue for回溯 via a Central Coordinator Agent)
**So that:** I can guide the iterative debugging and fixing process based on test failures.
**Notes:** Corresponds to Agent 5.6. Similar review to US009. Central Coordinator handles design回溯 (Q5.6.1).

**Acceptance Criteria:**
  - **AC_ID:** US034_AC1
    **Description:** Verify presentation of debugging report for human review and instruction.
    **Gherkin:**
    ```gherkin
    Given '调试分析报告_[子任务ID]_v1.0' (for failed tests) is available
    When '5.6 人工审核点5协调Agent' presents it to the user (Developer) via web interface with a checklist
    Then the user can review the analysis.
    ```
  - **AC_ID:** US034_AC2
    **Description:** Verify handling of user instruction for AI to fix code/test.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the debugging report
    When the user confirms the issue and instructs the AI to attempt a fix for product code or test code
    Then this instruction and confirmed analysis are passed to '5.7 代码/测试修复指令生成Agent'.
    ```
  - **AC_ID:** US034_AC3
    **Description:** Verify handling of user instruction for design 回溯 via Central Coordinator.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the debugging report
    When the user identifies a design issue requiring回溯
    Then the user instructs the system to initiate回溯
    And '5.6 人工审核点5协调Agent' records the issue in PKBM and sends a structured message (problem, error point, need for回溯, target agent name) to the 'Central Coordinator Agent' (US043 / Q5.6.1) to manage the回溯, pausing the current sub-task sequence.
    ```
  - **AC_ID:** US034_AC4
    **Description:** Verify handling if user provides manually fixed code/test.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the debugging report
    When the user manually fixes the code or test outside the system and provides the updated file(s) via the web interface
    Then '5.6 人工审核点5协调Agent' stores the new version in PKBM
    And re-triggers '5.4 测试执行协调Agent' with the updated code/test.
    ```

---

**User Story ID:** US035
**As a:** Developer
**I want to:** have the system generate a specific LLM prompt for fixing identified bugs in product code or test cases, based on my confirmed debugging analysis and instructions
**So that:** a targeted prompt is created for the LLM to attempt a fix, restarting the code generation/validation/test cycle for the fix.
**Notes:** Corresponds to Agent 5.7. Prompt detail similar to US024 (Q5.7.1).

**Acceptance Criteria:**
  - **AC_ID:** US035_AC1
    **Description:** Verify generation of specific LLM prompt for fixing code/tests.
    **Gherkin:**
    ```gherkin
    Given confirmed '调试分析报告' and user's repair instructions are received by '5.7 代码/测试修复指令生成Agent'
    And relevant original code (product or test) is available from PKBM
    When the agent generates a fix prompt for 'Roo code LLM'
    Then the prompt is extremely specific, including filename, exact code snippet to modify, clear description of logic/behavior to correct, and expected correct behavior (similar detail to US024/Q5.7.1)
    And this '修复指令_[子任务ID]_v1.x' is stored in PKBM and passed to '4.1 代码生成/修改执行协调Agent' to restart the generate-validate-test cycle for the fix.
    ```

---

**User Story ID:** US036
**As a:** System
**I want to:** mark a sub-task's final code as '待集成' (Pending Integration) in PKBM once it passes all tests and all its pre-requisite tasks (as per the '编码任务计划') are also 'Test Passed'
**So that:** completed and validated code modules are clearly identified for integration into the main project.
**Notes:** Corresponds to Agent 6.1. Considers sequential task dependencies (Q6.1.1).

**Acceptance Criteria:**
  - **AC_ID:** US036_AC1
    **Description:** Verify marking sub-task as '待集成' (Pending Integration).
    **Gherkin:**
    ```gherkin
    Given a sub-task is marked '测试通过' in PKBM
    And '编码任务计划 v1.0' is available
    When '6.1 子任务完成与集成准备Agent' is activated
    Then the agent checks if all pre-requisite tasks for this sub-task (as per '编码任务计划 v1.0') are also '测试通过' (Q6.1.1 - tasks are sequential)
    And if all dependencies are met, the sub-task's final code version in PKBM is marked as '待集成'
    And '6.2 代码集成协调Agent' is notified of new '待集成' code.
    ```

---

**User Story ID:** US037
**As a:** Developer
**I want to:** be provided with instructions and suggested Git commands (via web interface) to help me merge '待集成' code modules into the project's main development branch, including guidance for resolving any merge conflicts
**So that:** I can integrate completed features into the codebase, and a log of this integration (branches, hashes, files, conflict summary) is stored in PKBM.
**Notes:** Corresponds to Agent 6.2. 'Assistance' is instructions/commands (Q6.2.1).

**Acceptance Criteria:**
  - **AC_ID:** US037_AC1
    **Description:** Verify assistance for code integration and logging.
    **Gherkin:**
    ```gherkin
    Given '6.2 代码集成协调Agent' receives a list of '待集成' code modules/files
    When the agent provides instructions and suggested Git commands via web interface to the user for merging code into the main development branch (Q6.2.1a)
    And the user performs the merge, resolving conflicts if any, with agent's guidance
    Then an integration log ('集成日志_[批次ID]_v1.0') is created in PKBM, recording merged branches, commit hashes, file list, and conflict resolution summary (Q6.2.1b)
    And '6.3 构建与冒烟测试Agent' is notified upon completion.
    ```

---

**User Story ID:** US038
**As a:** Developer
**I want to:** be assisted (via web interface providing commands/instructions) to perform a full project build and run a predefined set of smoke tests after code integration, and then provide the build/smoke test results back to the system
**So that:** the stability of the integrated codebase is quickly verified, and results are stored in PKBM.
**Notes:** Corresponds to Agent 6.3. Build scripts/smoke test plan from PKBM, generated by other agents (Q6.3.1, Q6.3.2). Results provided via web text input (Q6.3.3).

**Acceptance Criteria:**
  - **AC_ID:** US038_AC1
    **Description:** Verify coordination of project build and smoke testing.
    **Gherkin:**
    ```gherkin
    Given code has been integrated and '6.3 构建与冒烟测试Agent' is activated
    And project build scripts/instructions and a smoke test plan are available from PKBM (generated by other agents - Q6.3.1, Q6.3.2)
    When the agent assists the user (providing commands/instructions via web interface) to perform a full project build and run smoke tests
    And the user provides build results (success/fail, logs) and smoke test results (pass/fail, issues) back via web text input (Q6.3.3)
    Then these results are stored in PKBM as '构建日志_[批次ID]_v1.0' and '冒烟测试报告_[批次ID]_v1.0'.
    ```
  - **AC_ID:** US038_AC2
    **Description:** Verify next steps based on build/smoke test outcome.
    **Gherkin:**
    ```gherkin
    Given build and smoke test results are recorded
    When '6.3 构建与冒烟测试Agent' analyzes them
    Then if build or smoke tests fail, the issue is flagged (potentially requiring回溯 to code fix or integration)
    And if both pass, '6.4 技术文档更新Agent - 代码注释' and '6.6 人工审核点6协调Agent' are notified.
    ```

---

**User Story ID:** US039
**As a:** Developer
**I want to:** have the system generate compliant, explanatory code comments for main classes, functions/methods, and complex logic blocks in the integrated code, and then be provided with a prompt (via web interface) to guide an LLM (Roo code plugin) to apply these comments to the actual code files
**So that:** the final codebase is well-documented with inline comments, improving maintainability, and the commented code is marked as final in PKBM.
**Notes:** Corresponds to Agent 6.4. Comments follow project spec (Q6.4.1). Applied via LLM prompt (Q6.4.2).

**Acceptance Criteria:**
  - **AC_ID:** US039_AC1
    **Description:** Verify generation and application of code comments.
    **Gherkin:**
    ```gherkin
    Given '构建与冒烟测试通过' signal is received by '6.4 技术文档更新Agent - 代码注释'
    And a list of involved integrated code modules is available
    And '实现指导书' and '编码规范' (commenting section) for these modules are in PKBM
    When the agent generates comments for main classes, functions/methods, complex logic blocks according to the规范 (Q6.4.1)
    And provides a prompt via web interface to guide an LLM (Roo code plugin) to apply these comments to the actual code files (Q6.4.2)
    And the user confirms application
    Then the updated code (with comments) is marked as the final version in PKBM.
    ```

---

**User Story ID:** US040
**As a:** Developer
**I want to:** have the system update or generate API documentation (Markdown format) and project README files based on the latest code, designs, and requirements, and also generate Changelog entries (Keep a Changelog format, with task IDs) for the iteration
**So that:** key technical documentation is kept current with the development, and these updated documents are stored in PKBM.
**Notes:** Corresponds to Agent 6.5. API docs are Markdown (Q6.5.1). Changelog format (Q6.5.2).

**Acceptance Criteria:**
  - **AC_ID:** US040_AC1
    **Description:** Verify update/generation of API documentation, README, and Changelog.
    **Gherkin:**
    ```gherkin
    Given '构建与冒烟测试通过' signal is received by '6.5 技术文档更新Agent - API Docs & README'
    And relevant 'API详细设计文档', '高层架构设计文档', '需求规格说明书', existing API docs (e.g. Swagger), and README are available from PKBM
    When the agent processes these based on latest code and design
    Then API documentation is updated/generated in Markdown format (Q6.5.1)
    And the project README is updated (new features, install/usage changes)
    And Changelog entries are generated (Keep a Changelog format, with task/US IDs - Q6.5.2)
    And these updated documents are stored in PKBM.
    ```

---

**User Story ID:** US041
**As a:** Developer
**I want to:** be presented with a final acceptance package (original SRS, access to final code, updated docs, smoke test report, changelog) via the web interface for my ultimate approval or rejection (with reasons)
**So that:** I can perform a final validation that the delivered functionality meets all requirements and is stable before considering the iteration complete.
**Notes:** Corresponds to Agent 6.6. Similar review to US009. No 'conditional approval'; rejection triggers further work (Q6.6.1).

**Acceptance Criteria:**
  - **AC_ID:** US041_AC1
    **Description:** Verify presentation of final acceptance package.
    **Gherkin:**
    ```gherkin
    Given build/smoke tests passed and docs are initially updated
    When '6.6 人工审核点6协调Agent' prepares the final acceptance package
    Then the package presented to the user (User/Product Owner) via web interface includes: original SRS, access to final code, updated API docs and README, smoke test report, and Changelog, along with a checklist.
    ```
  - **AC_ID:** US041_AC2
    **Description:** Verify handling of 'Approve Release' outcome.
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the final acceptance package
    When the user selects 'Approve Release'
    Then the acceptance result and any feedback are recorded in PKBM as '最终验收报告_v1.0'
    And the process for this iteration/project is considered complete.
    ```
  - **AC_ID:** US041_AC3
    **Description:** Verify handling of 'Reject' outcome (no conditional approval).
    **Gherkin:**
    ```gherkin
    Given the user has reviewed the final acceptance package
    When the user selects 'Reject' and provides reasons (Q6.6.1)
    Then the rejection and feedback are recorded in PKBM as '最终验收报告_v1.0'
    And this triggers a new small iteration or problem-fixing flow based on the feedback.
    ```

---

**User Story ID:** US042
**As a:** Developer
**I want to:** have the system generate a project summary report after project completion or major milestones, detailing completed features, LLM agent performance (e.g., first-pass quality, modification extent), common issues from reviews, process bottlenecks, and distillable knowledge/patterns (as text notes for a 'Best Practices/Prompt Library' in PKBM)
**So that:** I gain insights into the project's execution and the system's performance, and valuable knowledge is captured for future process improvements or LLM fine-tuning.
**Notes:** Corresponds to Agent 6.7. Performance metrics (Q6.7.1). Knowledge as text notes (Q6.7.2). Runs at project end or major milestones (Q6.7.3).

**Acceptance Criteria:**
  - **AC_ID:** US042_AC1
    **Description:** Verify generation of project summary report and knowledge distillation.
    **Gherkin:**
    ```gherkin
    Given a project is completed or a major milestone is reached (Q6.7.3)
    And '6.7 项目总结与知识沉淀Agent' has access to all project data in PKBM
    When the agent runs
    Then a project summary report is generated, including: completed features, LLM agent performance (first-pass quality, modification extent, subjective conformance rating - Q6.7.1), common review issues, process bottlenecks/timing, and distilled knowledge/patterns
    And this report and distilled knowledge (as text notes for 'Best Practices/Prompt Library' - Q6.7.2) are stored in PKBM.
    ```

---

**User Story ID:** US043
**As a:** System
**I want to:** employ a Central Coordinator Agent to manage overall process flow, including handling requests for回溯 to earlier stages if a significant design issue is identified late in the process
**So that:** the system can gracefully handle situations requiring major re-planning or re-design by pausing current work and redirecting to an appropriate earlier agent.
**Notes:** Derived from Q5.6.1. This agent orchestrates high-level flow adjustments.

**Acceptance Criteria:**
  - **AC_ID:** US043_AC1
    **Description:** Verify Central Coordinator Agent manages process回溯.
    **Gherkin:**
    ```gherkin
    Given an agent (e.g., '5.6 人工审核点5协调Agent') identifies a need for design回溯
    And sends a structured message to the 'Central Coordinator Agent' detailing the issue and target回溯 point (Q5.6.1)
    When the 'Central Coordinator Agent' receives this message
    Then the current sub-task sequence is paused
    And the process flow is redirected to the specified earlier agent with the relevant context/issue.
    ```

---

**User Story ID:** US044
**As a:** Developer
**I want to:** be notified via the web interface when an agent encounters an unrecoverable internal error (after 3 retries) or requires manual intervention
**So that:** I am promptly informed of issues requiring my attention so I can take corrective action.
**Notes:** Derived from QG1, QG2. Errors displayed in the web UI's session/log area.

**Acceptance Criteria:**
  - **AC_ID:** US044_AC1
    **Description:** Verify user notification for unrecoverable agent errors or manual intervention needs.
    **Gherkin:**
    ```gherkin
    Given an agent encounters an unrecoverable internal error after 3 retries (QG1)
    Or an agent explicitly requires manual intervention
    When this situation occurs
    Then a notification detailing the error or required action is displayed to the user in the web interface's session/log area (QG2).
    ```

---

**User Story ID:** US045
**As a:** Developer
**I want to:** interact with the system through a web interface that provides a session-like chat/log area for agent outputs and my inputs, with clear indications when my input is awaited
**So that:** I have a consistent and understandable way to interact with all agents and monitor the process.
**Notes:** Derived from QG3, QG4, and general UI descriptions.

**Acceptance Criteria:**
  - **AC_ID:** US045_AC1
    **Description:** Verify user interaction via web interface session/log area.
    **Gherkin:**
    ```gherkin
    Given the user is interacting with the system
    When an agent outputs information or requires user input
    Then this is displayed in a session-like chat/log area on the web interface (QG3)
    And when user input is awaited, this is clearly indicated (QG3)
    And the user can provide input via a text area in this interface (QG4).
    ```

---
```

14. **2.5 架构设计文档编纂Agent (Architecture Design Compiler Agent):**

以下是我将生成的 **高层架构设计文档 v1.0 (High-Level Architecture Design Document v1.0)**：

```markdown
# High-Level Architecture Design Document v1.0

**Document ID:** HighLevelArchitectureDesignDocument_v1.0
**Version:** 1.0
**Timestamp:** 2023-10-29T08:00:00Z 
**Generated By:** 2.5 Architecture Design Compiler Agent
**Based On:**
*   ProjectMetadata_v1.0
*   HighLevelComponentDefinition_v1.0
*   ComponentInterfaceDraft_v1.0
*   ArchitectureDiagramDescription_v1.0 (Synthesized by this agent)

## 1. Introduction

This document outlines the high-level architecture for the "多Agent软件开发自动化系统 (Simplified Version)". The system aims to automate various stages of the software development lifecycle, guided by user input and reviews through a minimalistic web interface. It leverages a collection of specialized AI agents (built with AutoGen) and a file-based Project Knowledge Base Manager (PKBM) for storing and managing all project artifacts. This architecture is designed to support a linear, sequential workflow with defined human review checkpoints.

## 2. Technology Selection and Rationale

The technology stack for this project has been selected based on the requirements outlined in `ProjectMetadata_v1.0` and subsequent clarifications. The key choices and their rationale are as follows:

*   **Primary Programming Language:**
    *   **Choice:** Python
    *   **Rationale:** Rich availability of AI/ML libraries, extensive community support, and suitability for rapid prototyping and agent development. This aligns with the core nature of the system. (As per `ProjectMetadata_v1.0`)
*   **Agent Framework:**
    *   **Choice:** AutoGen
    *   **Rationale:** Selected as the framework for developing and managing the specialized AI agents that form the backbone of the automation pipeline. (As per `ProjectMetadata_v1.0`)
*   **Web Service Framework:**
    *   **Choice:** Flask
    *   **Rationale:** Chosen for its minimalism and flexibility, making it suitable for quickly developing the basic web interface and API endpoints required for user interaction and workflow coordination. (As per `ProjectMetadata_v1.0`)
*   **Project Knowledge Base Manager (PKBM):**
    *   **Type:** File-based Knowledge Base
    *   **Organization:** Specific directory structure with individual files for each artifact. Information linking via filename and directory conventions.
    *   **Rationale:** User preference for simplicity, minimal deployment/maintenance overhead, and current insensitivity to performance/efficiency for this aspect. Traditional database systems are explicitly excluded. Write access will be strictly serialized. (As per `ProjectMetadata_v1.0` and QG8)
*   **LLM Interaction:**
    *   **Method:** Standard REST API calls via a Web Service (LLM Interaction Gateway Service).
    *   **Rationale:** Provides a standard way to integrate with external LLMs (e.g., Roo code LLM) for tasks like code generation, analysis, etc. (As per `ProjectMetadata_v1.0`)
*   **Frontend Technology:**
    *   **Approach:** AI-assisted design and generation for a minimalistic UI, focusing on basic operations, with user review.
    *   **Rationale:** The user is not familiar with frontend development and prefers a simple interface sufficient for core interactions, with low sensitivity to performance. (As per `ProjectMetadata_v1.0`)

## 3. Component List and Responsibilities

The system is decomposed into the following high-level components, each with specific responsibilities as defined in `HighLevelComponentDefinition_v1.0`:

1.  **Web_UI_Service (Flask-based):**
    *   **Description:** Handles all direct user interactions via a minimalistic web interface. Acts as the primary entry and exit point for user-driven data and system feedback.
    *   **Core Responsibilities:** Rendering web pages for user input (initial requirements, Q&A, review feedback, LLM prompt/code copy-paste); displaying system-generated information, questions, documents for review, and agent outputs/logs; receiving and validating basic user submissions; forwarding user requests to the 'Workflow_Management_Service'; presenting notifications and error messages.

2.  **Workflow_Management_Service:**
    *   **Description:** Orchestrates the overall end-to-end software development automation pipeline, managing the sequence of stages and agent activations.
    *   **Core Responsibilities:** Managing the state and progression of the workflow; receiving triggers from the 'Web_UI_Service'; initiating tasks for the 'Agent_Orchestration_Service'; coordinating human review points; interpreting outputs to determine the next step; invoking 'Central_Coordinator_Agent_Logic' for process回溯.

3.  **Agent_Orchestration_Service (AutoGen-based):**
    *   **Description:** Manages the execution and interaction of individual AutoGen agents responsible for specific tasks within the software development lifecycle.
    *   **Core Responsibilities:** Hosting, instantiating, and managing AutoGen agents; receiving task execution requests; configuring agents with context and access to shared services; overseeing agent task execution; handling inter-agent communication (if applicable); collecting and relaying agent outputs; implementing 'Central_Coordinator_Agent_Logic'.

4.  **Project_Knowledge_Base_Manager_Service (PKBM_Service):**
    *   **Description:** Manages all persistent storage and retrieval of project artifacts within the file-based system.
    *   **Core Responsibilities:** Providing an API for CRUD operations on artifacts (files); enforcing directory structures and file naming conventions (including versioning and tags); managing serialized write access; supporting artifact retrieval based on filenames/paths and simulated tag/keyword search.

5.  **LLM_Interaction_Gateway_Service:**
    *   **Description:** Provides a centralized and standardized interface for all internal services/agents to communicate with external Large Language Models (LLMs).
    *   **Core Responsibilities:** Abstracting LLM interaction specifics; managing API calls (REST-based); handling request construction and response parsing; implementing retry mechanisms; managing LLM API keys (simplified direct configuration); logging LLM interactions.

## 4. Component Interaction and Major Interfaces

The components interact through a set of defined interfaces, primarily internal function calls for co-located Python services and HTTP-based API calls between the `Web_UI_Service` and the `Workflow_Management_Service`. The `ComponentInterfaceDraft_v1.0` document provides a detailed list of these interfaces. Key interaction patterns include:

*   **User to System:** The `Web_UI_Service` receives all user inputs (e.g., initial requirements, review decisions, LLM outputs) and forwards them to the `Workflow_Management_Service` via HTTP API calls (e.g., `POST /workflow/initiate`, `POST /workflow/{id}/submit_review`).
*   **System to User:** The `Workflow_Management_Service` instructs the `Web_UI_Service` to display information (e.g., questions, documents for review, LLM prompts for user action, status updates, errors). This can be achieved via the UI polling an endpoint or a push mechanism (e.g., `GET /ui/display_request` or WebSocket message).
*   **Workflow Orchestration:** The `Workflow_Management_Service` directs the `Agent_Orchestration_Service` to execute specific agents or stages via internal Python function calls (e.g., `agent_orchestrator.execute_stage()`). It also handles回溯 requests through `agent_orchestrator.initiate_回溯()`.
*   **Agent Operations:** Agents within the `Agent_Orchestration_Service` interact with:
    *   `Project_Knowledge_Base_Manager_Service`: For storing and retrieving artifacts (e.g., `pkbm_service.store_artifact()`, `pkbm_service.retrieve_artifact()`) via internal Python function calls.
    *   `LLM_Interaction_Gateway_Service`: To make calls to external LLMs (e.g., `llm_gateway.execute_llm_call()`) via internal Python function calls.
*   **Feedback Loop:** The `Agent_Orchestration_Service` reports stage completion status or errors back to the `Workflow_Management_Service` (e.g., `workflow_manager.handle_stage_completion()`).

For detailed interface definitions including specific endpoints, parameters, and expected payloads/responses, refer to `ComponentInterfaceDraft_v1.0`.

## 5. Architecture Diagram

The following textual description and Mermaid diagram represent the high-level system architecture, illustrating the main components and their primary interactions.

### 5.1. Textual Description of Architecture

The system architecture is designed around a central `Workflow_Management_Service` that orchestrates the entire process. User interaction is exclusively through the `Web_UI_Service`, which communicates user inputs and review decisions to the `Workflow_Management_Service`.

The `Workflow_Management_Service` then delegates specific tasks to the `Agent_Orchestration_Service`. This service, built on AutoGen, manages the execution of various specialized AI agents. These agents perform the core tasks of requirement analysis, design, code generation (via the `LLM_Interaction_Gateway_Service`), review, and testing.

All persistent data, including requirements, design documents, generated code, and review feedback, is managed by the `Project_Knowledge_Base_Manager_Service (PKBM_Service)`, which provides a file-based storage solution. Agents access and store information in the PKBM via this service.

The `LLM_Interaction_Gateway_Service` acts as a dedicated interface for all communications with external Large Language Models, ensuring consistent handling of API calls, retries, and credential management.

Error handling and notifications are propagated from the executing services up to the `Workflow_Management_Service`, which then informs the user via the `Web_UI_Service`. A `Central_Coordinator_Agent_Logic` (within the `Agent_Orchestration_Service`, invoked by `Workflow_Management_Service`) handles requests for major process回溯.

### 5.2. Mermaid Diagram Description

```mermaid
graph TD
    subgraph User Interaction Layer
        User([User]) --> Web_UI_Service;
        Web_UI_Service --> User;
    end

    subgraph Application Core
        Web_UI_Service -- "User Input / Review Decisions (API CID001-CID005)" --> Workflow_Management_Service;
        Workflow_Management_Service -- "Display Requests / Status (API CID006)" --> Web_UI_Service;

        Workflow_Management_Service -- "Execute Stage / Initiate 回溯 (Internal Call CID007, CID008)" --> Agent_Orchestration_Service;
        Agent_Orchestration_Service -- "Stage Completion / Errors (Internal Call CID012)" --> Workflow_Management_Service;

        Agent_Orchestration_Service -- "Store/Retrieve Artifacts (Internal Call CID009, CID010)" --> PKBM_Service[Project_Knowledge_Base_Manager_Service];
        Agent_Orchestration_Service -- "LLM Calls (Internal Call CID011)" --> LLM_Interaction_Gateway_Service;
    end

    subgraph External Services
        LLM_Interaction_Gateway_Service -- "REST API" --> External_LLM["External LLM (e.g., Roo code LLM)"];
    end

    %% Styling
    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class Web_UI_Service,Workflow_Management_Service,Agent_Orchestration_Service,PKBM_Service,LLM_Interaction_Gateway_Service component;
```

17. **2.8 任务依赖与顺序规划Agent (Task Dependency & Sequencing Agent):**

以下是我分析后生成的 **编码任务计划 v1.0**:

```json
{
  "document_id": "CodingTaskPlan_v1.0",
  "version": "1.0",
  "timestamp": "CURRENT_TIMESTAMP",
  "generated_by": "2.8 Task Dependency & Sequencing Agent",
  "based_on_subtask_list": "CodingSubtaskList_v1.0",
  "task_plan_description": "A strictly sequential development plan. Tasks are ordered by priority. Each task lists its direct prerequisite sub_task_ids.",
  "tasks": [
    {
      "sub_task_id": "ST016",
      "description": "Implement the Project_Knowledge_Base_Manager_Service (PKBM_Service) core logic for file operations.",
      "priority": 1,
      "dependencies": []
    },
    {
      "sub_task_id": "ST017",
      "description": "Implement the `store_artifact` and `retrieve_artifact` interfaces in PKBM_Service.",
      "priority": 2,
      "dependencies": ["ST016"]
    },
    {
      "sub_task_id": "ST018",
      "description": "Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.",
      "priority": 3,
      "dependencies": []
    },
    {
      "sub_task_id": "ST019",
      "description": "Implement the `execute_llm_call` interface in LLM_Interaction_Gateway_Service.",
      "priority": 4,
      "dependencies": ["ST018"]
    },
    {
      "sub_task_id": "ST013",
      "description": "Implement the core Agent_Orchestration_Service (AutoGen) setup and agent management.",
      "priority": 5,
      "dependencies": []
    },
    {
      "sub_task_id": "ST014",
      "description": "Implement the `execute_stage` and `initiate_回溯` entry points in Agent_Orchestration_Service.",
      "priority": 6,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST008",
      "description": "Implement the core Workflow_Management_Service logic for state management and progression.",
      "priority": 7,
      "dependencies": []
    },
    {
      "sub_task_id": "ST011",
      "description": "Implement Workflow_Management_Service interface to Agent_Orchestration_Service for stage execution.",
      "priority": 8,
      "dependencies": ["ST008", "ST014"]
    },
    {
      "sub_task_id": "ST015",
      "description": "Implement the callback interface (CID012) from Agent_Orchestration_Service to Workflow_Management_Service.",
      "priority": 9,
      "dependencies": ["ST013", "ST008"]
    },
    {
      "sub_task_id": "ST009",
      "description": "Implement Workflow_Management_Service handlers for API calls from Web_UI_Service.",
      "priority": 10,
      "dependencies": ["ST008", "ST011"]
    },
    {
      "sub_task_id": "ST010",
      "description": "Implement Workflow_Management_Service logic to request display updates on Web_UI_Service.",
      "priority": 11,
      "dependencies": ["ST008"]
    },
    {
      "sub_task_id": "ST001",
      "description": "Implement the base Flask application setup for the Web_UI_Service.",
      "priority": 12,
      "dependencies": []
    },
    {
      "sub_task_id": "ST007",
      "description": "Implement the Web_UI_Service general display logic for status updates, logs, and error messages.",
      "priority": 13,
      "dependencies": ["ST001", "ST010"]
    },
    {
      "sub_task_id": "ST002",
      "description": "Implement the Web_UI_Service view for initial requirement input.",
      "priority": 14,
      "dependencies": ["ST001", "ST009", "ST007"]
    },
    {
      "sub_task_id": "ST003",
      "description": "Implement the Web_UI_Service view for displaying questions and receiving answers.",
      "priority": 15,
      "dependencies": ["ST001", "ST009", "ST007"]
    },
    {
      "sub_task_id": "ST004",
      "description": "Implement the Web_UI_Service view for presenting review materials and capturing review decisions.",
      "priority": 16,
      "dependencies": ["ST001", "ST009", "ST007"]
    },
    {
      "sub_task_id": "ST005",
      "description": "Implement the Web_UI_Service view for LLM prompt/code copy-paste interaction.",
      "priority": 17,
      "dependencies": ["ST001", "ST009", "ST007"]
    },
    {
      "sub_task_id": "ST006",
      "description": "Implement the Web_UI_Service view for submitting test execution results.",
      "priority": 18,
      "dependencies": ["ST001", "ST009", "ST007"]
    },
    {
      "sub_task_id": "ST020",
      "description": "Implement Agent: 0.1 User Initial Request Receiver Agent.",
      "priority": 19,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST021",
      "description": "Implement Agent: 0.3 Initial Intent Recognition Agent.",
      "priority": 20,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A0_4",
      "description": "Implement Agent: 0.4 Project Type & Tech Stack Elicitation Agent.",
      "priority": 21,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A1_1",
      "description": "Implement Agent: 1.1 Requirement Detail & Boundary Miner Agent.",
      "priority": 22,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A1_2",
      "description": "Implement Agent: 1.2 User Story Generation Agent.",
      "priority": 23,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A1_3",
      "description": "Implement Agent: 1.3 Acceptance Criteria Definition Agent.",
      "priority": 24,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A1_4",
      "description": "Implement Agent: 1.4 Requirements Specification Compiler Agent.",
      "priority": 25,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A1_5",
      "description": "Implement Agent: 1.5 Human Review Point 1 Coordinator.",
      "priority": 26,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A2_1",
      "description": "Implement Agent: 2.1 Technology Selection Advisor Agent.",
      "priority": 27,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A2_2",
      "description": "Implement Agent: 2.2 High-Level Component Definition Agent.",
      "priority": 28,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A2_3",
      "description": "Implement Agent: 2.3 Component Interface Draft Agent.",
      "priority": 29,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A2_4",
      "description": "Implement Agent: 2.4 Architecture Diagram Description Agent.",
      "priority": 30,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A2_5",
      "description": "Implement Agent: 2.5 Architecture Design Compiler Agent.",
      "priority": 31,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A2_6",
      "description": "Implement Agent: 2.6 Human Review Point 2 Coordinator.",
      "priority": 32,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A2_7",
      "description": "Implement Agent: 2.7 Coding Task Breakdown Agent.",
      "priority": 33,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A2_8",
      "description": "Implement Agent: 2.8 Task Dependency & Sequencing Agent (Self-implementation, meta-task).",
      "priority": 34,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A3_1",
      "description": "Implement Agent: 3.1 Current Subtask Context Preparer.",
      "priority": 35,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A3_2",
      "description": "Implement Agent: 3.2 API Detailed Design Agent.",
      "priority": 36,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A3_3",
      "description": "Implement Agent: 3.3 Data Model Detailed Design Agent.",
      "priority": 37,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A3_4",
      "description": "Implement Agent: 3.4 Class & Function Logic Design Agent.",
      "priority": 38,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A3_5",
      "description": "Implement Agent: 3.5 File & Directory Structure Planner Agent.",
      "priority": 39,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A3_6",
      "description": "Implement Agent: 3.6 Security Considerations & Checkpoint Definer Agent.",
      "priority": 40,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A3_7",
      "description": "Implement Agent: 3.7 Detailed Design Aggregation & Instruction Generator Agent.",
      "priority": 41,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A3_8",
      "description": "Implement Agent: 3.8 Human Review Point 3 Coordinator.",
      "priority": 42,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A4_1",
      "description": "Implement Agent: 4.1 Code Generation/Modification Execution Coordinator.",
      "priority": 43,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A4_2",
      "description": "Implement Agent: 4.2 Code Syntax & Basic Formatting Validator Agent.",
      "priority": 44,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A4_3",
      "description": "Implement Agent: 4.3 Static Code Analyzer Agent.",
      "priority": 45,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A5_1",
      "description": "Implement Agent: 5.1 AI Code Reviewer Agent.",
      "priority": 46,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A5_2",
      "description": "Implement Agent: 5.2 Human Review Point 4 Coordinator - Code & AI Review.",
      "priority": 47,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A5_3",
      "description": "Implement Agent: 5.3 Unit Test Case Generator Agent.",
      "priority": 48,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A5_4",
      "description": "Implement Agent: 5.4 Test Execution Coordinator.",
      "priority": 49,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A5_5",
      "description": "Implement Agent: 5.5 Test Result Analyzer & Debugging Guide Agent.",
      "priority": 50,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A5_6",
      "description": "Implement Agent: 5.6 Human Review Point 5 Coordinator - Debugging & Iteration.",
      "priority": 51,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A5_7",
      "description": "Implement Agent: 5.7 Code/Test Remediation Instruction Agent.",
      "priority": 52,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A6_1",
      "description": "Implement Agent: 6.1 Subtask Completion & Integration Prep Agent.",
      "priority": 53,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A6_2",
      "description": "Implement Agent: 6.2 Code Integration Coordinator.",
      "priority": 54,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A6_3",
      "description": "Implement Agent: 6.3 Build & Smoke Test Agent.",
      "priority": 55,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A6_4",
      "description": "Implement Agent: 6.4 Technical Documentation Updater - Code Comments.",
      "priority": 56,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A6_5",
      "description": "Implement Agent: 6.5 Technical Documentation Updater - API Docs & README.",
      "priority": 57,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST_A6_6",
      "description": "Implement Agent: 6.6 Human Review Point 6 Coordinator - Final Acceptance.",
      "priority": 58,
      "dependencies": ["ST013", "ST017"]
    },
    {
      "sub_task_id": "ST_A6_7",
      "description": "Implement Agent: 6.7 Project Summary & Knowledge Distillation Agent.",
      "priority": 59,
      "dependencies": ["ST013", "ST017", "ST019"]
    },
    {
      "sub_task_id": "ST012",
      "description": "Implement Workflow_Management_Service interface to Agent_Orchestration_Service for process回溯.",
      "priority": 60,
      "dependencies": ["ST008", "ST014"]
    },
    {
      "sub_task_id": "ST_LAST",
      "description": "Implement the Central Coordinator Agent Logic (within Agent_Orchestration_Service).",
      "priority": 61,
      "dependencies": ["ST013", "ST012"]
    }
  ]
}
```

**计划说明:**
1.  **优先级 (Priority):** 数字越小，代表越早执行。我已将基础服务（PKBM, LLM网关）、核心组件框架（Agent编排服务、工作流管理服务、Web UI基础）的优先级设置得较高。
2.  **依赖 (Dependencies):** 列出了每个子任务直接依赖的前置子任务ID。一个任务只有在其所有依赖任务完成后才能开始。
3.  **Agent 实现:** 各个具体Agent (0.1 到 6.7) 的实现任务 (ST020, ST021, ST\_A0\_4 至 ST\_A6\_7) 被安排在它们所依赖的核心服务组件（如ST013 - Agent编排服务核心, ST017 - PKBM服务接口, ST019 - LLM网关接口）建立之后。它们之间的顺序大致遵循了原始需求中定义的流程顺序。
4.  **回溯逻辑 (ST012, ST_LAST):** 与回溯相关的任务被安排在较后，因为它们依赖于基本工作流和Agent编排服务的稳定。
5.  **严格顺序:** 此计划严格遵循了“任务不可以并行执行”的约束。后续的流程协调器将按此`priority`顺序逐个启动任务，并确保其`dependencies`已满足。
18. **3.1 当前子任务上下文准备Agent (Current Subtask Context Preparer):**

## Current Subtask Context Package for ST018

**Package ID:** `Context_ST018_v1.0`
**Timestamp:** `2023-10-30T10:00:00Z` (Illustrative)
**Prepared For Subtask:** `ST018`
**Subtask Description (from CodingTaskPlan_v1.0):** "Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls."

### 1. Relevant User Stories & Acceptance Criteria Context

This sub-task (ST018) is a foundational technical enabler. It directly supports the architectural component `LLM_Interaction_Gateway_Service`, which is essential for multiple user stories involving LLM interactions. While ST018 doesn't map to a single end-user story, its successful implementation is crucial for:

*   **US024:** "As a Developer, I want to...generate an extremely specific, context-rich prompt text for an external code generation LLM (Roo code LLM)..." (This gateway will send that prompt).
*   **US029:** "As a Developer, I want to...have an AI Code Reviewer agent analyze the code..." (This agent will likely use an LLM via the gateway).
*   **US031:** "As a Developer, I want to...have the system generate unit test case code..." (LLM for test generation via gateway).
*   **US035:** "As a Developer, I want to...have the system generate a specific LLM prompt for fixing identified bugs..." (Fixes via LLM using the gateway).

The "acceptance criteria" for ST018 itself will be more technical, focusing on the correct implementation of HTTP call functionalities as per its defined interface (CID011) and component responsibilities. These will be further detailed by the `3.4 Class & Function Logic Design Agent`.

### 2. Architectural Context

**Source Document:** `HighLevelArchitectureDesignDocument_v1.0`

*   **Component Definition (LLM_Interaction_Gateway_Service):**
    *   **Description:** Provides a centralized and standardized interface for all internal services/agents to communicate with external Large Language Models (LLMs).
    *   **Core Responsibilities relevant to ST018:**
        *   Abstracting LLM interaction specifics.
        *   Managing API calls (REST-based).
        *   Handling request construction and response parsing.
        *   Implementing retry mechanisms.
        *   Logging LLM interactions.
        *   (Managing LLM API keys - simplified direct configuration for this task).

*   **Relevant Interface Definition (from `ComponentInterfaceDraft_v1.0` and HLDD Section 4):**
    *   **Interface ID:** `CID011` (Conceptual internal interface)
    *   **Interaction:** `Agent_Orchestration_Service` -> `LLM_Interaction_Gateway_Service`
    *   **Type:** Internal Python function call within the `LLM_Interaction_Gateway_Service` that will encapsulate the HTTP logic.
    *   **Conceptual Function Signature (to be refined by Agent 3.4):** `execute_llm_call(prompt_data: dict, target_llm_config: dict) -> dict`
    *   **Description:** "Sends a request to a configured LLM and returns the response. Handles actual HTTP communication."
    *   **ST018 Focus:** This subtask is about implementing the *actual HTTP communication* part within the `LLM_Interaction_Gateway_Service` that would be invoked by a function like `execute_llm_call`.

*   **Technology Selection for LLM Interaction:**
    *   Method: Standard REST API calls. Python's `requests` library is the anticipated choice for making these HTTP calls.

### 3. Project Metadata Summary

**Source Document:** `ProjectMetadata_v1.0`

*   **Primary Programming Language:** Python
*   **LLM Interaction Method:** Standard REST API calls via Web Service (this gateway).
*   **Key Simplifications & Constraints:**
    *   Reduced emphasis on efficiency.
    *   Focus on simplicity and success rate.
    *   Security considerations are explicitly deprioritized; all permissions should be as open as possible to ensure functionality.

### 4. Coding Standards and Guidelines

**Source:** User-provided "编码规范" (Full text to be provided to LLM for code generation).
Key aspects relevant to this task include:
*   **Modular Design:** The HTTP call logic should be a well-defined part of the `LLM_Interaction_Gateway_Service`.
*   **Configuration Management:** Any configurable parts of HTTP calls (e.g., default timeouts, base URLs if they become configurable) should be managed via `@dataclass`. For ST018, API keys and specific LLM endpoints will likely be passed in `target_llm_config`.
*   **Error Handling:** Comprehensive `try-except` blocks for network errors, HTTP errors (4xx, 5xx), timeouts, JSON parsing errors. Error messages should be clear.
*   **Logging:** Log requests (perhaps not full data for brevity, but target URL and status), responses (status, errors), and execution times.
*   **Code Style:** Python (4-space indent, 79-char lines, snake_case, Type Hinting).
*   **Dependencies:** Prioritize Python standard library (`json`) and `requests` for HTTP.
*   **No Comments/DocStrings:** Code should be self-explanatory.

### 5. Existing Code Snippets for Modification

*   Not applicable for ST018, as this is a new implementation.

### 6. Additional Notes or Constraints for ST018

*   **Focus:** The core task is to implement robust functions/methods that can make HTTP GET, POST (and potentially other methods if deemed necessary by detailed design) requests, primarily targeting JSON-based REST APIs.
*   **Input:** These functions will likely take parameters such as URL, HTTP method, headers, JSON payload, and timeout values.
*   **Output:** They should return the parsed JSON response or raise specific, informative exceptions on failure.
*   **Retry Logic:** Basic retry logic for transient network errors or specific HTTP status codes (e.g., 502, 503, 504) should be considered, as per component responsibility.
*   **Configuration of LLM Endpoints/Keys:** For this subtask, assume that specific LLM API endpoints and keys will be provided dynamically (e.g., as part of `target_llm_config` in the conceptual `execute_llm_call` function). The HTTP call logic itself doesn't need to manage a persistent store of these but should use what's provided.
*   **Security:** As per project guidelines, security is not a concern. No complex authentication schemes need to be implemented beyond what's typical for standard API key usage (e.g., passing a key in headers). SSL certificate verification should be enabled by default with `requests` but can be made configurable if absolutely necessary for specific non-production LLM endpoints (though generally not recommended). Prioritize successful connection.

19. **3.2 API详细设计Agent (API Detailed Design Agent):**

# Detailed API Design for External LLM Calls (Subtask ST018)

**Document ID:** `APIDetailedDesign_ST018_v1.0`
**Version:** 1.0
**Timestamp:** 2023-10-30T10:15:00Z
**Generated By:** Agent 3.2 API Detailed Design Agent
**For Subtask:** ST018 - Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
**Based On:** Context_ST018_v1.0

## 1. Introduction

This document outlines the detailed design considerations for the HTTP calls that the `LLM_Interaction_Gateway_Service` will make to external Large Language Model (LLM) APIs. It does not define an API exposed by the `LLM_Interaction_Gateway_Service` itself (which is an internal Python interface), but rather the structure of requests it must be capable of sending and the responses it must be prepared to handle from various third-party LLM providers. The primary goal is to ensure robust and flexible communication with diverse LLM REST APIs.

Given the project's explicit prioritization of success rate and simplicity over security, authentication mechanisms will be straightforward (e.g., passing API keys directly as configured), and SSL/TLS verification will be standard but potentially configurable for non-production endpoints if absolutely necessary (though generally not recommended, this flexibility is noted due to the emphasis on successful connection).

## 2. Generic HTTP Request Structure for LLM Calls

The internal functions within `LLM_Interaction_Gateway_Service` responsible for making HTTP calls should be designed to accommodate the following parameters to construct requests to external LLM APIs.

### 2.1. HTTP Method

*   **Supported Methods:** Primarily `POST`. `GET` may also be needed for some LLM utility endpoints (e.g., listing models, checking status). The internal client function should accept the method as a parameter.
    *   Example: `POST`, `GET`

### 2.2. URL

*   **Description:** The full URL of the external LLM API endpoint. This will be specific to each LLM provider and task.
*   **Type:** `string`
*   **Example:** `https://api.example-llm.com/v1/completions`

### 2.3. Request Headers

*   **Description:** A dictionary of HTTP headers. Common headers for LLM APIs include `Authorization`, `Content-Type`, and potentially custom provider-specific headers.
*   **Type:** `Dict[str, str]`
*   **Common Headers & Examples:**
    *   `Authorization`:
        *   Example (Bearer Token): `{"Authorization": "Bearer YOUR_API_KEY"}`
        *   Example (Custom Key Header): `{"X-Api-Key": "YOUR_API_KEY"}`
    *   `Content-Type`:
        *   Example: `{"Content-Type": "application/json"}`
    *   `Accept`:
        *   Example: `{"Accept": "application/json"}` (Usually default, but good to be explicit if needed)

### 2.4. Query Parameters

*   **Description:** Optional URL query parameters. Some LLM APIs might use these for versioning or minor configuration.
*   **Type:** `Optional[Dict[str, Any]]`
*   **Example:** `{"version": "2023-10-01"}`

### 2.5. Request Body (for `POST`, `PUT`, etc.)

*   **Description:** The payload sent to the LLM API, typically a JSON object. The structure is highly dependent on the specific LLM provider and the task (e.g., completion, chat, embedding).
*   **Format:** JSON
*   **Generic JSON Schema (Illustrative - actual schema varies by LLM):**
    ```json
    {
      "type": "object",
      "properties": {
        "model": {
          "type": "string",
          "description": "Identifier of the LLM model to use."
        },
        "prompt": {
          "type": "string",
          "description": "The input text or instruction for the LLM."
        },
        "messages": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "role": {"type": "string", "enum": ["system", "user", "assistant"]},
              "content": {"type": "string"}
            },
            "required": ["role", "content"]
          },
          "description": "For chat-based LLMs, a list of messages."
        },
        "max_tokens": {
          "type": "integer",
          "description": "Maximum number of tokens to generate."
        },
        "temperature": {
          "type": "number",
          "minimum": 0.0,
          "maximum": 2.0,
          "description": "Controls randomness. Higher values make output more random."
        },
        "stream": {
          "type": "boolean",
          "default": false,
          "description": "Whether to stream back partial progress."
        }
        // Other provider-specific parameters like top_p, presence_penalty, etc.
      },
      "anyOf": [
        {"required": ["model", "prompt"]},
        {"required": ["model", "messages"]}
      ]
    }
    ```
*   **Example Request Body:**
    ```json
    {
      "model": "example-text-davinci-003",
      "prompt": "Translate 'hello' to French.",
      "max_tokens": 50,
      "temperature": 0.5
    }
    ```

## 3. HTTP Response Handling

The internal functions must be prepared to handle various HTTP status codes and response body formats.

### 3.1. Successful Responses

*   **Status Codes:** `200 OK`, `201 Created`
*   **Response Body Format:** Typically JSON. The structure varies significantly by LLM provider.
*   **Generic JSON Schema for Successful Completion (Illustrative):**
    ```json
    {
      "type": "object",
      "properties": {
        "id": {"type": "string", "description": "A unique identifier for the response."},
        "object": {"type": "string", "description": "The type of object (e.g., 'text_completion')."},
        "created": {"type": "integer", "description": "Timestamp of creation."},
        "model": {"type": "string", "description": "The model used for the request."},
        "choices": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "text": {"type": "string"},
              "message": { // For chat models
                "type": "object",
                "properties": {
                  "role": {"type": "string"},
                  "content": {"type": "string"}
                }
              },
              "index": {"type": "integer"},
              "finish_reason": {"type": "string", "enum": ["stop", "length", "content_filter"]}
            }
          }
        },
        "usage": {
          "type": "object",
          "properties": {
            "prompt_tokens": {"type": "integer"},
            "completion_tokens": {"type": "integer"},
            "total_tokens": {"type": "integer"}
          }
        }
      },
      "required": ["id", "object", "created", "model", "choices"]
    }
    ```
*   **Action:** Parse the JSON response and return the relevant data.

### 3.2. Client Error Responses

*   **Status Codes:**
    *   `400 Bad Request`: Invalid request payload (e.g., missing required fields, malformed JSON).
    *   `401 Unauthorized`: Authentication failed (e.g., invalid API key).
    *   `403 Forbidden`: API key valid but lacks permission for the requested resource/action.
    *   `404 Not Found`: Requested resource/model does not exist.
    *   `422 Unprocessable Entity`: Request was well-formed but semantically incorrect.
    *   `429 Too Many Requests`: Rate limit exceeded.
*   **Response Body Format:** Often JSON, containing error details.
*   **Generic JSON Schema for Error Response (Illustrative):**
    ```json
    {
      "type": "object",
      "properties": {
        "error": {
          "type": "object",
          "properties": {
            "message": {"type": "string", "description": "A human-readable error message."},
            "type": {"type": "string", "description": "A short string identifying the error type (e.g., 'invalid_request_error', 'authentication_error')."},
            "param": {"type": ["string", "null"], "description": "The specific parameter that caused the error, if applicable."},
            "code": {"type": ["string", "null"], "description": "A provider-specific error code."}
          },
          "required": ["message", "type"]
        }
      }
    }
    ```
*   **Action:** Parse the JSON error if available. Log detailed error information. Raise specific, informative exceptions within the gateway service (e.g., `LLMAuthenticationError`, `LLMRateLimitError`, `LLMBadRequestError`).

### 3.3. Server Error Responses

*   **Status Codes:**
    *   `500 Internal Server Error`: An unexpected error occurred on the LLM provider's side.
    *   `502 Bad Gateway`: Upstream server issue.
    *   `503 Service Unavailable`: The LLM service is temporarily unavailable (e.g., overloaded, under maintenance).
    *   `504 Gateway Timeout`: Upstream server did not respond in time.
*   **Response Body Format:** May be JSON (similar to client errors), HTML, or plain text.
*   **Action:** Log the error. Implement retry mechanisms (as per component responsibility) for transient errors like 500, 503, 504. If retries fail, raise an appropriate exception (e.g., `LLMServiceUnavailableError`).

## 4. Authentication and Authorization

*   **Mechanism:** As specified in the `Context_ST018_v1.0` and project guidelines, authentication with external LLMs will be handled by passing API keys.
*   **Implementation:** The internal HTTP client functions must accept header information (which will contain the API key) as a parameter. The API key itself will likely be part of the `target_llm_config` passed to the higher-level `execute_llm_call` function and then formatted into the appropriate header (e.g., `Authorization: Bearer <key>` or `X-Api-Key: <key>`) by the logic within `LLM_Interaction_Gateway_Service` before making the actual HTTP call.
*   **Security Note:** No complex credential management, encryption of keys in transit (beyond HTTPS provided by the LLM endpoint), or obfuscation is required by this gateway service. The keys are assumed to be provided securely to the gateway's configuration.

## 5. Timeout Configuration

*   **Description:** Requests to external LLMs can sometimes take a long time. It's crucial to have configurable timeouts to prevent indefinite blocking.
*   **Types:**
    *   Connect Timeout: Time to establish a connection.
    *   Read Timeout: Time to wait for data after connection is established.
*   **Implementation:** The internal HTTP client functions should accept timeout values (e.g., a single float for total timeout or a tuple for connect/read timeouts, compatible with the `requests` library). Default values should be sensible (e.g., 10 seconds for connect, 60-300 seconds for read, depending on typical LLM response times).

## 6. Streaming Support (Optional but Recommended)

*   **Description:** Some LLM APIs support streaming responses, which is useful for long generations or interactive applications.
*   **Implementation:** If streaming is to be supported, the internal HTTP client function should have a parameter to enable it (e.g., `stream: bool`). When enabled, it should handle the response as a stream of events (e.g., Server-Sent Events or chunked transfer encoding) and yield data chunks. This detail will be further explored by Agent 3.4. For ST018 core logic, ensuring the HTTP client *can* make a request that initiates a stream is the primary concern.

---

20. **3.3 数据模型详细设计Agent (Data Model Detailed Design Agent):** 

No database table structures or complex file organization schemas for PKBM are directly relevant to the *core HTTP call logic* of ST018 itself.

Here is the Data Model Detailed Design:

```python
# Data Model Detailed Design for Subtask ST018
# Document ID: DataModelDetailedDesign_ST018_v1.0
# Version: 1.0
# Timestamp: 2023-10-30T10:30:00Z # Illustrative
# Generated By: 3.3 Data Model Detailed Design Agent
# Based On: Context_ST018_v1.0, APIDetailedDesign_ST018_v1.0

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

@dataclass
class HttpRequestConfig:
    """
    Represents the configuration for a single outgoing HTTP request.
    This dataclass will be used to pass all necessary parameters to the
    core HTTP client function(s) within the LLM_Interaction_Gateway_Service.
    """
    url: str
    method: str # e.g., "GET", "POST", "PUT", "DELETE"
    headers: Optional[Dict[str, str]] = None
    params: Optional[Dict[str, Any]] = None # URL query parameters
    json_payload: Optional[Any] = None # For POST/PUT requests with JSON body
    data_payload: Optional[Any] = None # For POST/PUT requests with form data or other types
    timeout_seconds: float = 60.0 # Default total timeout for the request
    # Alternatively, for more control (as used by 'requests' library):
    # timeout_seconds_tuple: Optional[Tuple[float, float]] = None # (connect_timeout, read_timeout)
    allow_redirects: bool = True
    verify_ssl: bool = True # Per project guidelines, prioritize success; can be overridden for non-prod if absolutely necessary.

@dataclass
class HttpResponseData:
    """
    Represents the structured data returned from an HTTP request execution.
    This helps in standardizing how HTTP responses are handled internally
    after a call is made.
    """
    status_code: int
    headers: Dict[str, str]
    url: str # The final URL after any redirects
    elapsed_time_seconds: float
    json_body: Optional[Any] = None # Parsed JSON content if response is JSON
    text_body: Optional[str] = None # Raw text content of the response
    byte_content: Optional[bytes] = None # Raw byte content for non-text responses

    # Method to attempt parsing JSON, to be called by the HTTP client logic
    # def _parse_json_if_applicable(self, response_content: bytes, response_headers: Dict[str, str]):
    #     content_type = response_headers.get("content-type", "").lower()
    #     if "application/json" in content_type:
    #         try:
    #             self.json_body = json.loads(response_content.decode('utf-8')) # Assuming UTF-8
    #         except json.JSONDecodeError:
    #             # Log error, json_body remains None
    #             self.text_body = response_content.decode('utf-8', errors='replace') # Store as text if JSON fails
    #     else:
    #         self.text_body = response_content.decode('utf-8', errors='replace')


@dataclass
class HttpRetryConfig:
    """
    Configuration for retry mechanisms when making HTTP calls.
    This will be used by the LLM_Interaction_Gateway_Service to manage
    retries for transient errors.
    """
    max_retries: int = 3
    backoff_factor: float = 0.5  # Sleep for {backoff_factor} * (2 ** ({number_of_previous_attempts} - 1))
    status_codes_to_retry_on: List[int] = field(default_factory=lambda: [429, 500, 502, 503, 504])
    # List of HTTP status codes that should trigger a retry.
    # 429: Too Many Requests
    # 500: Internal Server Error
    # 502: Bad Gateway
    # 503: Service Unavailable
    # 504: Gateway Timeout
    retry_on_connection_error: bool = True
    retry_on_timeout: bool = True

# Note on LLMTargetConfig:
# While the conceptual `execute_llm_call(prompt_data: dict, target_llm_config: dict)`
# was mentioned in the context, the `target_llm_config` (containing API keys,
# specific base URLs for an LLM provider) would typically be used by the
# LLM_Interaction_Gateway_Service *before* constructing the `HttpRequestConfig`.
# The `HttpRequestConfig` is what the *core HTTP call logic* (ST018) would directly consume.
# For example, `target_llm_config.api_key` would be formatted into `HttpRequestConfig.headers`.
# `target_llm_config.base_url` + endpoint_path would become `HttpRequestConfig.url`.
# Therefore, `LLMTargetConfig` itself is not a direct data model for the *functions*
# being implemented in ST018, but rather for the service class that orchestrates them.
# The design of such a higher-level configuration dataclass would be part of the
# detailed design for the overall LLM_Interaction_Gateway_Service class structure,
# likely in task ST019 or the class/function logic design (Agent 3.4) for ST018/ST019.
# For ST018, the above three dataclasses are the most directly relevant for the
# functions making and handling HTTP calls.
```

21. **3.4 类与函数逻辑设计Agent (Class & Function Logic Design Agent):** 

以下是我为子任务ST018生成的类与函数逻辑设计文档：

```python
# Class and Function Logic Design Document
# Document ID: ClassFunctionLogicDesign_ST018_v1.0
# Version: 1.0
# Timestamp: 2023-10-30T11:00:00Z # Illustrative
# Generated By: 3.4 Class & Function Logic Design Agent
# For Subtask: ST018 - Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.
# Based On: Context_ST018_v1.0, APIDetailedDesign_ST018_v1.0, DataModelDetailedDesign_ST018_v1.0

import time
import logging
import json
from typing import Any, Callable, Dict, Optional, Tuple
# Assume 'requests' library will be used for actual HTTP calls.
# from requests import Request, Response, Session
# from requests.exceptions import RequestException, Timeout, ConnectionError, HTTPError

# Data models from DataModelDetailedDesign_ST018_v1.0 are assumed to be available:
# from .data_models import HttpRequestConfig, HttpResponseData, HttpRetryConfig
# For design purposes, their structure is referenced.

# --- Custom Exceptions ---
# These exceptions will be raised by the HTTP client utility to provide
# more specific error information to the calling service (LLMInteractionGatewayService).

classHttpClientBaseException(Exception):
    """Base exception for HTTP client errors."""
    def __init__(self, message: str, status_code: Optional[int] = None, url: Optional[str] = None):
        super().__init__(message)
        self.status_code = status_code
        self.url = url
        self.message = message

    def __str__(self) -> str:
        return f"{self.__class__.__name__}: {self.message} (URL: {self.url}, Status Code: {self.status_code})"

classHttpRequestNonSuccessException(HttpClientBaseException):
    """Raised when an HTTP request returns a non-2xx status code that is not specifically handled."""
    pass

classHttpConnectionErrorException(HttpClientBaseException):
    """Raised for network connectivity issues (e.g., DNS failure, refused connection)."""
    def __init__(self, message: str, url: Optional[str] = None):
        super().__init__(message, url=url)

classHttpRequestTimeoutException(HttpClientBaseException):
    """Raised when a request times out."""
    def __init__(self, message: str, url: Optional[str] = None):
        super().__init__(message, url=url)

classHttpTooManyRedirectsException(HttpClientBaseException):
    """Raised when a request exceeds the maximum number of redirects."""
    def __init__(self, message: str, url: Optional[str] = None):
        super().__init__(message, url=url)

# LLM-specific exceptions as outlined in APIDetailedDesign_ST018_v1.0,
# which could be raised by this utility if status codes are directly mapped here.
classLLMAuthenticationError(HttpRequestNonSuccessException):
    """Raised for 401 Unauthorized or 403 Forbidden status codes."""
    pass

classLLMRateLimitError(HttpRequestNonSuccessException):
    """Raised for 429 Too Many Requests status code."""
    pass

classLLMBadRequestError(HttpRequestNonSuccessException):
    """Raised for 400 Bad Request or 422 Unprocessable Entity status codes."""
    pass

classLLMNotFoundError(HttpRequestNonSuccessException):
    """Raised for 404 Not Found status code."""
    pass

classLLMServiceUnavailableError(HttpRequestNonSuccessException):
    """Raised for 500, 502, 503, 504 status codes after retries are exhausted."""
    pass


# --- Core HTTP Client Utility Functions ---
# These functions are intended to be part of a utility module, e.g., `http_client_utils.py`.

# Initialize logger (as per coding standards, logging should be set up)
logger = logging.getLogger(__name__)
# Assume logger is configured elsewhere in the application.
# For pseudocode, explicit logging calls will be shown.

# --- Function: _send_single_http_request ---
# Responsibility:
#   Sends a single HTTP request using the 'requests' library.
#   Handles basic request construction, timeout, SSL verification, and redirection.
#   Parses the response into an HttpResponseData object or raises low-level exceptions.
# Interface:
#   Name: _send_single_http_request
#   Parameters:
#     config: HttpRequestConfig - Configuration for the HTTP request.
#     session: Optional[requests.Session] - An optional requests.Session object to use for the call.
#   Return Type:
#     HttpResponseData - Contains the structured response data.
#   Raises:
#     HttpConnectionErrorException - For connection-related issues.
#     HttpRequestTimeoutException - If the request times out.
#     HttpTooManyRedirectsException - If too many redirects occur.
#     HttpClientBaseException - For other unhandled 'requests' library exceptions.
# Pseudocode/Logic:
"""
FUNCTION _send_single_http_request(config: HttpRequestConfig, session: Optional[SessionObject]) -> HttpResponseData:
    logger.info(f"Preparing to send HTTP {config.method} request to {config.url}")

    request_kwargs = {
        "method": config.method.upper(),
        "url": config.url,
        "headers": config.headers,
        "params": config.params,
        "json": config.json_payload,
        "data": config.data_payload,
        "timeout": config.timeout_seconds,
        "allow_redirects": config.allow_redirects,
        "verify": config.verify_ssl
    }

    IF session IS None:
        http_callable = requests.request // Actual 'requests.request'
    ELSE:
        http_callable = session.request // Actual 'session.request'

    start_time = time.monotonic()
    raw_response = None

    TRY:
        raw_response = http_callable(**request_kwargs)
        elapsed_time = time.monotonic() - start_time
        logger.info(f"Received response for {config.method} {config.url} - Status: {raw_response.status_code} in {elapsed_time:.4f}s")

        response_data = HttpResponseData(
            status_code=raw_response.status_code,
            headers=dict(raw_response.headers),
            url=raw_response.url,
            elapsed_time_seconds=elapsed_time,
            byte_content=raw_response.content
        )

        // Attempt to parse JSON body
        content_type = raw_response.headers.get("content-type", "").lower()
        IF "application/json" IN content_type AND raw_response.content:
            TRY:
                response_data.json_body = raw_response.json() // requests library's json() method
            EXCEPT json.JSONDecodeError as e:
                logger.warning(f"Failed to decode JSON response from {config.url}: {e}")
                response_data.text_body = raw_response.text // Store as text if JSON fails
            EXCEPT Exception as e: // Catch other potential errors from .json()
                logger.warning(f"Error processing JSON response from {config.url}: {e}")
                response_data.text_body = raw_response.text
        ELSE IF raw_response.content:
            response_data.text_body = raw_response.text

        RETURN response_data

    EXCEPT requests.exceptions.Timeout as e:
        elapsed_time = time.monotonic() - start_time
        logger.error(f"Request to {config.url} timed out after {elapsed_time:.4f}s: {e}")
        RAISE HttpRequestTimeoutException(message=f"Request timed out: {e}", url=config.url)

    EXCEPT requests.exceptions.ConnectionError as e:
        elapsed_time = time.monotonic() - start_time
        logger.error(f"Connection error for {config.url} after {elapsed_time:.4f}s: {e}")
        RAISE HttpConnectionErrorException(message=f"Connection error: {e}", url=config.url)

    EXCEPT requests.exceptions.TooManyRedirects as e:
        elapsed_time = time.monotonic() - start_time
        logger.error(f"Too many redirects for {config.url} after {elapsed_time:.4f}s: {e}")
        RAISE HttpTooManyRedirectsException(message=f"Too many redirects: {e}", url=config.url)

    EXCEPT requests.exceptions.RequestException as e: // Catch-all for other requests exceptions
        elapsed_time = time.monotonic() - start_time
        logger.error(f"HTTP request failed for {config.url} after {elapsed_time:.4f}s: {e}")
        status_code_from_exception = e.response.status_code IF e.response IS NOT None ELSE None
        RAISE HttpClientBaseException(message=f"HTTP request failed: {e}", status_code=status_code_from_exception, url=config.url)

    FINALLY:
        IF raw_response IS NOT None:
            raw_response.close() // Ensure connection is closed
END FUNCTION
"""

# --- Function: _map_status_to_llm_exception ---
# Responsibility:
#   Maps an HTTP status code from HttpResponseData to a specific LLM-related exception.
# Interface:
#   Name: _map_status_to_llm_exception
#   Parameters:
#     response_data: HttpResponseData - The response data containing the status code.
#   Return Type:
#     Optional[Callable] - The exception class to be raised, or None if no specific mapping.
# Pseudocode/Logic:
"""
FUNCTION _map_status_to_llm_exception(response_data: HttpResponseData) -> Optional[Type[HttpRequestNonSuccessException]]:
    status_code = response_data.status_code
    error_message_detail = ""
    IF response_data.json_body AND isinstance(response_data.json_body, dict):
        error_detail = response_data.json_body.get("error")
        IF isinstance(error_detail, dict):
            error_message_detail = error_detail.get("message", "")
        ELSE: # Sometime error is not nested
            error_message_detail = response_data.json_body.get("message", "")
    ELSE IF response_data.text_body:
        error_message_detail = response_data.text_body[:200] # Truncate long messages

    message = f"HTTP Error {status_code}. Detail: {error_message_detail}"

    IF status_code == 400 OR status_code == 422:
        RETURN LLMBadRequestError(message=message, status_code=status_code, url=response_data.url)
    ELSE IF status_code == 401:
        RETURN LLMAuthenticationError(message="Authentication failed (401). Check API key.", status_code=status_code, url=response_data.url)
    ELSE IF status_code == 403:
        RETURN LLMAuthenticationError(message="Forbidden (403). API key may lack permissions.", status_code=status_code, url=response_data.url)
    ELSE IF status_code == 404:
        RETURN LLMNotFoundError(message="Resource not found (404).", status_code=status_code, url=response_data.url)
    ELSE IF status_code == 429:
        RETURN LLMRateLimitError(message="Rate limit exceeded (429).", status_code=status_code, url=response_data.url)
    ELSE IF status_code >= 500: // 500, 502, 503, 504 etc.
        RETURN LLMServiceUnavailableError(message=message, status_code=status_code, url=response_data.url)
    ELSE IF 400 <= status_code < 500: // Other 4xx errors
        RETURN HttpRequestNonSuccessException(message=message, status_code=status_code, url=response_data.url)
    ELSE: // Should not happen if called for non-2xx, but as a fallback
        RETURN None
END FUNCTION
"""


# --- Function: make_http_request_with_retry ---
# Responsibility:
#   Manages the execution of an HTTP request with a retry mechanism for transient failures.
#   Uses _send_single_http_request for individual attempts.
#   Interprets response status codes to raise appropriate LLM-specific or general HTTP exceptions.
# Interface:
#   Name: make_http_request_with_retry
#   Parameters:
#     request_config: HttpRequestConfig - Configuration for the HTTP request.
#     retry_config: HttpRetryConfig - Configuration for the retry behavior.
#     session: Optional[requests.Session] - An optional requests.Session object.
#   Return Type:
#     HttpResponseData - Contains the structured response data if successful.
#   Raises:
#     HttpConnectionErrorException, HttpRequestTimeoutException, HttpTooManyRedirectsException
#     LLMAuthenticationError, LLMRateLimitError, LLMBadRequestError, LLMNotFoundError, LLMServiceUnavailableError
#     HttpRequestNonSuccessException (for unmapped non-2xx errors)
#     HttpClientBaseException (for other errors)
# Pseudocode/Logic:
"""
FUNCTION make_http_request_with_retry(
    request_config: HttpRequestConfig,
    retry_config: HttpRetryConfig,
    session: Optional[SessionObject] = None
) -> HttpResponseData:

    last_exception = None
    FOR attempt FROM 0 TO retry_config.max_retries:
        logger.info(f"Attempt {attempt + 1} of {retry_config.max_retries + 1} for {request_config.method} {request_config.url}")
        TRY:
            response = _send_single_http_request(config=request_config, session=session)

            IF 200 <= response.status_code < 300: // Success
                RETURN response

            // Handle non-success status codes
            specific_llm_exception = _map_status_to_llm_exception(response)

            IF specific_llm_exception IS NOT None:
                // Check if this status code is in the retry list
                IF response.status_code IN retry_config.status_codes_to_retry_on AND attempt < retry_config.max_retries:
                    last_exception = specific_llm_exception
                    logger.warning(f"Attempt {attempt + 1} failed with retryable status {response.status_code}. Retrying...")
                    // Proceed to sleep and retry
                ELSE: // Not retryable or max retries reached for this status
                    logger.error(f"Request failed with status {response.status_code}. Raising {specific_llm_exception.__class__.__name__}.")
                    RAISE specific_llm_exception
            ELSE: // Should not happen if _map_status_to_llm_exception is comprehensive for non-2xx
                  // But if it does, or for other non-2xx not mapped to LLM specific ones
                IF 400 <= response.status_code < 600: // Generic non-2xx
                    generic_exception = HttpRequestNonSuccessException(
                        message=f"HTTP request failed with status code {response.status_code}. Body: {response.text_body[:200] if response.text_body else ''}",
                        status_code=response.status_code,
                        url=request_config.url
                    )
                    IF response.status_code IN retry_config.status_codes_to_retry_on AND attempt < retry_config.max_retries:
                        last_exception = generic_exception
                        logger.warning(f"Attempt {attempt + 1} failed with retryable status {response.status_code}. Retrying...")
                    ELSE:
                        RAISE generic_exception
                ELSE: // Should ideally be a 2xx or a mapped error
                    logger.error(f"Unexpected status code {response.status_code} encountered.")
                    // This case should be rare if logic is correct. Fallback to generic non-success.
                    RAISE HttpRequestNonSuccessException(
                        message=f"Unexpected HTTP status: {response.status_code}",
                        status_code=response.status_code,
                        url=request_config.url
                    )


        EXCEPT (HttpConnectionErrorException, HttpRequestTimeoutException) as e:
            last_exception = e
            is_conn_error_retryable = isinstance(e, HttpConnectionErrorException) AND retry_config.retry_on_connection_error
            is_timeout_retryable = isinstance(e, HttpRequestTimeoutException) AND retry_config.retry_on_timeout

            IF (is_conn_error_retryable OR is_timeout_retryable) AND attempt < retry_config.max_retries:
                logger.warning(f"Attempt {attempt + 1} failed with {e.__class__.__name__}. Retrying...")
                // Proceed to sleep and retry
            ELSE: // Not retryable or max retries reached
                logger.error(f"Request failed with non-retryable or maxed-out {e.__class__.__name__}. Raising.")
                RAISE e // Re-raise the caught specific exception

        EXCEPT HttpClientBaseException as e: // Catch other HttpClientBaseException from _send_single_http_request
            last_exception = e
            logger.error(f"Attempt {attempt + 1} failed with unhandled HttpClientBaseException: {e}. Not retrying this type.")
            RAISE e // Re-raise immediately, not typically retryable unless specifically configured

        // If we are going to retry, calculate sleep time
        IF attempt < retry_config.max_retries:
            sleep_duration = retry_config.backoff_factor * (2 ** attempt)
            logger.info(f"Sleeping for {sleep_duration:.2f} seconds before next retry.")
            time.sleep(sleep_duration)

    // If loop finishes, all retries were exhausted
    logger.error(f"All {retry_config.max_retries + 1} attempts failed for {request_config.url}. Last error: {last_exception}")
    IF last_exception IS NOT None:
        RAISE last_exception
    ELSE:
        // Should not happen if logic is correct, but as a fallback
        RAISE HttpClientBaseException(message="All retries failed without a specific last exception.", url=request_config.url)

END FUNCTION
"""

# --- Considerations for LLMInteractionGatewayService class ---
# The functions designed above (`_send_single_http_request`, `make_http_request_with_retry`, `_map_status_to_llm_exception`)
# are low-level HTTP utilities.
# The `LLMInteractionGatewayService` class itself (likely designed as part of ST019 or refined for ST018's integration)
# would use `make_http_request_with_retry`.
#
# Example structure for the service class method (conceptual):
#
# CLASS LLMInteractionGatewayService:
#     // ... constructor, config loading ...
#
#     FUNCTION execute_llm_call(self, provider_config: LLMProviderConfig, request_details: LLMRequestDetails) -> LLMResponse:
#         // 1. Construct HttpRequestConfig from provider_config and request_details
#         //    (e.g., build URL, headers with API key, JSON payload)
#         http_req_config = self._build_http_request_config(provider_config, request_details)
#
#         // 2. Define HttpRetryConfig (could be part of provider_config or service default)
#         retry_cfg = self._get_retry_config(provider_config)
#
#         // 3. Call the utility function
#         http_response_data = make_http_request_with_retry(
#             request_config=http_req_config,
#             retry_config=retry_cfg,
#             session=self.get_session_for_provider(provider_config) // Potentially use sessions
#         )
#
#         // 4. Parse http_response_data.json_body into a provider-specific LLMResponse object
#         llm_response = self._parse_llm_response(provider_config, http_response_data)
#
#         RETURN llm_response
#
# END CLASS
#
# The design of `LLMProviderConfig`, `LLMRequestDetails`, `LLMResponse` and the helper methods
# `_build_http_request_config`, `_get_retry_config`, `_parse_llm_response` would be part of the
# service-level design, not strictly ST018's "core HTTP call logic". ST018 provides the robust
# `make_http_request_with_retry` foundation.
```

22. **3.5 文件与目录结构规划Agent (File & Directory Structure Planner Agent):**

My goal is to define the necessary new files and directories for this subtask, ensuring the structure aligns with Python best practices (while adhering to your specific coding standards, especially regarding modularity and the absence of comments) and the overall architecture.

**Input Analysis:**

1.  **Current Subtask Context Package (`Context_ST018_v1.0`):** This confirms the task is foundational for the `LLM_Interaction_Gateway_Service` and involves implementing the actual HTTP communication logic, likely using Python's `requests` library.
2.  **API Detailed Design (`APIDetailedDesign_ST018_v1.0`):** This describes the *external* LLM APIs the gateway will call, not an API exposed by the gateway itself for ST018. It guides the capabilities needed in our HTTP client logic.
3.  **Data Model Detailed Design (`DataModelDetailedDesign_ST018_v1.0`):** This provided crucial Python `@dataclass` definitions: `HttpRequestConfig`, `HttpResponseData`, and `HttpRetryConfig`. These will need a home.
4.  **Class & Function Logic Design (`ClassFunctionLogicDesign_ST018_v1.0`):** This is the most direct input, outlining:
    *   Custom exceptions (`HttpClientBaseException`, `HttpRequestNonSuccessException`, etc.).
    *   Core utility functions (`_send_single_http_request`, `_map_status_to_llm_exception`, `make_http_request_with_retry`).
    *   The note that these are "intended to be part of a utility module, e.g., `http_client_utils.py`".
    *   A conceptual `LLMInteractionGatewayService` class which would *use* these utilities (implementation of this class is likely ST019).
5.  **PKBM (Project Knowledge Base Manager):** For this run, I assume no prior project file structure exists for the `LLM_Interaction_Gateway_Service` component. I will be proposing a new structure for it.
6.  **Coding Standards:** Emphasis on modularity, Python data classes for configuration, clear error handling, and specific naming conventions. No comments/docstrings.

**File and Directory Structure Plan for ST018:**

Based on the inputs, the primary outputs of ST018 are Python modules containing data models, custom exceptions, and the HTTP client utility functions. These will form part of the `LLM_Interaction_Gateway_Service` component.

Assuming a project root directory named `multi_agent_dev_system` (or similar, this can be adjusted) and an application-specific directory, say `src` or `app`, I propose the following structure. For simplicity and following common Python project layouts, I'll use `app` as the main application code directory.

**Proposed Directory Structure (New Additions Highlighted):**

```
multi_agent_dev_system/
├── app/
│   ├── __init__.py
│   ├── services/
│   │   ├── __init__.py
│   │   └── llm_gateway_service/      <-- NEW DIRECTORY for ST018 and related
│   │       ├── __init__.py           <-- NEW FILE
│   │       ├── data_models.py        <-- NEW FILE (for HttpRequestConfig, etc.)
│   │       ├── exceptions.py         <-- NEW FILE (for custom HTTP exceptions)
│   │       └── http_client.py        <-- NEW FILE (for core HTTP call logic)
│   ├── agents/
│   │   └── ... (other agent implementations)
│   ├── web_ui_service/
│   │   └── ... (Flask app specifics)
│   └── workflow_management_service/
│       └── ...
├── pkbm_root/
│   └── ... (knowledge base artifacts)
└── ... (other project files like main.py, tests/, etc.)
```

**Detailed Plan:**

1.  **New Directories to Create:**
    *   `app/services/llm_gateway_service/`
        *   **Rationale:** This directory will house all modules specifically related to the `LLM_Interaction_Gateway_Service` component, promoting modularity as per the coding standards and the HLDD.

2.  **New Files to Create for Subtask ST018:**
    *   `app/services/llm_gateway_service/__init__.py`
        *   **Path:** `app/services/llm_gateway_service/__init__.py`
        *   **Content:** Can be initially empty.
        *   **Rationale:** Makes the `llm_gateway_service` directory a Python package, allowing modules within it to be imported using dot notation (e.g., `from app.services.llm_gateway_service.http_client import make_http_request_with_retry`). This will also be where the `LLMInteractionGatewayService` class (from ST019) might be exposed for easier import.
    *   `app/services/llm_gateway_service/data_models.py`
        *   **Path:** `app/services/llm_gateway_service/data_models.py`
        *   **Content:** Will contain the Python `@dataclass` definitions: `HttpRequestConfig`, `HttpResponseData`, and `HttpRetryConfig` as specified in `DataModelDetailedDesign_ST018_v1.0`.
        *   **Rationale:** Separates data structure definitions from logic, improving organization and adhering to the principle of single responsibility for modules.
    *   `app/services/llm_gateway_service/exceptions.py`
        *   **Path:** `app/services/llm_gateway_service/exceptions.py`
        *   **Content:** Will contain the custom exception classes defined in `ClassFunctionLogicDesign_ST018_v1.0` (e.g., `HttpClientBaseException`, `HttpRequestNonSuccessException`, `LLMAuthenticationError`, etc.).
        *   **Rationale:** Centralizes custom exceptions related to HTTP client operations, making them easy to import and manage.
    *   `app/services/llm_gateway_service/http_client.py`
        *   **Path:** `app/services/llm_gateway_service/http_client.py`
        *   **Content:** Will contain the core HTTP client functions: `_send_single_http_request`, `_map_status_to_llm_exception`, and `make_http_request_with_retry` as detailed in `ClassFunctionLogicDesign_ST018_v1.0`. This module will import from `data_models.py` and `exceptions.py` within the same package.
        *   **Rationale:** This module encapsulates the fundamental HTTP communication logic, as per ST018's description.

3.  **Existing Files to Modify:**
    *   For subtask ST018 itself, no existing *code* files are being modified. We are creating new modules that will be *used by* other parts of the `LLM_Interaction_Gateway_Service` (likely implemented in ST019, e.g., in a `service.py` file within the `llm_gateway_service` package).

4.  **Python `__init__.py` Files:**
    *   `app/__init__.py` (if `app` is the top-level source dir)
    *   `app/services/__init__.py`
        *   **Rationale:** These ensure that `app` and `app.services` are treated as Python packages, allowing for structured imports. They can be empty.

This plan establishes a clean, modular structure for the `LLM_Interaction_Gateway_Service`'s core HTTP utilities, aligning with the provided design documents and general Python project organization principles.

I will now save this plan to the PKBM.

**Output to PKBM:**

*   **Filename:** `FileStructurePlan_ST018_v1.0.json`
*   **Content:**

```json
{
  "document_id": "FileStructurePlan_ST018_v1.0",
  "version": "1.0",
  "timestamp": "2023-10-30T11:15:00Z",
  "generated_by": "3.5 File & Directory Structure Planner Agent",
  "subtask_id": "ST018",
  "subtask_description": "Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.",
  "assumptions": [
    "Project root directory is 'multi_agent_dev_system'.",
    "Main application source code resides in an 'app/' subdirectory.",
    "No conflicting file structure for 'LLM_Interaction_Gateway_Service' currently exists."
  ],
  "new_directories": [
    {
      "path": "app/services/llm_gateway_service/",
      "rationale": "Dedicated directory for all modules of the LLM_Interaction_Gateway_Service component, promoting modularity."
    }
  ],
  "new_files": [
    {
      "path": "app/services/llm_gateway_service/__init__.py",
      "description": "Makes 'llm_gateway_service' a Python package. Can be initially empty.",
      "rationale": "Enables structured imports from and within the llm_gateway_service package."
    },
    {
      "path": "app/services/llm_gateway_service/data_models.py",
      "description": "Contains @dataclass definitions: HttpRequestConfig, HttpResponseData, HttpRetryConfig.",
      "source_design_doc": "DataModelDetailedDesign_ST018_v1.0",
      "rationale": "Separates data structure definitions from operational logic."
    },
    {
      "path": "app/services/llm_gateway_service/exceptions.py",
      "description": "Contains custom HTTP client exception classes (HttpClientBaseException, LLMAuthenticationError, etc.).",
      "source_design_doc": "ClassFunctionLogicDesign_ST018_v1.0",
      "rationale": "Centralizes custom exceptions for HTTP client operations."
    },
    {
      "path": "app/services/llm_gateway_service/http_client.py",
      "description": "Contains core HTTP client functions (_send_single_http_request, make_http_request_with_retry, etc.). Will import from local data_models.py and exceptions.py.",
      "source_design_doc": "ClassFunctionLogicDesign_ST018_v1.0",
      "rationale": "Encapsulates the fundamental HTTP communication logic for ST018."
    }
  ],
  "modified_files": [],
  "notes": "This plan focuses on ST018. The LLMInteractionGatewayService class itself, which will use these utilities, is expected to be implemented in a subsequent subtask (e.g., ST019) likely within a 'service.py' file in the 'app/services/llm_gateway_service/' package."
}
```

23. **3.6 安全考量与检查点定义Agent (Security Considerations & Checkpoint Definer Agent):**

## Security Considerations Report

**Document ID:** `SecurityConsiderations_ST018_v1.0`
**Version:** 1.0
**Timestamp:** `2023-10-30T11:30:00Z` (Illustrative)
**Generated By:** 3.6 Security Considerations & Checkpoint Definer Agent
**For Subtask:** ST018 - Implement the LLM_Interaction_Gateway_Service core logic for making HTTP calls.

### 1. Introduction

This document outlines the security considerations for subtask ST018, which involves implementing the core HTTP client logic for the `LLM_Interaction_Gateway_Service`.

**Overriding Project Directive:** As per explicit user requirements, this project is designed for operation within an absolutely secure environment. Consequently, security measures are to be **completely deprioritized**. The absolute priorities are functionality, success rate, ease of configuration, and simplicity of use. All permissions should be as open as possible to prevent failures due to security settings. Standard security practices like OWASP Top 10 considerations, strict input validation for security purposes, complex authentication/authorization schemes, and rigorous secret management are **not applicable or will be intentionally simplified** for this subtask and the project as a whole, unless they directly support the primary goals of functionality and simplicity without causing obstruction.

### 2. Scope of Analysis

This analysis focuses on the design artifacts for ST018:
*   The Python functions responsible for making outbound HTTP requests (`_send_single_http_request`, `make_http_request_with_retry`).
*   The data models used for configuring these requests (`HttpRequestConfig`, `HttpRetryConfig`) and representing responses (`HttpResponseData`).
*   The custom exceptions related to HTTP operations.

### 3. General Security Stance for ST018

Consistent with the project-wide directive:
*   **No new security complexities:** The implementation should avoid introducing any security features that could complicate configuration, deployment, or usage.
*   **Open by default:** Where choices exist (e.g., SSL verification), flexibility to ensure connection success is paramount, even if it means less stringent settings for specific non-production environments.
*   **Focus on robustness for functionality:** Error handling and retry mechanisms are important for success rate, which indirectly contributes to a stable (though not "secure" in the traditional sense) service.

### 4. Specific Considerations and (Non-)Recommendations

#### 4.1. API Key Handling for External LLMs
*   **Context:** The gateway will call external LLM APIs that require API keys.
*   **Standard Security Concern (Dismissed):** Secure storage, rotation, and transmission of API keys.
*   **Project Approach (ST018 & LLM Gateway Service):**
    *   API keys will be passed to the HTTP client functions as part of the request configuration (e.g., within `HttpRequestConfig.headers`).
    *   The `LLM_Interaction_Gateway_Service` (likely ST019) will be responsible for obtaining these keys from its own configuration (e.g., Python dataclass, direct environment variables if simplest for user).
    *   **No encryption or obfuscation of keys within the ST018 logic is required or desired.** Simplicity of passing the key in the header is key.
    *   **Checkpoint:** Ensure the HTTP client logic correctly incorporates headers provided in `HttpRequestConfig.headers` without modification or added security layers.

#### 4.2. SSL/TLS Certificate Verification
*   **Context:** Outbound HTTPS calls to external LLMs.
*   **Standard Security Concern (Partially Relaxed):** Always verify SSL/TLS certificates to prevent Man-in-the-Middle (MitM) attacks.
*   **Project Approach:**
    *   The `HttpRequestConfig` data model includes a `verify_ssl: bool = True` field. This default is good practice.
    *   **Crucially, this MUST remain configurable.** If a user encounters issues connecting to a specific (e.g., local, non-production, or misconfigured) LLM endpoint due to SSL verification, they MUST be able to set `verify_ssl = False` for that specific request configuration to ensure functionality. This aligns with "preventing failures due to security settings."
    *   **Checkpoint:** The `_send_single_http_request` function must honor the `verify_ssl` flag from `HttpRequestConfig` when using the `requests` library.

#### 4.3. Input Validation (of parameters to the HTTP client functions)
*   **Context:** The HTTP client functions receive URLs, headers, payloads.
*   **Standard Security Concern (Dismissed for Security):** Validating inputs to prevent injection attacks (e.g., CRLF injection in headers, SSRF if URLs were constructed from less trusted input – though here URLs are for specific LLMs).
*   **Project Approach:**
    *   The primary consumer of these HTTP client functions is the `LLM_Interaction_Gateway_Service` itself, which is assumed to construct valid `HttpRequestConfig` objects.
    *   No specific security-focused input sanitization is required within ST018 beyond what the `requests` library itself might do for basic well-formedness.
    *   The design assumes that upstream components generating data for LLM prompts are responsible for their content.
    *   **Checkpoint:** Focus on ensuring the `requests` library calls are made correctly with the provided data. Avoid adding custom validation logic that could break valid, albeit unusual, inputs needed for certain LLM APIs.

#### 4.4. Output Handling (from external LLMs)
*   **Context:** The HTTP client functions retrieve responses from LLMs.
*   **Standard Security Concern (Dismissed):** Sanitizing output from LLMs if it's rendered directly to users (to prevent XSS, etc.).
*   **Project Approach:**
    *   The output from these HTTP utilities is `HttpResponseData`, which is then processed by the `LLM_Interaction_Gateway_Service` and subsequently by other internal agents. It's not directly rendered to a web user by ST018's logic.
    *   No output sanitization is required in ST018.
    *   **Checkpoint:** Ensure the full, raw response (or parsed JSON) is available in `HttpResponseData` for upstream processing.

#### 4.5. Logging
*   **Context:** Logging HTTP requests and responses for debugging.
*   **Standard Security Concern (Dismissed):** Avoiding logging of sensitive data like API keys or full request/response payloads if they contain PII or secrets.
*   **Project Approach:**
    *   Comprehensive logging is crucial for debugging and ensuring "success rate."
    *   Given the "absolutely secure environment" assumption, logging of headers (which might include API keys if not careful, though `requests` usually redacts `Authorization`) and potentially snippets of payloads for debugging purposes is acceptable if it aids in problem resolution. The `ClassFunctionLogicDesign_ST018_v1.0` already includes logging.
    *   **Checkpoint:** Ensure logging is informative for debugging. If `requests` library automatically redacts `Authorization` headers in its own logging (if used), that's fine. The custom logging within ST018 functions should prioritize debugging information. For this project, explicitly logging the URL and status is good; logging full request/response bodies should be done judiciously if it aids debugging, without concern for data sensitivity *within this assumed secure environment*.

#### 4.6. Retry Mechanisms and Resource Usage
*   **Context:** The `make_http_request_with_retry` function implements retries.
*   **Standard Security Concern (Minimal relevance):** Overly aggressive retries could contribute to DoS against an external service or exhaust local resources.
*   **Project Approach:**
    *   The `HttpRetryConfig` (max_retries, backoff_factor) provides basic control. This is sufficient.
    *   No complex circuit breaker patterns are needed.
    *   **Checkpoint:** The retry logic as designed in `ClassFunctionLogicDesign_ST018_v1.0` using `HttpRetryConfig` is adequate.

#### 4.7. Error Handling
*   **Context:** Handling various HTTP errors.
*   **Project Approach (Good for Robustness):** The `ClassFunctionLogicDesign_ST018_v1.0` defines a good set of custom exceptions. This is excellent for functionality and debugging, as it allows the calling service to understand failures clearly.
*   **Checkpoint:** No additional security-specific error handling is needed. The current design for propagating errors is suitable.

### 5. Security Checkpoints / Implementation Requirements for ST018

Based on the above and the project's unique security stance, the "security" requirements are minimal and focused on not obstructing functionality:

1.  **Configurable SSL Verification:** The `_send_single_http_request` function **must** use the `verify_ssl` boolean from `HttpRequestConfig` to control SSL certificate verification in the underlying `requests` call. It should default to `True` in the `HttpRequestConfig` dataclass but be overridable.
2.  **Direct Header Passthrough:** The HTTP client **must** pass through headers provided in `HttpRequestConfig.headers` to the `requests` library without adding its own security-related modifications (e.g., stripping "unsafe" headers, unless `requests` itself does so). This ensures API keys and other necessary headers are sent as intended by the `LLM_Interaction_Gateway_Service`.
3.  **No Internal Secret Management:** The ST018 logic **must not** implement any form of secret storage, encryption, or complex management for API keys. It consumes them as configured in `HttpRequestConfig`.
4.  **Prioritize Connection Success:** When implementing `_send_single_http_request` and `make_http_request_with_retry`, if any ambiguity arises in library usage (e.g., `requests`), choices should favor successful connection and data retrieval over more stringent security defaults, provided this is controllable (like `verify_ssl`).

### 6. Conclusion

For subtask ST018, the security posture is one of intentional minimalism to prioritize functionality, success rate, and ease of use in an assumed absolutely secure environment. The existing design documents (`DataModelDetailedDesign_ST018_v1.0` and `ClassFunctionLogicDesign_ST018_v1.0`) largely align with this by providing robust, configurable HTTP client utilities. The key "security" checkpoints are effectively non-security measures: ensuring configurability to bypass potential SSL issues and ensuring direct passthrough of necessary credentials as configured by the calling service.

24. **3.7 详细设计汇总与指令生成Agent (Detailed Design Aggregation & Instruction Generator Agent):**


**第一步：整合详细设计并生成《代码实现指导书_ST018_v1.0》**

**《代码实现指导书_ST018_v1.0》**

**1. 概述 (Overview)**
   本文档旨在指导 ST018 子任务的实现，即创建项目知识库管理 (PKBM) 的核心模块。该模块负责以结构化的方式存储、检索、更新和删除项目相关的各种知识条目。考虑到项目对简便性和成功率的绝对优先，本模块将采用内存中的字典作为基础存储，不涉及复杂的数据库或文件持久化机制（除非后续任务明确要求）。

**2. 文件结构 (File Structure)**
   *   **创建新文件**: `src/pkbm/pkbm_core.py`

**3. 核心类设计 (Core Class Design)**
   *   **类名**: `PKBMCore`
   *   **职责**: 提供对项目知识库中条目的增删改查（CRUD）基本操作。
   *   **内部数据结构**:
      *   `self.knowledge_base`: 一个字典，用于存储所有知识条目。
          *   键 (Key): `entry_id` (str) - 知识条目的唯一标识符。
          *   值 (Value): 一个字典，包含条目的元数据和内容。
              *   `type`: (str) 条目类型 (e.g., "design_doc", "code_snippet", "requirement", "meeting_minutes", "llm_prompt", "implementation_guide").
              *   `version`: (str) 版本号 (e.g., "1.0", "1.1_draft").
              *   `content`: (any) 条目的实际内容 (可以是文本、字典、列表等)。
              *   `metadata`: (dict) 其他元数据 (e.g., `{"timestamp": "YYYY-MM-DDTHH:MM:SS", "source_agent": "AgentID", "tags": ["tag1", "tag2"]}`).
              *   `dependencies`: (list) 依赖的其他 `entry_id`列表。

**4. API 接口设计 (API Design)**

   *   `__init__(self)`:
      *   初始化 `self.knowledge_base = {}`。
   *   `add_entry(self, entry_id: str, entry_type: str, content: any, version: str = "1.0", metadata: dict = None, dependencies: list = None) -> bool`:
      *   添加一个新的知识条目。
      *   如果 `entry_id` 已存在，则操作失败，返回 `False` (为简化，不处理版本冲突，直接覆盖或要求唯一ID)。
      *   成功添加则返回 `True`。
      *   `metadata` 如果为 `None`，则初始化为空字典。
      *   `dependencies` 如果为 `None`，则初始化为空列表。
   *   `get_entry(self, entry_id: str) -> dict | None`:
      *   根据 `entry_id` 检索知识条目。
      *   如果找到，返回包含条目所有信息（type, version, content, metadata, dependencies）的字典。
      *   如果未找到，返回 `None`。
   *   `update_entry(self, entry_id: str, content: any = None, entry_type: str = None, version: str = None, metadata_update: dict = None, dependencies: list = None) -> bool`:
      *   更新现有知识条目。
      *   如果 `entry_id` 不存在，返回 `False`。
      *   只更新提供的参数，未提供的参数保持不变。`metadata_update` 应与现有 `metadata` 合并（新的覆盖旧的）。
      *   成功更新则返回 `True`。
   *   `delete_entry(self, entry_id: str) -> bool`:
      *   根据 `entry_id` 删除知识条目。
      *   如果 `entry_id` 不存在，返回 `False`。
      *   成功删除则返回 `True`。
   *   `list_entries(self, entry_type: str = None, tags: list = None) -> list[dict]`:
      *   列出所有知识条目，可选择按 `entry_type` 或 `metadata` 中的 `tags`进行过滤。
      *   如果 `tags` 提供，则条目的 `metadata['tags']` 必须包含所有指定的 `tags`。
      *   返回一个包含符合条件条目字典的列表。每个字典包含 `entry_id` 及其完整数据。

**5. 核心逻辑与算法 (Core Logic and Algorithms)**
   *   **add_entry**:
      1.  检查 `entry_id` 是否已存在于 `self.knowledge_base`。
      2.  若存在，记录警告并返回 `False`（或根据明确需求选择覆盖或抛出异常）。
      3.  若不存在，构建条目字典，并存入 `self.knowledge_base[entry_id]`。
      4.  返回 `True`。
   *   **get_entry**:
      1.  使用 `self.knowledge_base.get(entry_id)` 尝试获取条目。
      2.  返回获取到的条目或 `None`。
   *   **update_entry**:
      1.  检查 `entry_id` 是否存在于 `self.knowledge_base`。
      2.  若不存在，返回 `False`。
      3.  若存在，逐个检查传入的更新参数 (`content`, `entry_type`, `version`, `metadata_update`, `dependencies`)。
      4.  如果参数不是 `None`，则更新对应字段。对于 `metadata_update`，使用 `dict.update()` 方法合并。
      5.  返回 `True`。
   *   **delete_entry**:
      1.  检查 `entry_id` 是否存在于 `self.knowledge_base`。
      2.  若不存在，返回 `False`。
      3.  若存在，使用 `del self.knowledge_base[entry_id]` 删除条目。
      4.  返回 `True`。
   *   **list_entries**:
      1.  初始化一个空的结果列表 `results`。
      2.  遍历 `self.knowledge_base.items()`。
      3.  对于每个条目，检查是否满足 `entry_type` 过滤条件（如果 `entry_type` 非 `None`）。
      4.  检查是否满足 `tags` 过滤条件（如果 `tags` 非 `None`）：检查条目的 `metadata.get('tags', [])` 是否是提供 `tags` 列表的超集。
      5.  如果所有条件满足，将条目（包含 `entry_id` 作为其中一个键）添加到 `results` 列表。
      6.  返回 `results`。

**6. 数据结构 (Data Structures)**
   *   主要数据结构为 Python 字典和列表，如上文 "核心类设计" 中所述。

**7. 错误处理 (Error Handling)**
   *   主要通过函数返回值（`True`/`False`/`None`）来指示操作成功与否或数据存在与否。
   *   为简化，不主动抛出异常，除非是Python内置操作（如访问不存在的字典键而不使用 `.get()`）导致的。
   *   可以在控制台打印警告信息，例如当尝试添加已存在的 `entry_id` 时。

**8. 编码规范 (Coding Standards)**
   *   遵循 PEP 8 编码规范。
   *   使用类型提示 (Type Hinting)。
   *   编写清晰的 Docstrings (Google Style)。
   *   类名使用 `PascalCase`，函数/方法名和变量名使用 `snake_case`。
   *   代码应简洁易懂。

**9. 安全考量 (Security Considerations)**
   *   根据项目要求，**完全不考虑安全性**。所有权限等完全放开，绝对优先保证代码、功能的成功率和简便性。

**10. 预期输出 (Expected Output)**
    *   一个完整的 Python 文件: `src/pkbm/pkbm_core.py`，包含 `PKBMCore` 类的实现。


**第二步：生成给 Roo code LLM 的精确指令文本 (Prompt)**

```english
You are an expert Python programmer. Your task is to create a new Python module for a Project Knowledge Base Management (PKBM) system.
This project **ABSOLUTELY PRIORITIZES functionality, simplicity, and speed of development over security**. All permissions are assumed to be fully open. Do NOT implement any security checks, authentication, or authorization mechanisms. Focus solely on making the code work as described and easy to configure/use.

**Project Coding Standards to strictly follow:**
1.  **PEP 8:** Adhere to PEP 8 style guidelines.
2.  **Type Hinting:** Use Python type hints for all function/method signatures and important local variables. Import necessary types from the `typing` module (e.g., `Dict`, `List`, `Any`, `Optional`).
3.  **Docstrings:** Write comprehensive docstrings for the class and all public methods using Google Style.
    Example:
    ```python
    def my_method(self, arg1: str, arg2: int) -> bool:
        """Does something interesting.

        Args:
            arg1 (str): The first argument.
            arg2 (int): The second argument.

        Returns:
            bool: True if successful, False otherwise.
        """
        # ... implementation ...
    ```
4.  **Naming Conventions:**
    *   Class names: `PascalCase` (e.g., `PKBMCore`).
    *   Function/method names: `snake_case` (e.g., `add_entry`).
    *   Variable names: `snake_case` (e.g., `knowledge_base`).
5.  **Simplicity:** Keep the code simple and direct. Avoid overly complex constructs if simpler alternatives exist. No external libraries beyond the Python standard library are needed for this task.
6.  **Error Handling:** For this module, error handling should primarily be through return values (e.g., `True`/`False` for success/failure, `None` if an item is not found). You can print warning messages to the console for non-critical issues (e.g., trying to add an entry with an existing ID). Do not raise custom exceptions unless absolutely necessary for Python's internal logic.

**Task Details:**

1.  **Create a new file** named `pkbm_core.py` within a directory structure `src/pkbm/`.
    So the full path will be `src/pkbm/pkbm_core.py`.

2.  **Implement a class** named `PKBMCore`.

3.  **Class `PKBMCore` Details:**

    *   **`__init__(self)`:**
        *   Initializes an instance variable `self.knowledge_base: Dict[str, Dict[str, Any]] = {}`. This dictionary will store all knowledge entries.
        *   The structure for an entry in `self.knowledge_base` is:
            `entry_id (str)` as the key, and its value is a dictionary:
            ```python
            {
                "type": str,  # e.g., "design_doc", "code_snippet", "requirement"
                "version": str, # e.g., "1.0", "1.1_draft"
                "content": Any, # The actual content
                "metadata": Dict[str, Any], # e.g., {"timestamp": "YYYY-MM-DDTHH:MM:SS", "source_agent": "AgentID", "tags": ["tag1", "tag2"]}
                "dependencies": List[str] # List of other entry_ids this entry depends on
            }
            ```

    *   **`add_entry(self, entry_id: str, entry_type: str, content: Any, version: str = "1.0", metadata: Optional[Dict[str, Any]] = None, dependencies: Optional[List[str]] = None) -> bool`:**
        *   Adds a new knowledge entry to `self.knowledge_base`.
        *   If `entry_id` already exists in `self.knowledge_base`, print a warning message to the console (e.g., f"Warning: Entry ID '{entry_id}' already exists. Not overwriting.") and return `False`. (For simplicity, we are not implementing versioning conflict resolution here, just preventing overwrite on add).
        *   If `metadata` is `None`, it should be initialized as an empty dictionary before storing.
        *   If `dependencies` is `None`, it should be initialized as an empty list before storing.
        *   Construct the entry dictionary as specified above and store it.
        *   Return `True` if the entry is successfully added.

    *   **`get_entry(self, entry_id: str) -> Optional[Dict[str, Any]]`:**
        *   Retrieves a knowledge entry by its `entry_id`.
        *   Returns the complete entry dictionary (including type, version, content, metadata, dependencies) if found.
        *   Returns `None` if the `entry_id` is not found.

    *   **`update_entry(self, entry_id: str, content: Any = None, entry_type: Optional[str] = None, version: Optional[str] = None, metadata_update: Optional[Dict[str, Any]] = None, dependencies: Optional[List[str]] = None) -> bool`:**
        *   Updates an existing knowledge entry.
        *   If `entry_id` does not exist in `self.knowledge_base`, return `False`.
        *   Only update the fields for which an argument is provided (i.e., the argument is not `None`).
        *   If `content` is provided, update the 'content' field.
        *   If `entry_type` is provided, update the 'type' field.
        *   If `version` is provided, update the 'version' field.
        *   If `metadata_update` is provided, it should be merged with the existing 'metadata'. Keys in `metadata_update` will overwrite existing keys in the entry's 'metadata' dictionary. If 'metadata' doesn't exist on the entry, it should be treated as an empty dict before updating.
        *   If `dependencies` is provided, update the 'dependencies' field.
        *   Return `True` if the update is successful.

    *   **`delete_entry(self, entry_id: str) -> bool`:**
        *   Deletes a knowledge entry by its `entry_id`.
        *   If `entry_id` does not exist in `self.knowledge_base`, return `False`.
        *   Otherwise, delete the entry and return `True`.

    *   **`list_entries(self, entry_type: Optional[str] = None, tags: Optional[List[str]] = None) -> List[Dict[str, Any]]`:**
        *   Returns a list of knowledge entries. Each entry in the list should be a dictionary that includes its `entry_id` plus all its stored data.
        *   Example item in the returned list:
            ```python
            {
                "entry_id": "some_id",
                "type": "design_doc",
                "version": "1.0",
                "content": "...",
                "metadata": {"tags": ["ui", "frontend"]},
                "dependencies": ["req_001"]
            }
            ```
        *   If `entry_type` is provided, filter entries by this type.
        *   If `tags` is provided (e.g., `['tagA', 'tagB']`), filter entries where the entry's `metadata['tags']` list (if it exists and is a list) contains ALL of the tags specified in the `tags` argument. This is a "contains all" (subset) check.
        *   If both `entry_type` and `tags` are provided, entries must satisfy both conditions.
        *   If no filters are provided, return all entries.

4.  **Imports:**
    *   Make sure to include `from typing import Dict, List, Any, Optional` at the beginning of the file.

**Expected Output:**
A single, complete Python file `src/pkbm/pkbm_core.py` containing the `PKBMCore` class, with all specified methods, type hints, and Google-style docstrings. The code should be runnable as is (assuming it's imported into another Python script).

```
我目前暂时的编码规范如下：
1. 模块化设计
    1. 职责单一: 每个类或模块仅专注于一项功能，实现高内聚、低耦合。
    2. 高级特性: 积极利用Python的高级特性，如装饰器、类、上下文管理器等，以增强代码的表达力和灵活性。
    3. 插件架构: 考虑采用插件化或扩展模块的设计，以便于未来功能的无缝集成和扩展。
    4. 接口明确: 模块间通过定义清晰、稳定的接口进行交互，降低模块间的耦合度。
    5. 依赖注入:
        1. 使用依赖注入（Dependency Injection）管理模块间的依赖关系。
        2. 通过构造函数、属性或方法参数等方式注入依赖，降低模块间的直接耦合。
        3. 考虑使用依赖注入容器或框架来简化依赖管理。
    6. 事件驱动/回调机制:
        1. 对于异步任务或需要解耦的场景，采用事件驱动或回调机制。
        2. 定义清晰的事件和回调接口。
        3. 考虑使用事件总线（Event Bus）或类似机制来实现模块间的松散耦合通信。
2. 配置管理
    1. 数据类（@dataclass） : 所有配置项必须通过Python数据类进行管理，提供类型安全和便捷的访问方式。
    2. 集中定义: 所有配置项应集中定义在一个或少数几个数据类中，避免分散和重复。
    3. 禁止硬编码: 严禁在代码中硬编码任何配置值，确保配置的灵活性和可维护性。
    4. 禁止配置文件: 禁止使用任何形式的外部配置文件（如JSON、YAML等），配置完全由数据类承载。
    5. 描述性命名: 配置项名称应具有良好的描述性，准确反映其用途和含义，避免歧义。
3. 错误处理
    1. 全面捕获: 使用try-except语句块全面捕获并妥善处理所有可能的异常，确保程序的健壮性。
    2. 明确提示: 异常信息必须包含清晰、准确的问题描述，并采用格式化输出，突出关键信息，便于问题定位。
    3. 避免崩溃: 确保程序在遇到异常时能够优雅地处理或安全退出，避免程序崩溃或数据丢失。
4. 日志记录
    1. 独立日志文件: 使用独立的日志文件记录程序的运行状态和关键事件，便于问题追踪和性能分析。
    2. 多级别日志: 使用不同级别的日志（如DEBUG, INFO, WARNING, ERROR, CRITICAL）对事件进行分类记录，方便筛选和查看。
    3. 详细信息: 每条日志信息应包含时间戳、模块名、日志级别、函数名、行号以及其他有助于问题定位的trace信息。
    4. 性能追踪: 记录关键操作或函数的执行时间，以便进行性能分析和优化。
5. 代码风格
    1. 缩进: 统一使用4个空格进行代码缩进，保持代码结构的清晰和一致性。
    2. 行长: 每行代码的长度应限制在79个字符以内，以提高代码的可读性。
    3. 命名规范:
        1. 类名采用驼峰命名法（CamelCase）。
        2. 函数名、变量名、方法名采用下划线命名法（snake_case）。
        3. 常量名采用全大写字母，单词间用下划线分隔（UPPER_SNAKE_CASE）。
        4. 所有命名都应具有描述性，避免使用缩写和模糊的名称，确保名称能够准确反映其所代表的含义。
    4. 类型提示: 使用类型提示（Type Hint）明确函数参数和返回值的类型，提高代码的可读性和可维护性，并支持静态类型检查。
6. 依赖管理
    1. 优先Python: 优先使用Python标准库和高质量的第三方库来实现功能，减少对外部工具的依赖。
    2. 减少外部依赖: 尽量避免使用Shell命令或其他外部脚本，以提高代码的跨平台兼容性和可移植性。
7. 代码复用
    1. 抽象: 将重复出现的代码逻辑抽象为独立的函数或方法，提高代码的复用性和可维护性。
    2. 通用性: 确保抽象出的函数或方法具有良好的通用性，能够适应不同的使用场景，避免重复编写相似的代码。
    3. 封装: 对具有相关性的变量和参数进行封装，形成更高级别的抽象，简化代码结构。
8. 禁止注释
    1. 代码中不允许出现任何注释和DocString
18. **3.1 当前子任务上下文准备Agent (Current Subtask Context Preparer):**

## Current Subtask Context Package for ST019

**Package ID:** `Context_ST019_v1.0`
**Timestamp:** (Current Timestamp)
**Prepared For Subtask:** `ST019`
**Subtask Description (from CodingTaskPlan_v1.0):** "Implement the `execute_llm_call` interface in LLM_Interaction_Gateway_Service."

### 1. Relevant User Stories & Acceptance Criteria Context

Subtask ST019 is a critical internal component. It directly implements the primary interface of the `LLM_Interaction_Gateway_Service`. This service is a foundational enabler for numerous user stories that rely on Large Language Model (LLM) interactions. The successful implementation of ST019 is crucial for:

*   **US003 (Indirectly):** If intent recognition uses an LLM.
*   **US006, US007, US010, US011, US012, US013, US016:** Many of these specification, design, and planning agents will leverage LLMs via this gateway.
*   **US021:** "As a Developer, I want to have the system design class and function logic...using unambiguous pseudocode..." (Agent 3.4 will use an LLM via this gateway).
*   **US024:** "As a Developer, I want to...generate an extremely specific, context-rich prompt text for an external code generation LLM (Roo code LLM)..." (Agent 3.7 will use an LLM via this gateway to formulate the prompt, and the gateway itself will be used in ST026/Agent 4.1 to send that prompt to Roo code LLM).
*   **US027:** "...auto-suggested simple formatting fixes (which would then be formulated as a new prompt for the LLM to apply)..." (Agent 4.2 may use an LLM via this gateway).
*   **US028:** "...plain text report (for LLM consumption)..." (Agent 4.3's output might be fed to an LLM by another agent via this gateway).
*   **US029:** "As a Developer, I want to...have an AI Code Reviewer agent analyze the code..." (Agent 5.1 will heavily rely on an LLM via this gateway).
*   **US031:** "As a Developer, I want to...have the system generate unit test case code..." (Agent 5.3 will use an LLM via this gateway).
*   **US033:** "...if tests fail, generate a Markdown debugging analysis report..." (Agent 5.5 may use an LLM via this gateway).
*   **US035:** "As a Developer, I want to...have the system generate a specific LLM prompt for fixing identified bugs..." (Agent 5.7 will use an LLM via this gateway).
*   **US039:** "...generate compliant, explanatory code comments...guide an LLM...to apply these comments..." (Agent 6.4 will use an LLM via this gateway).
*   **US040:** "...update or generate API documentation...README files...Changelog entries..." (Agent 6.5 may use an LLM via this gateway).
*   **US042:** "...generate a project summary report...distillable knowledge/patterns..." (Agent 6.7 may use an LLM via this gateway).

The "acceptance criteria" for ST019 itself will be technical, focusing on the correct implementation of the `execute_llm_call` method within the `LLMInteractionGatewayService` class. This method must correctly:
*   Receive high-level LLM call parameters (e.g., target LLM provider, prompt, model-specific settings).
*   Utilize the outputs of ST018 (data models, exceptions, and `http_client.py`) to:
    *   Construct the appropriate `HttpRequestConfig`.
    *   Invoke `make_http_request_with_retry`.
    *   Handle `HttpResponseData` and exceptions returned by the HTTP client.
*   Parse the LLM's JSON response into a usable format.
*   Manage different LLM provider configurations (API keys, base URLs) in a simple, direct way.

### 2. Architectural Context

**Source Document:** `HighLevelArchitectureDesignDocument_v1.0`

*   **Component Definition (LLM_Interaction_Gateway_Service):**
    *   **Description:** Provides a centralized and standardized interface for all internal services/agents to communicate with external Large Language Models (LLMs).
    *   **Core Responsibilities relevant to ST019:**
        *   Abstracting LLM interaction specifics.
        *   Managing API calls (REST-based, using ST018's utilities).
        *   **Handling request construction (translating high-level LLM parameters into `HttpRequestConfig`).**
        *   **Handling response parsing (translating `HttpResponseData` into a service-level response).**
        *   Implementing retry mechanisms (delegated to ST018's utilities).
        *   **Managing LLM API keys and endpoint configurations (simplified direct configuration, e.g., via Python dataclasses passed at runtime or service initialization).**
        *   Logging LLM interactions (leveraging ST018's logging and adding service-level logging).

*   **Relevant Interface Definition (from `ComponentInterfaceDraft_v1.0` and HLDD Section 4):**
    *   **Interface ID:** `CID011` (Conceptual internal interface to be implemented by the `LLMInteractionGatewayService` class).
    *   **Interaction:** `Agent_Orchestration_Service` (or any agent) -> `LLM_Interaction_Gateway_Service`.
    *   **Type:** Internal Python method call.
    *   **Conceptual Method Signature (to be implemented by ST019):** `execute_llm_call(self, provider_config: LLMProviderConfig, request_details: LLMRequestDetails) -> LLMResponse` (The exact types `LLMProviderConfig`, `LLMRequestDetails`, `LLMResponse` will be designed as part of ST019's detailed design phase by Agents 3.3 and 3.4).
    *   **Description:** "Sends a request to a configured LLM and returns the response. Handles actual HTTP communication (via ST018 utilities)."
    *   **ST019 Focus:** Implementing the `LLMInteractionGatewayService` class and its `execute_llm_call` method, which orchestrates the use of ST018's `http_client.py`.

*   **Technology Selection for LLM Interaction:**
    *   Method: Standard REST API calls (handled by ST018). Python.

### 3. Project Metadata Summary

**Source Document:** `ProjectMetadata_v1.0`

*   **Primary Programming Language:** Python
*   **LLM Interaction Method:** Standard REST API calls via Web Service (this gateway).
*   **Key Simplifications & Constraints:**
    *   Reduced emphasis on efficiency.
    *   Focus on simplicity, success rate, and ease of configuration.
    *   **Security Considerations:** Explicitly deprioritized; all permissions should be as open as possible to ensure functionality. Configuration of API keys should be direct and simple (e.g., within dataclasses or passed directly).

### 4. Coding Standards and Guidelines (Crucial for LLM Prompt)

**Source:** User-provided "编码规范" (Full text to be provided to LLM for code generation).
Key aspects relevant to this task (ST019):
1.  **Modular Design:**
    *   The `LLMInteractionGatewayService` class will be a key module.
    *   It will use dependency injection principles by utilizing the modules from ST018 (e.g., `http_client`, `data_models`, `exceptions`).
2.  **Configuration Management:**
    *   All configurations for different LLM providers (e.g., API keys, base URLs, model names, specific header requirements) **must** be managed via Python `@dataclass`.
    *   These configurations should be passed to the `execute_llm_call` method or to the service during initialization.
    *   **No external config files (JSON, YAML, etc.). No hardcoding.**
3.  **Error Handling:**
    *   The `execute_llm_call` method must comprehensively catch exceptions raised by ST018's `http_client.py` (e.g., `HttpConnectionErrorException`, `LLMAuthenticationError`).
    *   It should provide clear, formatted error messages if it re-raises exceptions or returns an error status.
4.  **Logging:**
    *   Log key events within `execute_llm_call`: request received, LLM provider targeted, parameters used (excluding full sensitive data if possible, but prioritize debuggability), success/failure of the call, and response summary.
    *   Utilize Python's `logging` module.
5.  **Code Style:**
    *   4-space indent, 79-char lines.
    *   Class names: `CamelCase` (e.g., `LLMInteractionGatewayService`).
    *   Method/variable names: `snake_case` (e.g., `execute_llm_call`, `provider_config`).
    *   Constants: `UPPER_SNAKE_CASE`.
    *   Type Hinting is mandatory.
6.  **Dependencies:**
    *   Primarily Python standard library and the modules created in ST018.
    *   The `requests` library is used internally by ST018.
7.  **Code Reuse:**
    *   Leverage the functions and data models from ST018.
8.  **No Comments/Docstrings:** Code should be self-explanatory. (This applies to the *generated code*; this context package itself IS documentation).

### 5. Existing Code Snippets & File Structure (Output of ST018, Input for ST019)

**Source:** `FileStructurePlan_ST018_v1.0.json` and assumed implementation of ST018.
The following files and their contents (as designed for ST018) are critical context for ST019, as ST019 will import and use them:

*   **Location:** `app/services/llm_gateway_service/`
    *   `__init__.py` (Makes it a package)
    *   `data_models.py`: Contains `HttpRequestConfig`, `HttpResponseData`, `HttpRetryConfig`.
    *   `exceptions.py`: Contains `HttpClientBaseException`, `LLMAuthenticationError`, etc.
    *   `http_client.py`: Contains `make_http_request_with_retry` and helper functions.

**ST019 will likely involve creating a new file within this package, for example:**
*   `app/services/llm_gateway_service/service.py` (or `gateway.py`)
    *   This file would contain the `LLMInteractionGatewayService` class definition and its `execute_llm_call` method.

### 6. Additional Notes or Constraints for ST019

*   **Provider Abstraction:** The `execute_llm_call` method needs to be designed to handle different LLM providers. This means the `provider_config` and `request_details` parameters must be flexible enough to carry provider-specific information (e.g., API key, base URL, prompt formatting, specific model parameters).
*   **Simplicity of Configuration:** How different LLM providers are configured (e.g., a dictionary of `LLMProviderConfig` objects managed by the `Agent_Orchestration_Service` and passed to this gateway) should be straightforward. For ST019, assume the necessary `LLMProviderConfig` is passed in for each call.
*   **Request Construction:** A key part of ST019 is translating the abstract `request_details` and `provider_config` into the concrete `HttpRequestConfig` that `http_client.make_http_request_with_retry` expects. This might involve:
    *   Formatting the correct API endpoint URL.
    *   Setting appropriate headers (e.g., `Authorization`, `Content-Type`).
    *   Structuring the JSON payload according to the target LLM provider's API specification.
*   **Response Parsing:** Another key part is taking the `HttpResponseData.json_body` and parsing it into a more generic `LLMResponse` structure, or at least returning the raw JSON data in a well-defined way.
*   **Security (Reiteration):** No security features. API keys are passed directly. SSL verification is configurable as per ST018's design to ensure connection success.

# 下一步任务 3.2 API详细设计Agent (API Detailed Design Agent)

基于上述“原始用户需求描述”提供的完整细节和“已完成阶段输出”中提炼的关键点、已确认事项以及明确的设计任务，请你作为 3.2 API详细设计Agent (API Detailed Design Agent) ，开始执行“3.2 API详细设计Agent (API Detailed Design Agent): ”的任务。对用户说明、介绍、回答和提问的内容请使用专业严谨的简体中文，对Agent和LLM输出的内容请使用专业的英文。请额外注意，我项目预期会在绝对安全的环境中使用，请完全绝对不考虑安全性，所有权限等完全放开，绝对优先保证代码、功能的成功率和简便性，防止因为权限和安全设定造成的失败，方便配置使用。如果需要Roo code 中 LLM 修改、生成代码，请额外在Prompt中强调我的编码规范中的内容。本次循环要完成的是ST019这个子任务，我会提供其他依赖的子任务已有的内容。

**3.2 API详细设计Agent (API Detailed Design Agent)：**

    *   **任务:**
        1.  接收“当前子任务上下文包”。
        2.  根据子任务需求和架构中的接口草案，详细设计API：
            *   精确的URL路径和HTTP方法。
            *   请求头、路径参数、查询参数的详细定义。
            *   请求体的JSON/XML Schema (或等效的详细结构描述)。
            *   响应体的JSON/XML Schema (或等效的详细结构描述) for 各种状态码 (200, 201, 400, 401, 403, 500等)。
            *   认证和授权机制说明。
        3.  输出API详细设计文档（例如，OpenAPI/Swagger片段或Markdown格式的详细描述）。
        4.  将文档存入PKBM，标记为“API详细设计_[子任务ID]_v1.0”。
    *   **输入:** 子任务上下文包。
    *   **输出:** API详细设计文档。
